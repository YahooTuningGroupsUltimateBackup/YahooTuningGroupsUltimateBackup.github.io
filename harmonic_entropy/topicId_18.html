<a href="/harmonic_entropy">back to list</a><h1>Information Theory</h1><h3><a id=18 href="#18">ðŸ”—</a>David Finnamore &#x3C;daeron@...&#x3E;</h3><span>9/30/2000 10:14:09 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul,</p><p>Maybe a good thing to do near the beginning of this new list is to<br/>cover the bare basics of Info Theory as they apply to Harmonic<br/>Entropy Theory.  I think I might have read a little about it once in<br/>Discovery Magazine or some such place.  As I understand it, &quot;entropy&quot;<br/>in Info Theory has to do with the efficiency with which a<br/>communiqu&eacute; communicates, essentially.  You&apos;ve used the term<br/>&quot;compressibility.&quot;  If a message is concise and clear, it has low<br/>entropy.  If it&apos;s either wordy or vague, it has high entropy.  Is<br/>that in the ballpark?  If so, let&apos;s expand on the specific<br/>application<br/>to H. E.  This was probably roughly covered on the Tuning List a<br/>couple of years ago but it might be wise to give a refresher here for<br/>the sake of getting off on the right foot.</p><p>David Finnamore</p></div><h3><a id=24 href="#24">ðŸ”—</a>Paul Erlich &#x3C;PERLICH@...&#x3E;</h3><span>10/1/2000 1:23:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:harmonic_entropy@egroups.com">harmonic_entropy@egroups.com</a>, &quot;David Finnamore&quot; &lt;daeron@b...&gt;<br/>wrote:<br/>&gt; Paul,<br/>&gt;<br/>&gt; Maybe a good thing to do near the beginning of this new list is to<br/>&gt; cover the bare basics of Info Theory as they apply to Harmonic<br/>&gt; Entropy Theory.  I think I might have read a little about it once in<br/>&gt; Discovery Magazine or some such place.  As I understand<br/>it, &quot;entropy&quot;<br/>&gt; in Info Theory has to do with the efficiency with which a<br/>&gt; communiqu&eacute; communicates, essentially.  You&apos;ve used the term<br/>&gt; &quot;compressibility.&quot;  If a message is concise and clear, it has low<br/>&gt; entropy.  If it&apos;s either wordy or vague, it has high entropy.  Is<br/>&gt; that in the ballpark?</p><p>Yup!</p><p>&gt; If so, let&apos;s expand on the specific<br/>&gt; application<br/>&gt; to H. E.  This was probably roughly covered on the Tuning List a<br/>&gt; couple of years ago but it might be wise to give a refresher here<br/>for<br/>&gt; the sake of getting off on the right foot.</p><p>From <a href="http://www.ixpres.com/interval/td/entropy.htm:">http://www.ixpres.com/interval/td/entropy.htm:</a></p><p>[In a Farey or other series of fractions, representing dyads from the<br/>harmonic series than the brain could ideally recognize as such, the<br/>simpler-integer ratios take up a lot of room, defined as the interval<br/>between the mediant below and the mediant above, in interval space,<br/>and so are associated with large &quot;slices&quot; of the probability<br/>distribution, while the more complex ratios are more crowded and<br/>therefore are associated with smaller &quot;slices.&quot; Now the harmonic<br/>entropy is defined, just like in information theory, as the [minus<br/>the] sum over all ratios of a certain function of the probability<br/>associated with that ratio. The function is x*log(x). (See an<br/>information theory text to find out why.) When the true interval is<br/>near a simple-integer ratio, there will be one large probability and<br/>many much smaller ones. When the true interval is far from any simple-<br/>integer ratios, many more complex ratios will all have roughly equal<br/>probabilities. The entropy function will come out quite small in the<br/>former case, and quite large in the latter case. In the case of 700<br/>cents, 3/2 will have far more probabilty than any other ratio, and<br/>the harmonic entropy is nearly minimal. In the case of 300 cents, 6/5<br/>will have the largest probability in most cases, but 7/6, 13/11, and<br/>19/16 will all have non-negligible amounts of probability, so the<br/>harmonic entropy is moderate. In the case of 100 cents, 15/14, 16/15,<br/>17/16, 18/17, 19/18, 20/19, and 1/1 will all have significant<br/>probability, and the harmonic entropy is nearly maximal.</p></div><h3><a id=31 href="#31">ðŸ”—</a>David J. Finnamore &#x3C;daeron@...&#x3E;</h3><span>10/2/2000 8:51:05 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thanks, Paul, for the re- over-view.  That clears things up for me for now.</p><p>--<br/>David J. Finnamore<br/>Nashville, TN, USA<br/><a href="http://personal.bna.bellsouth.net/bna/d/f/dfin/index.html">http://personal.bna.bellsouth.net/bna/d/f/dfin/index.html</a><br/>--</p></div>