<a href="/harmonic_entropy">back to list</a><h1>RE: [harmonic_entropy] Digest Number 188</h1><h3>Bohlen, Heinz &#x3C;heinz.bohlen@...&#x3E;</h3><span>2/3/2004 8:30:45 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This is a message for Paul.<br/>Your answer to the &quot;newbie question&quot; made me look at your entropy graphs<br/>once more, and from that resulted two comments.</p><p>1. It would be helpful if the graphs didn&apos;t just end at 2:1, but would<br/>include 3:1. Not only because this would aid an assessment of the BP scale<br/>with regard to harmonic entropy, but also because it might generally shed<br/>some light on the perception of intervals that exceed the octave span.</p><p>2. The differentiation between intervals as presented in the graphs appears<br/>a bit low, and the entropy for intervals generally accepted as strikingly<br/>consonant a bit high, at least from my admittedly subjective view point. To<br/>explain what I mean: using Shannon&apos;s entropy expression, the probability<br/>resulting from the Mann series for the fifth (HE ~ 2.5) turns out to be ~ 18<br/>% only, and for the major sixth (HE ~ 3) ~ 12 % only, while the worst case<br/>is as high as ~ 7 % (HE ~ 3.8). Or am I interpreting the graphs incorrectly?</p><p>Heinz</p><p>-----Original Message-----<br/>From: <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a><br/>[<a href="mailto:harmonic_entropy@yahoogroups.com">mailto:harmonic_entropy@yahoogroups.com</a>]<br/>Sent: Monday, February 02, 2004 1:17 PM<br/>To: <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a><br/>Subject: [harmonic_entropy] Digest Number 188</p><p>There are 2 messages in this issue.</p><p>Topics in this digest:</p><p>      1. Newbie question: How is harmonic entropy computed?<br/>           From: Denix 13 &lt;denix13@...&gt;<br/>      2. Re: Newbie question: How is harmonic entropy computed?<br/>           From: &quot;wallyesterpaulrus&quot; &lt;wallyesterpaulrus@...&gt;</p><p>________________________________________________________________________<br/>________________________________________________________________________</p><p>Message: 1<br/>   Date: Mon, 2 Feb 2004 02:41:21 +0100<br/>   From: Denix 13 &lt;denix13@...&gt;<br/>Subject: Newbie question: How is harmonic entropy computed?</p><p>Hi there,</p><p>I  am  a  new subscriber  to  the  list  so  excuse me  for  my  silly<br/>preliminary questions.</p><p>1)  I  must  be missing  something,  but  I  did  not find  any  clear<br/>indication of a how harmonic entropy is  computed. What I see is a lot<br/>of nice figures and  diagrams but I am still looking  for clues to how<br/>they  are obtained...  I understand  that  it is  directly related  to<br/>Shannon&apos;s entropy definition Xlog(X), where X is a probability density<br/>function, but could  not derive an algorithm by  myself. If available,<br/>could anyone  give a  pointer to  a place  where the  computations are<br/>explained  more precisely?  The best  introductory site  I could  find<br/>is  on  the &lt;a  href=&quot;<a href="http://tonalsoft.com/enc/harmentr.htm">http://tonalsoft.com/enc/harmentr.htm</a>&quot;&gt;  harmony<br/>entropy page  at Joe  Monzo&apos;s Site&lt;/a&gt;  but this  page is  lacking the<br/>maths I am looking for.</p><p>2)  Actually  I  am  interested  in  finding  a  way  to  measure  the<br/>consonance/dissonance of  complex chords: not only  for dyads, triads,<br/>or tetrads, but for an arbitrary multi-component cluster. I understand<br/>that in  this case  the decomposition into  individual dyads  does not<br/>lead  to a  correct  measure of  harmonic entropy,  as  stated in  the<br/>page &lt;a href=&quot;<a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a>&quot;&gt; on Harmonic<br/>Entropy&lt;/a&gt;. How did you obtain the HE for triads and tetrads ?</p><p>3) Last question, how closely  is harmonic entropy related to Barlow&apos;s<br/>harmonicity?  Harmonicity  seems   less  mathematically  founded  than<br/>harmonic  entropy,  but it  has  a  straightforward generalization  to<br/>complex chords by substituting LCM/GCD to the original p/q ratio.</p><p>Thanks.</p><p>________________________________________________________________________<br/>________________________________________________________________________</p><p>Message: 2<br/>   Date: Mon, 02 Feb 2004 05:04:47 -0000<br/>   From: &quot;wallyesterpaulrus&quot; &lt;wallyesterpaulrus@...&gt;<br/>Subject: Re: Newbie question: How is harmonic entropy computed?</p><p>--- In <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a>, Denix 13 &lt;denix13@w...&gt;<br/>wrote:</p><p>&gt; Hi there,<br/>&gt;<br/>&gt; I  am  a  new subscriber  to  the  list  so  excuse me  for  my<br/>silly<br/>&gt; preliminary questions.</p><p>Not silly at all -- don&apos;t be shy!</p><p>&gt; 1)  I  must  be missing  something,  but  I  did  not find  any<br/>clear<br/>&gt; indication of a how harmonic entropy is  computed. What I see is a<br/>lot<br/>&gt; of nice figures and  diagrams but I am still looking  for clues to<br/>how<br/>&gt; they  are obtained...  I understand  that  it is  directly related<br/>to<br/>&gt; Shannon&apos;s entropy definition Xlog(X), where X is a probability<br/>density<br/>&gt; function, but could  not derive an algorithm by  myself. If<br/>available,<br/>&gt; could anyone  give a  pointer to  a place  where the  computations<br/>are<br/>&gt; explained  more precisely?  The best  introductory site  I could<br/>find<br/>&gt; is  on  the &lt;a  href=&quot;<a href="http://tonalsoft.com/enc/harmentr.htm">http://tonalsoft.com/enc/harmentr.htm</a>&quot;&gt;<br/>harmony<br/>&gt; entropy page  at Joe  Monzo&apos;s Site&lt;/a&gt;  but this  page is  lacking<br/>the<br/>&gt; maths I am looking for.</p><p>It might have helped to also look at<br/><a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a> . . .</p><p>Then again, it might not.</p><p>The probability function you are looking for begins as a three-<br/>parameter function, where the first parameter (call it j) is an index<br/>into your big series of ratios (Farey series or n*d &lt; 10000 or 65536<br/>or whatever) and the second (call it i) is the interval actually<br/>being heard.</p><p>We define p(j,i,s) as 1/(s*sqrt(2*pi)) times<br/>the integral from<br/>mediant(j-1,j)<br/>to<br/>mediant(j,j+1)<br/>of<br/>exp( -(cents(t)-cents(i))^2 / (2s^2) ) dt</p><p>IMPORTANT: the units of t should be logarithmic, e.g., cents.</p><p>The function you&apos;re integrating is nearly constant, so it makes<br/>little difference if we replace this with</p><p>(mediant(j+1,j)-mediant(j,j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2) )</p><p>and a constant of proportionality determined such that</p><p>SUM (p(j,i,s)) = 1<br/>j</p><p>It apparently also makes little difference in the result, if for the<br/>partitioning, we replace the mediants with means (which will allow us<br/>to use voronoi cells in the generalization to higher dimensions),<br/>giving:</p><p>1/2*(cents(j+1)-cents(j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2)</p><p>In any case, the harmonic entropy HE(i,s) is then simply</p><p>-SUM (p(j,i,s) log(p(j,i,s)))<br/>j<br/>.</p><p>&gt; 2)  Actually  I  am  interested  in  finding  a  way  to  measure<br/>the<br/>&gt; consonance/dissonance of  complex chords: not only  for dyads,<br/>triads,<br/>&gt; or tetrads, but for an arbitrary multi-component cluster. I<br/>understand<br/>&gt; that in  this case  the decomposition into  individual dyads  does<br/>not<br/>&gt; lead  to a  correct  measure of  harmonic entropy,  as  stated in<br/>the<br/>&gt; page &lt;a href=&quot;<a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a>&quot;&gt; on<br/>Harmonic<br/>&gt; Entropy&lt;/a&gt;. How did you obtain the HE for triads and tetrads ?</p><p>I haven&apos;t yet, but I&apos;ve described how the calculation is to be done,<br/>shown some partitionings for triads using Voronoi cells, and how the<br/>sizes of these partitions follow the same dependence on the product<br/>of the numbers in the three-term ratio as, in the dyadic case, the<br/>mediants follow on the product of the numbers in the ordinary ratio.<br/>So some features of triadic and tetradic harmonic entropy are already<br/>known with a high degree of certainty.</p><p>&gt; 3) Last question, how closely  is harmonic entropy related to<br/>Barlow&apos;s<br/>&gt; harmonicity?  Harmonicity  seems   less  mathematically  founded<br/>than<br/>&gt; harmonic  entropy,  but it  has  a  straightforward generalization<br/>to<br/>&gt; complex chords by substituting LCM/GCD to the original p/q ratio.</p><p>For a two-note chord, LCM/GCD gives you p*q, not p/q, so I&apos;m not sure<br/>I understand this substitution. But the harmonic entropy calculations<br/>and graphs seem to be saying that for any selection of not-too-<br/>complex ratios or chords, the product of the numbers in the ratio (be<br/>it two-term, three-term . . .) gives the correct dissonance ranking,<br/>and in fact the log of the product gives the approximate relative<br/>entropy. See the &quot;files&quot; folder of this group.</p><p>But that doesn&apos;t explain what you then *do* with the ratio in<br/>Barlow&apos;s Harmonicity, which doesn&apos;t seem related to how we perceive<br/>consonance, especially in that it assumes that simple ratios have<br/>zero tolerance for mistuning, and can&apos;t even accept irrational<br/>inputs! This substitution you mention, to the extent that it&apos;s valid,<br/>really has little to do with the rest of Barlow&apos;s formulation,<br/>correct?</p><p>Best,<br/>Paul</p><p>________________________________________________________________________<br/>________________________________________________________________________</p><p>------------------------------------------------------------------------<br/>Yahoo! Groups Links</p><p>To visit your group on the web, go to:<br/> <a href="http://groups.yahoo.com/group/harmonic_entropy/">http://groups.yahoo.com/group/harmonic_entropy/</a></p><p>To unsubscribe from this group, send an email to:<br/> <a href="mailto:harmonic_entropy-unsubscribe@yahoogroups.com">harmonic_entropy-unsubscribe@yahoogroups.com</a></p><p>Your use of Yahoo! Groups is subject to:<br/> <a href="http://docs.yahoo.com/info/terms/">http://docs.yahoo.com/info/terms/</a><br/>------------------------------------------------------------------------</p></div><h3>wallyesterpaulrus &#x3C;wallyesterpaulrus@...&#x3E;</h3><span>2/3/2004 1:07:19 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a>, &quot;Bohlen, Heinz&quot;<br/>&lt;heinz.bohlen@c...&gt; wrote:<br/>&gt; This is a message for Paul.<br/>&gt; Your answer to the &quot;newbie question&quot; made me look at your entropy<br/>&gt;graphs<br/>&gt; once more, and from that resulted two comments.<br/>&gt;<br/>&gt; 1. It would be helpful if the graphs didn&apos;t just end at 2:1, but<br/>&gt;would<br/>&gt; include 3:1. Not only because this would aid an assessment of the<br/>&gt;BP scale<br/>&gt; with regard to harmonic entropy, but also because it might<br/>&gt;generally shed<br/>&gt; some light on the perception of intervals that exceed the octave<br/>&gt;span.</p><p>Most of the graphs (including the one on the homepage of this group)<br/>go to 4:1, some go even further.</p><p>&gt; 2. The differentiation between intervals as presented in the graphs<br/>&gt;appears<br/>&gt; a bit low, and the entropy for intervals generally accepted as<br/>&gt;strikingly<br/>&gt; consonant a bit high, at least from my admittedly subjective view<br/>&gt;point.</p><p>Sure. You may prefer the version of harmonic entropy which includes<br/>unreduced ratios, though then the weighting of the ratios is more<br/>arbitrary since the concept of &quot;width&quot; is no longer applicable. It<br/>seems that in the usual case, the simplest ratios get an entropy<br/>proportional to log(n*d) above the entropy of 1:1, while if you<br/>include unreduced ratios, it&apos;s proportional to n*d above the entropy<br/>of 1:1.</p><p>&gt; To<br/>&gt; explain what I mean: using Shannon&apos;s entropy expression, the<br/>&gt;probability<br/>&gt; resulting from the Mann series for the fifth (HE ~ 2.5) turns out<br/>&gt;to be ~ 18<br/>&gt; % only, and for the major sixth (HE ~ 3) ~ 12 % only,</p><p>How are you obtaining these probabilities? You have to assume an<br/>actual heard interval in addition to a putative ratio . . . Maybe<br/>you&apos;re misinterpreting Shannon&apos;s expression? It&apos;s a sum of (-p*log(p))<br/>over probabilites which themselves sum to 1, but it cannot relate to<br/>a single probability.</p></div><h3>wallyesterpaulrus &#x3C;wallyesterpaulrus@...&#x3E;</h3><span>2/3/2004 10:49:40 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot;<br/>&lt;wallyesterpaulrus@y...&gt; wrote:<br/>&gt; --- In <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a>, &quot;Bohlen, Heinz&quot;<br/>&gt; &lt;heinz.bohlen@c...&gt; wrote:<br/>&gt; &gt; This is a message for Paul.<br/>&gt; &gt; Your answer to the &quot;newbie question&quot; made me look at your entropy<br/>&gt; &gt;graphs<br/>&gt; &gt; once more, and from that resulted two comments.<br/>&gt; &gt;<br/>&gt; &gt; 1. It would be helpful if the graphs didn&apos;t just end at 2:1, but<br/>&gt; &gt;would<br/>&gt; &gt; include 3:1. Not only because this would aid an assessment of the<br/>&gt; &gt;BP scale<br/>&gt; &gt; with regard to harmonic entropy, but also because it might<br/>&gt; &gt;generally shed<br/>&gt; &gt; some light on the perception of intervals that exceed the octave<br/>&gt; &gt;span.<br/>&gt;<br/>&gt; Most of the graphs (including the one on the homepage of this<br/>group)<br/>&gt; go to 4:1, some go even further.</p><p>This one goes nearly to 18:1 --</p><p><a href="http://groups.yahoo.com/group/harmonic_entropy/files/dyadic/heinz.gif">http://groups.yahoo.com/group/harmonic_entropy/files/dyadic/heinz.gif</a></p></div>