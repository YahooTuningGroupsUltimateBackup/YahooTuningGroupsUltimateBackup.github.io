<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup harmonic_entropy Newbie question: How is harmonic entropy computed?</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/harmonic_entropy">back to list</a><h1>Newbie question: How is harmonic entropy computed?</h1><h3><a id=707 href="#707">ðŸ”—</a>Denix 13 &#x3C;denix13@...&#x3E;</h3><span>2/1/2004 5:41:21 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hi there,</p><p>I  am  a  new subscriber  to  the  list  so  excuse me  for  my  silly<br/>preliminary questions.</p><p>1)  I  must  be missing  something,  but  I  did  not find  any  clear<br/>indication of a how harmonic entropy is  computed. What I see is a lot<br/>of nice figures and  diagrams but I am still looking  for clues to how<br/>they  are obtained...  I understand  that  it is  directly related  to<br/>Shannon&apos;s entropy definition Xlog(X), where X is a probability density<br/>function, but could  not derive an algorithm by  myself. If available,<br/>could anyone  give a  pointer to  a place  where the  computations are<br/>explained  more precisely?  The best  introductory site  I could  find<br/>is  on  the &lt;a  href=&quot;<a href="http://tonalsoft.com/enc/harmentr.htm">http://tonalsoft.com/enc/harmentr.htm</a>&quot;&gt;  harmony<br/>entropy page  at Joe  Monzo&apos;s Site&lt;/a&gt;  but this  page is  lacking the<br/>maths I am looking for.</p><p>2)  Actually  I  am  interested  in  finding  a  way  to  measure  the<br/>consonance/dissonance of  complex chords: not only  for dyads, triads,<br/>or tetrads, but for an arbitrary multi-component cluster. I understand<br/>that in  this case  the decomposition into  individual dyads  does not<br/>lead  to a  correct  measure of  harmonic entropy,  as  stated in  the<br/>page &lt;a href=&quot;<a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a>&quot;&gt; on Harmonic<br/>Entropy&lt;/a&gt;. How did you obtain the HE for triads and tetrads ?</p><p>3) Last question, how closely  is harmonic entropy related to Barlow&apos;s<br/>harmonicity?  Harmonicity  seems   less  mathematically  founded  than<br/>harmonic  entropy,  but it  has  a  straightforward generalization  to<br/>complex chords by substituting LCM/GCD to the original p/q ratio.</p><p>Thanks.</p></div><h3><a id=708 href="#708">ðŸ”—</a>wallyesterpaulrus &#x3C;wallyesterpaulrus@...&#x3E;</h3><span>2/1/2004 9:04:47 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:harmonic_entropy@yahoogroups.com">harmonic_entropy@yahoogroups.com</a>, Denix 13 &lt;denix13@w...&gt;<br/>wrote:</p><p>&gt; Hi there,<br/>&gt;<br/>&gt; I  am  a  new subscriber  to  the  list  so  excuse me  for  my<br/>silly<br/>&gt; preliminary questions.</p><p>Not silly at all -- don&apos;t be shy!</p><p>&gt; 1)  I  must  be missing  something,  but  I  did  not find  any<br/>clear<br/>&gt; indication of a how harmonic entropy is  computed. What I see is a<br/>lot<br/>&gt; of nice figures and  diagrams but I am still looking  for clues to<br/>how<br/>&gt; they  are obtained...  I understand  that  it is  directly related<br/>to<br/>&gt; Shannon&apos;s entropy definition Xlog(X), where X is a probability<br/>density<br/>&gt; function, but could  not derive an algorithm by  myself. If<br/>available,<br/>&gt; could anyone  give a  pointer to  a place  where the  computations<br/>are<br/>&gt; explained  more precisely?  The best  introductory site  I could<br/>find<br/>&gt; is  on  the &lt;a  href=&quot;<a href="http://tonalsoft.com/enc/harmentr.htm">http://tonalsoft.com/enc/harmentr.htm</a>&quot;&gt;<br/>harmony<br/>&gt; entropy page  at Joe  Monzo&apos;s Site&lt;/a&gt;  but this  page is  lacking<br/>the<br/>&gt; maths I am looking for.</p><p>It might have helped to also look at<br/><a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a> . . .</p><p>Then again, it might not.</p><p>The probability function you are looking for begins as a three-<br/>parameter function, where the first parameter (call it j) is an index<br/>into your big series of ratios (Farey series or n*d &lt; 10000 or 65536<br/>or whatever) and the second (call it i) is the interval actually<br/>being heard.</p><p>We define p(j,i,s) as 1/(s*sqrt(2*pi)) times<br/>the integral from<br/>mediant(j-1,j)<br/>to<br/>mediant(j,j+1)<br/>of<br/>exp( -(cents(t)-cents(i))^2 / (2s^2) ) dt</p><p>IMPORTANT: the units of t should be logarithmic, e.g., cents.</p><p>The function you&apos;re integrating is nearly constant, so it makes<br/>little difference if we replace this with</p><p>(mediant(j+1,j)-mediant(j,j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2) )</p><p>and a constant of proportionality determined such that</p><p>SUM (p(j,i,s)) = 1<br/>j</p><p>It apparently also makes little difference in the result, if for the<br/>partitioning, we replace the mediants with means (which will allow us<br/>to use voronoi cells in the generalization to higher dimensions),<br/>giving:</p><p>1/2*(cents(j+1)-cents(j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2)</p><p>In any case, the harmonic entropy HE(i,s) is then simply</p><p>-SUM (p(j,i,s) log(p(j,i,s)))<br/>j<br/>.</p><p>&gt; 2)  Actually  I  am  interested  in  finding  a  way  to  measure<br/>the<br/>&gt; consonance/dissonance of  complex chords: not only  for dyads,<br/>triads,<br/>&gt; or tetrads, but for an arbitrary multi-component cluster. I<br/>understand<br/>&gt; that in  this case  the decomposition into  individual dyads  does<br/>not<br/>&gt; lead  to a  correct  measure of  harmonic entropy,  as  stated in<br/>the<br/>&gt; page &lt;a href=&quot;<a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a>&quot;&gt; on<br/>Harmonic<br/>&gt; Entropy&lt;/a&gt;. How did you obtain the HE for triads and tetrads ?</p><p>I haven&apos;t yet, but I&apos;ve described how the calculation is to be done,<br/>shown some partitionings for triads using Voronoi cells, and how the<br/>sizes of these partitions follow the same dependence on the product<br/>of the numbers in the three-term ratio as, in the dyadic case, the<br/>mediants follow on the product of the numbers in the ordinary ratio.<br/>So some features of triadic and tetradic harmonic entropy are already<br/>known with a high degree of certainty.</p><p>&gt; 3) Last question, how closely  is harmonic entropy related to<br/>Barlow&apos;s<br/>&gt; harmonicity?  Harmonicity  seems   less  mathematically  founded<br/>than<br/>&gt; harmonic  entropy,  but it  has  a  straightforward generalization<br/>to<br/>&gt; complex chords by substituting LCM/GCD to the original p/q ratio.</p><p>For a two-note chord, LCM/GCD gives you p*q, not p/q, so I&apos;m not sure<br/>I understand this substitution. But the harmonic entropy calculations<br/>and graphs seem to be saying that for any selection of not-too-<br/>complex ratios or chords, the product of the numbers in the ratio (be<br/>it two-term, three-term . . .) gives the correct dissonance ranking,<br/>and in fact the log of the product gives the approximate relative<br/>entropy. See the &quot;files&quot; folder of this group.</p><p>But that doesn&apos;t explain what you then *do* with the ratio in<br/>Barlow&apos;s Harmonicity, which doesn&apos;t seem related to how we perceive<br/>consonance, especially in that it assumes that simple ratios have<br/>zero tolerance for mistuning, and can&apos;t even accept irrational<br/>inputs! This substitution you mention, to the extent that it&apos;s valid,<br/>really has little to do with the rest of Barlow&apos;s formulation,<br/>correct?</p><p>Best,<br/>Paul</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            