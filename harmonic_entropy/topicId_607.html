<a href="/harmonic_entropy">back to list</a><h1>HE algorithms</h1><h3><a id=607 href="#607">ðŸ”—</a>John Chalmers &#x3C;JHCHALMERS@...&#x3E;</h3><span>7/7/2002 5:49:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve had a few free days this week and wanted to program the harmonic<br/>entropy algorithm(s) in TrueBasic. Alas, I couldn&apos;t find them in the<br/>list archives; can someone post them or email them to me?</p><p>--John</p></div><h3><a id=608 href="#608">ðŸ”—</a>manuel.op.de.coul@...</h3><span>7/8/2002 9:07:38 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>John,</p><p>I&apos;ve posted my code to this list on 1 Feb. 2001.<br/>For the error function I use an approximation, this is the code:</p><p>   function Erf (Of_Number : Long_Float) return Long_Float is</p><p>      function Norm (Zin : in Long_Float) return Long_Float is<br/>         P : Long_Float;<br/>         Z : constant Long_Float := abs Zin;<br/>      begin<br/>         P := 1.0 + Z * (0.04986735 + Z * (0.02114101 + Z * (0.00327763 +<br/>           Z * (0.0000380036 + Z * (0.0000488906 + Z * 0.000005383)))));<br/>         P := P * P; P := P * P; P := P * P;<br/>         if Zin &gt;= 0.0 then<br/>            return 1.0 - 0.5 / (P * P);<br/>         else<br/>            return 0.5 / (P * P);<br/>         end if;<br/>      end Norm;</p><p>   begin<br/>      if Of_Number &gt;= 5.0 then<br/>         return 1.0;<br/>      elsif Of_Number &lt;= -5.0 then<br/>         return 0.0;<br/>      else<br/>         return Norm(Of_Number);<br/>      end if;<br/>   end Erf;</p><p>Manuel</p></div><h3><a id=611 href="#611">ðŸ”—</a>emotionaljourney22 &#x3C;paul@...&#x3E;</h3><span>7/8/2002 4:14:29 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In harmonic_entropy@y..., manuel.op.de.coul@e... wrote:</p><p>&gt; John,<br/>&gt;<br/>&gt; I&apos;ve posted my code to this list on 1 Feb. 2001.</p><p>thanks manuel -- john, let us know if this can be clarified. i&apos;ve<br/>also made a number of posts that sketch, conceptually, how harmonic<br/>entropy is calculated -- these should be helpful.</p><p>certain things, though, have developed since the code manuel posted.</p><p>firstly, i don&apos;t remember when the last time the issue was discussed,<br/>but the issue of approximating the error integral would rarely be<br/>important -- a simple rectangular approximation, where the width is<br/>the mediant-to-mediant distance and the height is measured at the<br/>candidate ratio itself, should normally be fine.</p><p>secondly, the farey limit has been almost entirely superceded by the<br/>tenney limit (n*d), for which i&apos;ve typically been using n*d &lt; 65536<br/>or n*d &lt; 10000 (there&apos;s rather little difference between the two).<br/>the reason is that the farey series, and other series that were tried<br/>(all of which have superparticular &quot;steps&quot; [actually ratios between<br/>adjacent interval sizes]), lead to a harmonic entropy curve with an<br/>overall &quot;trend&quot;. i observed, however, that regardless of the series<br/>used and the trend observed, the local minima had a &quot;strength&quot; or<br/>prominence that agreed very closely with an n*d ranking. when i tried<br/>a tenney series (all ratios with n*d below a certain limit), which<br/>also has superparticular &quot;steps&quot;, the same harmonic entropy curve<br/>showed up but without the &quot;trend&quot; -- i.e., the overall &quot;trend&quot; was<br/>flat.</p><p>next, the 1/sqrt(n*d) approximation is often used for the &quot;widths&quot; in<br/>conjuction with the tenney seeding above. the &quot;widths&quot;<br/>are &quot;traditionally&quot; computed as mediant-to-mediant distances. i<br/>observed (and it should be easy to prove) that, except for the most<br/>complex ratios near the tenney limit, the mediant-to-mediant distance<br/>surrounding each ratio n/d is approximately proportional to 1/sqrt<br/>(n*d). any tenney limit is an arbitrary and artifical stopping point,<br/>and if low enough this arbitrary choice will create visible artifacts<br/>on the curve. using the 1/sqrt(n*d) tends to minimize the effects of<br/>these artifacts. i&apos;ve posted graphs that directly compare the results<br/>of using this approximation vs. using the actual mediant-to-mediant<br/>widths -- those graphs can be found in the files section of this list<br/>and/or the tuning list.</p><p>(then there&apos;s the unreduced fraction variation, which i&apos;ll skip over<br/>for now)</p><p>finally, the vos curve. i posted a summary of some of joos vos&apos;s<br/>research to the tuning list -- basically he found that the &quot;purity&quot;<br/>rating for harmonic intervals, even with sine waves, follows an<br/>absolute exponential, rather than bell, curve (with the peak, of<br/>course, at the just ratio). it&apos;s easy enough to incorporate this idea<br/>into the harmonic entropy function by using a probability function of<br/>the form exp(-|r|/s) instead of exp(-r^2/s). there may be some<br/>justification for this in the neurological timing mechanisms for<br/>pitch perception that have come to the forefront in the last 20<br/>years. the resulting curves seem to be fractal-like; there appears to<br/>be a infinitude of ever-finer local minima at more and more complex<br/>ratios, but only big minima, corresponding as always to the simplest<br/>ratios, are visible. the results have seemed to provide a much more<br/>satisfying dissonance function for various listeners such as margo,<br/>gene, and george.</p><p>here&apos;s the matlab code i&apos;m using to calculate the latest curves,<br/>which are tenney-seeded, 1/sqrt(n*d)-width-approximated, vos-curve<br/>entropy functions:</p><p>function entropy=t2vosinp(cents,r,s);<br/>in=cents/1200*log(2);<br/>a=size(r,1);<br/>p=r(:,1).*exp(-abs((in-log(r(:,2)./r(:,3)))/s));<br/>n=sum(p);<br/>p=p/n;<br/>p(find(p==0))=ones(size(find(p==0)));<br/>entropy=-sum(p.*log(p))&apos;;</p><p>the input arguments are cents (the independent variable which is the<br/>actual interval in cents), s (the hearing resolution, usually .01 for<br/>a 1% hearing resolution), and the seed-ratio array r. r has three<br/>columns: &quot;width&quot;, numerator, and denominator. for example, for the<br/>latest graph i produced for george, which was seeded with tenney<br/>limit n*d &lt;=10000, r was an array with 63868 rows. here are the first<br/>10 rows:</p><p>0.01                         1                     10000<br/>0.0100005000375031                         1                      9999<br/>0.010001000150025                         1                      9998<br/>0.0100015003375844                         1                      9997<br/>0.0100020006002001                         1                      9996<br/>0.0100025009378908                         1                      9995<br/>0.0100030013506754                         1                      9994<br/>0.0100035018385725                         1                      9993<br/>0.0100040024016011                         1                      9992<br/>0.0100045030397799                         1                      9991</p><p>clear? this itself only required a few lines of code to generate. so<br/>altogether, not much is required in the way of coding, and the entire<br/>harmonic entropy function could probably be written as a single-line<br/>expression if you have fancy enough symbology for taking sums over a<br/>pair of indices which are restricted to be mutually prime, etc. . . .</p><p>happy computing!</p></div>