<a href="/tuning-math">back to list</a><h1>Summing up the Convolution-HE stuff</h1><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 5:06:07 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Let me make sure that I have this right before I go any further:</p><p>- The rationals numbers are uniformly distributed if you integrate<br/>with respect to dx.<br/>- The fact that the Farey/Tenney numbers are NOT uniformly distributed<br/>if you integrate with respect to dx are due to the fact that these<br/>series are SUBSETS of the rationals, and as such their distribution<br/>isn&apos;t going to be uniform:<br/><a href="http://linas.org/math/chap-rat/chap-rat.html">http://linas.org/math/chap-rat/chap-rat.html</a><br/>- Therefore, when we say that intervals have 1/d widths or 1/sqrt(n*d)<br/>widths, we&apos;re talking about their widths with respect to some series,<br/>NOT with respect to the actual distribution of the rationals,<br/>integrated with respect to dc.<br/>- How uniform the Farey numbers end up getting seems to be subject to<br/>the Riemann hypothesis, and although this is really interesting, in<br/>light of the above, it doesn&apos;t really matter for this model.<br/>- I have proven that as N goes to infinity, HE is better and better<br/>modeled using the convolution of a Gaussian and a bunch of impulses:<br/><a href="http://www.mikebattagliamusic.com/music/HEConvolutionTheorem.html">http://www.mikebattagliamusic.com/music/HEConvolutionTheorem.html</a></p><p>So in light of this, my DC model was the same thing as HE all along.<br/>By setting the impulses in DC as having different heights, you can<br/>approximate the behavior in the limit of different subsets of the<br/>rationals (series, enumerators, whatever). They&apos;re just two related<br/>models that happen to converge at infinity, or rather they may or may<br/>not converge because of some weird facet of number theory that<br/>involves the Riemann hypothesis, but who cares. So I&apos;m not going to<br/>call it DC anymore, I&apos;m just going to call it HE from now on.</p><p>So this is the way forward then:<br/>1) Center an impulse around each rational.<br/>2) Give these impulses heights that make some kind of logical sense.<br/>If you want to go with the Farey series, give them 1/d heights and you<br/>get Thomae&apos;s function. If you like the way the Tenney series looks,<br/>give them 1/sqrt(n*d) heights.<br/>3) Convolve with a Gaussian.</p><p>I have opted to give them 1/n*d heights. This is because I left<br/>something out about #2 - to actually make it equivalent to Paul&apos;s HE<br/>calculation, you don&apos;t actually give them 1/sqrt(n*d) heights, you<br/>give them something like N/sqrt(n*d) * log(N/sqrt(n*d)) heights. This<br/>is complicated and, since it&apos;s all pretty much arbitrary, and this is<br/>basically still a scaled version of 1/n*d, why not just do that? I<br/>think that 1/log(n*d+1) might be a better estimate for what&apos;s going<br/>on, but might as well keep it simple.</p><p>If you do that, you get this, with all of the minima and maxima labeled:</p><p><a href="http://www.mikebattagliamusic.com/music/HEs1.0ndN40.png">http://www.mikebattagliamusic.com/music/HEs1.0ndN40.png</a></p><p>Keep in mind - I&apos;m using the n*d approximation, which is more related<br/>to the Tenney series widths, but I&apos;m using the Farey series to<br/>actually seed the entropy calculation. That is - I&apos;m using the Farey<br/>series to come up with the list of rationals that I&apos;m going to use,<br/>and then I&apos;m giving them Tenney-ish heights. In this case, it doesn&apos;t<br/>matter what series you use, so long as it generates rationals<br/>relatively evenly - what matters is the heights you choose. The Farey<br/>series can be calculated in real time, so that&apos;s good.</p><p>The same shoudl apply to the triadic case - the whole process of<br/>coming up with an enumerator for the triads and then giving them<br/>Voronoi cells will, in and of itself, cause a particular pattern of<br/>&quot;heights&quot; for the triads; however, the triads should themselves be<br/>just as evenly distributed as the rationals. Rather than waste time<br/>playing with different series to find the most sensible looking one,<br/>we can just work backwards and pick sensible heights to begin with.</p><p>So let&apos;s run everything above through the good ol Lumma shredder, and<br/>assuming it passes that acid test, let&apos;s move onto triads and then,<br/>finally, tetrads.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 5:19:21 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 8:06 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; - Therefore, when we say that intervals have 1/d widths or 1/sqrt(n*d)<br/>&gt; widths, we&apos;re talking about their widths with respect to some series,<br/>&gt; NOT with respect to the actual distribution of the rationals,<br/>&gt; integrated with respect to dc.</p><p>Typo - meant with respect to dc here.</p><p>Also, one last thing: if everyone is in agreement that this is a<br/>sensible way to proceed - to try and develop a well-behaved function<br/>that generalizes what HE is doing, rather than delving into the annals<br/>of number theory to study the behavior of the rationals at infinite<br/>scales - and if I haven&apos;t missed anything major and stupid, then all<br/>of this can be done really quickly!</p><p>I keep talking about some mysterious speedup, and I&apos;ve sent this to<br/>Carl offlist already, but the idea is that by doing this, we&apos;re<br/>basically &quot;Gaussian blurring&quot; the impulses in 1d space rather than 2d<br/>space. Gaussian blurring = low pass filtering, so all of the high<br/>frequencies get shut down. &quot;Get shut down&quot; is a statement that can be<br/>made mathematically rigorous by looking at the magnitude response of<br/>the FFT of the computed entropy curve.</p><p>Since this is the case, we can compute HE at like 10 cent resolution<br/>and then use sinc interpolation to recover the original curve with<br/>something like 0.01% error; whatever the exact number is, I&apos;ve<br/>overlaid the two on top of each other without seeing any visible<br/>difference.</p><p>It is also the case that with this approach, it no longer becomes<br/>necessary to use Farey series of N=80, 90, etc, since we&apos;re<br/>pre-programming the heights into the equation. Empirically speaking,<br/>N=40 seems to work really well, and N=20 seems to work well enough<br/>except for some exaggerated entropy spikes near the maxima around 1/1,<br/>3/2, etc. So this will make life easy for triads and tetrads, where<br/>the computational complexity of all of this will really take off.</p><p>I&apos;ll go further in detail once I get the first part worked out.</p><p>-Mike</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 11:06:31 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; Let me make sure that I have this right before I go any further:<br/>&gt;<br/>&gt; - The rationals numbers are uniformly distributed if you integrate<br/>&gt; with respect to dx.<br/>&gt; - The fact that the Farey/Tenney numbers are NOT uniformly distributed<br/>&gt; if you integrate with respect to dx are due to the fact that these<br/>&gt; series are SUBSETS of the rationals, and as such their distribution<br/>&gt; isn&apos;t going to be uniform:</p><p>I wouldn&apos;t put it like that. We get a uniform measure dx if we use the equal divisions En of the interval you used in email. That is,<br/>[0 1/n 2/n ... n/n], and take the limit. This is probably best done by way of powers, so they fit inside each other, as for instance with the dyadic rationals. We get another measure, d?, if we take a limit instead of Farey sequences. You can call either measure &quot;correct&quot;; it&apos;s just a matter of how you define the measure; however, dx makes the most sense for the reals and hence the most sense in general. You wouldn&apos;t want to do geometry with a d? measure.</p><p>&gt; The same shoudl apply to the triadic case - the whole process of<br/>&gt; coming up with an enumerator for the triads and then giving them<br/>&gt; Voronoi cells will, in and of itself, cause a particular pattern of<br/>&gt; &quot;heights&quot; for the triads; however, the triads should themselves be<br/>&gt; just as evenly distributed as the rationals.</p><p>Good! Sounds worth pursuing.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 11:09:45 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; Also, one last thing: if everyone is in agreement that this is a<br/>&gt; sensible way to proceed - to try and develop a well-behaved function<br/>&gt; that generalizes what HE is doing, rather than delving into the annals<br/>&gt; of number theory to study the behavior of the rationals at infinite<br/>&gt; scales - and if I haven&apos;t missed anything major and stupid, then all<br/>&gt; of this can be done really quickly!</p><p>I&apos;ve always thought that was the way to go, since I didn&apos;t know why you were delving into those things anyway.</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/26/2011 11:13:54 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;So let&apos;s run everything above through the good ol Lumma shredder, and<br/>&gt;assuming it passes that acid test, let&apos;s move onto triads and then,<br/>&gt;finally, tetrads.</p><p>The Lumma shredder likes tabular data.  1-cent increments, s=1% please.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 12:43:06 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 8:19 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; On Wed, Jan 26, 2011 at 8:06 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;&gt; - Therefore, when we say that intervals have 1/d widths or 1/sqrt(n*d)<br/>&gt;&gt; widths, we&apos;re talking about their widths with respect to some series,<br/>&gt;&gt; NOT with respect to the actual distribution of the rationals,<br/>&gt;&gt; integrated with respect to dc.<br/>&gt;<br/>&gt; Typo - meant with respect to dc here.</p><p>Wow, I can&apos;t believe I just did this. With respect to dc, I mean. Just<br/>kidding. dx.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 12:56:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 2:06 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; I wouldn&apos;t put it like that. We get a uniform measure dx if we use the equal divisions En of the interval you used in email. That is,<br/>&gt; [0 1/n 2/n ... n/n], and take the limit. This is probably best done by way of powers, so they fit inside each other, as for instance with the dyadic rationals. We get another measure, d?, if we take a limit instead of Farey sequences. You can call either measure &quot;correct&quot;; it&apos;s just a matter of how you define the measure; however, dx makes the most sense for the reals and hence the most sense in general. You wouldn&apos;t want to do geometry with a d? measure.</p><p>What do you mean if we take a limit instead of Farey sequences? A limit of what?</p><p>Would it be more correct to say the widths of different subsets of the<br/>rationals, with respect to the dx measure specifically, vary depending<br/>on the type of subset used? e.g. the Farey series has a different set<br/>of widths than the Tenney series, and they both have a different set<br/>of widths than the Stern-Brocot tree, etc.</p><p>In general I think I could devise a series with any set of widths I<br/>wanted, and just as long as for two consecutive ratios a/b and c/d,<br/>ad-bc = 1, we&apos;ll get unreduced fractions. All of these series are<br/>basically subsets of the Stern-Brocot tree anyway that are distributed<br/>more evenly. Maybe a good exercise would be to find a series in which<br/>more complex fractions have a larger width than more simple ones, but<br/>where ad-bc still equals 1. Anyways...</p><p>&gt; &gt; Also, one last thing: if everyone is in agreement that this is a<br/>&gt; &gt; sensible way to proceed - to try and develop a well-behaved function<br/>&gt; &gt; that generalizes what HE is doing, rather than delving into the annals<br/>&gt; &gt; of number theory to study the behavior of the rationals at infinite<br/>&gt; &gt; scales - and if I haven&apos;t missed anything major and stupid, then all<br/>&gt; &gt; of this can be done really quickly!<br/>&gt;<br/>&gt; I&apos;ve always thought that was the way to go, since I didn&apos;t know why you were delving into those things anyway.</p><p>Partly at Paul&apos;s behest; he thought that what I was doing was more or<br/>less an attempt to generate a &quot;curve of a certain shape&quot; and wasn&apos;t as<br/>deep as the actual HE model, which he claims spontanteously predicts<br/>the complexity of intervals due to the distribution of the rationals.<br/>As it seemed like we were saying above, this isn&apos;t true; it really<br/>reflects the distribution of the Farey numbers, and/or the Tenney<br/>numbers, which is why you can get 1/d widths for one series and<br/>1/sqrt(n*d) widths for another. I don&apos;t know if Paul was referring to<br/>using ?(x) as a measure or if he&apos;d really explored it, but in general,<br/>it seems like he was mistaken about this. Maybe a different way to<br/>formalize it would have been to integrate the Gaussian in his model<br/>with respect to ?(x).</p><p>Now that I&apos;ve worked out all the math, I&apos;m pretty confident in saying<br/>that I&apos;m doing something that really is harmonic entropy, or related<br/>to harmonic entropy.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 1:05:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 3:56 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; In general I think I could devise a series with any set of widths I<br/>&gt; wanted, and just as long as for two consecutive ratios a/b and c/d,<br/>&gt; ad-bc = 1, we&apos;ll get unreduced fractions. All of these series are<br/>&gt; basically subsets of the Stern-Brocot tree anyway that are distributed<br/>&gt; more evenly. Maybe a good exercise would be to find a series in which<br/>&gt; more complex fractions have a larger width than more simple ones, but<br/>&gt; where ad-bc still equals 1. Anyways...</p><p>Actually, in light of this, a good way to go would be to find a series<br/>that is as evenly distributed as possible, but is still a unimodular<br/>series that passes through all of the simple ratios first. This would<br/>make for the greatest ease of computing. Since we&apos;re going to<br/>artificially seed their heights with n*d anyway, we actually want an<br/>evenly distributed series anyway.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 1:28:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 2:13 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;So let&apos;s run everything above through the good ol Lumma shredder, and<br/>&gt; &gt;assuming it passes that acid test, let&apos;s move onto triads and then,<br/>&gt; &gt;finally, tetrads.<br/>&gt;<br/>&gt; The Lumma shredder likes tabular data. 1-cent increments, s=1% please.<br/>&gt;<br/>&gt; -Carl</p><p>OK, here&apos;s 3 examples:<br/><a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/</a></p><p>These correspond to Farey series of N=30, 40, and 80. Keep in mind<br/>that I&apos;m although I&apos;m using the Farey series to seed the model, since<br/>it can be computed really quickly, I have the model set up so that the<br/>impulses have Tenney-ish heights.</p><p>Keep in mind when I send this to you that, last time I did this, s=1%<br/>in the impulse model corresponded to I think something like s=1.2% in<br/>HE. After looking at the equations it&apos;s probably because of what<br/>happens when you multiply G(d) .*log(G(d)). When you check this,<br/>compare to s=1.2% in HE as well. Then we&apos;ll find a way to normalize<br/>it.</p><p>-Mike</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 1:30:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; What do you mean if we take a limit instead of Farey sequences? A limit of what?</p><p>A limit of the Farey sequences. If you treat the successive members of the Farey sequence of order n and go to the limit, you get a measure distinct from the usual one. If you are integrating f(x) on the unit interval, you take a limit the average of f evaluated at the Farey fractions over [0 1]. That&apos;s not the usual measure.</p><p>&gt; Would it be more correct to say the widths of different subsets of the<br/>&gt; rationals, with respect to the dx measure specifically, vary depending<br/>&gt; on the type of subset used? e.g. the Farey series has a different set<br/>&gt; of widths than the Tenney series, and they both have a different set<br/>&gt; of widths than the Stern-Brocot tree, etc.</p><p>Not sure what you are saying here.</p><p>&gt; Partly at Paul&apos;s behest; he thought that what I was doing was more or<br/>&gt; less an attempt to generate a &quot;curve of a certain shape&quot; and wasn&apos;t as<br/>&gt; deep as the actual HE model, which he claims spontanteously predicts<br/>&gt; the complexity of intervals due to the distribution of the rationals.</p><p>I&apos;ve never been convinced of that. And I&apos;d still like to see what you get by taking the derivative of various Weierstass transforms of ?. One difference we can predict right off is that it is periodic, assigning the same octave-equivalent numbers to a ratio, its inversion, and its octave translates. That, I am thinking, could be useful.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 1:32:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; What do you mean if we take a limit instead of Farey sequences? A limit of what?</p><p>A limit of the Farey sequences. If you treat the successive members of the Farey sequence of order n and go to the limit, you get a measure distinct from the usual one. If you are integrating f(x) on the unit interval, you take a limit the average of f evaluated at the Farey fractions over [0 1]. That&apos;s not the usual measure.</p><p>&gt; Would it be more correct to say the widths of different subsets of the<br/>&gt; rationals, with respect to the dx measure specifically, vary depending<br/>&gt; on the type of subset used? e.g. the Farey series has a different set<br/>&gt; of widths than the Tenney series, and they both have a different set<br/>&gt; of widths than the Stern-Brocot tree, etc.</p><p>Not sure what you are saying here.</p><p>&gt; Partly at Paul&apos;s behest; he thought that what I was doing was more or<br/>&gt; less an attempt to generate a &quot;curve of a certain shape&quot; and wasn&apos;t as<br/>&gt; deep as the actual HE model, which he claims spontanteously predicts<br/>&gt; the complexity of intervals due to the distribution of the rationals.</p><p>I&apos;ve never been convinced of that. And I&apos;d still like to see what you get by taking the derivative of various Weierstass transforms of ?. One difference we can predict right off is that it is periodic, assigning the same octave-equivalent numbers to a ratio, its inversion, and its octave translates. That, I am thinking, could be useful.</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 1:39:40 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 4:30 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt; What do you mean if we take a limit instead of Farey sequences? A limit of what?<br/>&gt;<br/>&gt; A limit of the Farey sequences. If you treat the successive members of the Farey sequence of order n and go to the limit, you get a measure distinct from the usual one. If you are integrating f(x) on the unit interval, you take a limit the average of f evaluated at the Farey fractions over [0 1]. That&apos;s not the usual measure.</p><p>OK, I see. So then this is just a more formal way to say what I&apos;ve<br/>been trying to say: while the Farey numbers do converge on the<br/>rationals, their widths with respect to dx don&apos;t converge on the<br/>widths of the rationals with respect to dx. Rather, if you use the<br/>limit of the widths of the Farey sequences, you simply get one<br/>measure, and it&apos;s this measure that actually generates the behavior of<br/>the HE curve. If you use the limit of the widths of the Tenney<br/>sequence, you get a different measure, and this measure generates a<br/>better looking HE curve. If you use the limit of the widths of the<br/>Stern-Brocot tree, you get yet another measure, etc.</p><p>&gt; &gt; Partly at Paul&apos;s behest; he thought that what I was doing was more or<br/>&gt; &gt; less an attempt to generate a &quot;curve of a certain shape&quot; and wasn&apos;t as<br/>&gt; &gt; deep as the actual HE model, which he claims spontanteously predicts<br/>&gt; &gt; the complexity of intervals due to the distribution of the rationals.<br/>&gt;<br/>&gt; I&apos;ve never been convinced of that.</p><p>But it does spontaneously predict the complexity of intervals due to<br/>the distribution of the Farey numbers, right? Or whatever series you<br/>decide to use.</p><p>&gt; And I&apos;d still like to see what you get by taking the derivative of various Weierstass transforms of ?. One difference we can predict right off is that it is periodic, assigning the same octave-equivalent numbers to a ratio, its inversion, and its octave translates. That, I am thinking, could be useful.</p><p>I need to find an algorithm to compute it. I found a MATLAB program to<br/>do it and spent a long time on it, and couldn&apos;t get it to work<br/>satisfactorily. I think the problem is that if I&apos;m in MATLAB and I<br/>compute ?(x) for 0:0.1:1200, I end up basically computing it for only<br/>rationals. So I need some function that converges to ?(x) that I can<br/>use.</p><p>-Mike</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 1:57:53 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; I need to find an algorithm to compute it. I found a MATLAB program to<br/>&gt; do it and spent a long time on it, and couldn&apos;t get it to work<br/>&gt; satisfactorily. I think the problem is that if I&apos;m in MATLAB and I<br/>&gt; compute ?(x) for 0:0.1:1200, I end up basically computing it for only<br/>&gt; rationals. So I need some function that converges to ?(x) that I can<br/>&gt; use.</p><p>Do you have a routine for computing the continued fraction of a floating point number? Starting from that, it&apos;s easy to compute ?(x).</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 2:01:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 4:57 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; &gt; I need to find an algorithm to compute it. I found a MATLAB program to<br/>&gt; &gt; do it and spent a long time on it, and couldn&apos;t get it to work<br/>&gt; &gt; satisfactorily. I think the problem is that if I&apos;m in MATLAB and I<br/>&gt; &gt; compute ?(x) for 0:0.1:1200, I end up basically computing it for only<br/>&gt; &gt; rationals. So I need some function that converges to ?(x) that I can<br/>&gt; &gt; use.<br/>&gt;<br/>&gt; Do you have a routine for computing the continued fraction of a floating point number? Starting from that, it&apos;s easy to compute ?(x).</p><p>I do, but like I said, all of the numbers we use are going to be<br/>rational. Let&apos;s say we go between 0 and 1 with a resolution of 0.0001<br/>- the best you&apos;re going to do is get a rational number that&apos;s 4<br/>decimal places out. Is that still acceptable?</p><p>-Mike</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 2:45:06 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; I do, but like I said, all of the numbers we use are going to be<br/>&gt; rational. Let&apos;s say we go between 0 and 1 with a resolution of 0.0001<br/>&gt; - the best you&apos;re going to do is get a rational number that&apos;s 4<br/>&gt; decimal places out. Is that still acceptable?</p><p>Since the dyadic rational this computes is precisely ?(x) for that value, of course.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 2:52:17 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; I do, but like I said, all of the numbers we use are going to be<br/>&gt; rational. Let&apos;s say we go between 0 and 1 with a resolution of 0.0001<br/>&gt; - the best you&apos;re going to do is get a rational number that&apos;s 4<br/>&gt; decimal places out. Is that still acceptable?</p><p>I could send you a list of ?(i/10000) for i from 1 to 10000 if that would help.</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 2:54:17 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>That would be nice. Can you put it in a CSV?</p><p>-Mike</p><p>On Wed, Jan 26, 2011 at 5:52 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; &gt; I do, but like I said, all of the numbers we use are going to be<br/>&gt; &gt; rational. Let&apos;s say we go between 0 and 1 with a resolution of 0.0001<br/>&gt; &gt; - the best you&apos;re going to do is get a rational number that&apos;s 4<br/>&gt; &gt; decimal places out. Is that still acceptable?<br/>&gt;<br/>&gt; I could send you a list of ?(i/10000) for i from 1 to 10000 if that would help.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 3:03:31 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; That would be nice. Can you put it in a CSV?</p><p>I don&apos;t really know the specification of a CSV, but Maple output will simply consist of values separated by commas, and I could put each value on its own separate line, with or without terminating commas. How many digits of accuracy?</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 3:04:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; That would be nice. Can you put it in a CSV?</p><p>I don&apos;t really know the specification of a CSV, but Maple output will simply consist of values separated by commas, and I could put each value on its own separate line, with or without terminating commas. How many digits of accuracy?</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 3:07:44 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 6:03 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; That would be nice. Can you put it in a CSV?<br/>&gt;<br/>&gt; I don&apos;t really know the specification of a CSV, but Maple output will simply consist of values separated by commas, and I could put each value on its own separate line, with or without terminating commas. How many digits of accuracy?</p><p>CSV stands for &quot;comma-separated values,&quot; so there you go. How many<br/>digits of accuracy - as many as you can cram into the file, doesn&apos;t<br/>matter here.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 3:08:45 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Actually, do to this right, could you do all of the above for<br/>?(log_2(x)) instead? The thing is that we&apos;re convolving in log space,<br/>not linear space.</p><p>-Mike</p><p>On Wed, Jan 26, 2011 at 6:07 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; On Wed, Jan 26, 2011 at 6:03 PM, genewardsmith<br/>&gt; &lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt; That would be nice. Can you put it in a CSV?<br/>&gt;&gt;<br/>&gt;&gt; I don&apos;t really know the specification of a CSV, but Maple output will simply consist of values separated by commas, and I could put each value on its own separate line, with or without terminating commas. How many digits of accuracy?<br/>&gt;<br/>&gt; CSV stands for &quot;comma-separated values,&quot; so there you go. How many<br/>&gt; digits of accuracy - as many as you can cram into the file, doesn&apos;t<br/>&gt; matter here.<br/>&gt;<br/>&gt; -Mike<br/>&gt;</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 3:18:29 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 6:08 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; Actually, do to this right, could you do all of the above for<br/>&gt; ?(log_2(x)) instead? The thing is that we&apos;re convolving in log space,<br/>&gt; not linear space.</p><p>Or I guess that it would be ?(2^x). Agh. But this brings up another<br/>point, which is - how do we work this out?</p><p>So check out Thomae&apos;s function, which I keep coming back to because of<br/>its being the projective version of almost the Lambdoma:</p><p><a href="http://en.wikipedia.org/wiki/Euclid%27s_orchard">http://en.wikipedia.org/wiki/Euclid%27s_orchard</a></p><p>This is just between 0 and 1. So we want to map the intervals between<br/>[0,1] to the intervals between [-Inf, Inf]? So I guess we&apos;d have to<br/>set up some kind of hyperbolic curve for this?</p><p>-Mike</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 3:32:36 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; Actually, do to this right, could you do all of the above for<br/>&gt; ?(log_2(x)) instead? The thing is that we&apos;re convolving in log space,<br/>&gt; not linear space.</p><p>Can we do both? I&apos;m more interested in the results for linear space.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 3:48:44 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; So check out Thomae&apos;s function, which I keep coming back to because of<br/>&gt; its being the projective version of almost the Lambdoma:<br/>&gt;<br/>&gt; <a href="http://en.wikipedia.org/wiki/Euclid%27s_orchard">http://en.wikipedia.org/wiki/Euclid%27s_orchard</a><br/>&gt;<br/>&gt; This is just between 0 and 1. So we want to map the intervals between<br/>&gt; [0,1] to the intervals between [-Inf, Inf]? So I guess we&apos;d have to<br/>&gt; set up some kind of hyperbolic curve for this?</p><p>Well, hell. Sure, tanh would work, but why all this complication? I really think ?(x) itself is the most interesting: it&apos;s octave equivalent by nature.</p></div><h3>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>1/26/2011 4:39:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;genewardsmith&quot; &lt;genewardsmith@...&gt; wrote:</p><p>&gt; Can we do both? I&apos;m more interested in the results for linear space.<br/>&gt;</p><p>Ah hell, what am I saying? It won&apos;t work for an OE score. I&apos;ll send something shortly.</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/26/2011 4:42:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Jan 26, 2011 at 6:32 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Actually, do to this right, could you do all of the above for<br/>&gt; &gt; ?(log_2(x)) instead? The thing is that we&apos;re convolving in log space,<br/>&gt; &gt; not linear space.<br/>&gt;<br/>&gt; Can we do both? I&apos;m more interested in the results for linear space.</p><p>Sure, but the result won&apos;t be the same as harmonic entropy. The log<br/>space convolution is what produces the beauty of the model. Otherwise<br/>you would end up with a result whereby wider intervals are more<br/>sensitive to mistuning than narrower intervals. I don&apos;t think this is<br/>true, but there might be a correlation between more -complex-<br/>intervals being more sensitive to mistuning than narrower intervals.<br/>Or less sensitive. But I don&apos;t think wideness has anything to do with<br/>it.</p><p>If you send me both we can see what the model spits out anyways.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/27/2011 1:49:30 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;OK, here&apos;s 3 examples:<br/>&gt;<a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/</a><br/>&gt;<br/>&gt;These correspond to Farey series of N=30, 40, and 80. Keep in mind<br/>&gt;that I&apos;m although I&apos;m using the Farey series to seed the model, since<br/>&gt;it can be computed really quickly, I have the model set up so that the<br/>&gt;impulses have Tenney-ish heights.<br/>&gt;<br/>&gt;Keep in mind when I send this to you that, last time I did this, s=1%<br/>&gt;in the impulse model corresponded to I think something like s=1.2% in<br/>&gt;HE. After looking at the equations it&apos;s probably because of what<br/>&gt;happens when you multiply G(d) .*log(G(d)). When you check this,<br/>&gt;compare to s=1.2% in HE as well. Then we&apos;ll find a way to normalize<br/>&gt;it.</p><p>Here&apos;s a plot</p><p><a href="http://i.min.us/ibTr3W.png">http://i.min.us/ibTr3W.png</a></p><p>going to bed... more tomorrow.  -C.</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/27/2011 1:07:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jan 27, 2011 at 4:49 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Here&apos;s a plot<br/>&gt;<br/>&gt; <a href="http://i.min.us/ibTr3W.png">http://i.min.us/ibTr3W.png</a><br/>&gt;<br/>&gt; going to bed... more tomorrow. -C.</p><p>What is it, the difference between this and normal HE?</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/27/2011 1:40:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 01:07 PM 1/27/2011, you wrote:<br/>&gt;On Thu, Jan 27, 2011 at 4:49 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; Here&apos;s a plot<br/>&gt;&gt;<br/>&gt;&gt; <a href="http://i.min.us/ibTr3W.png">http://i.min.us/ibTr3W.png</a><br/>&gt;&gt;<br/>&gt;&gt; going to bed... more tomorrow. -C.<br/>&gt;<br/>&gt;What is it, the difference between this and normal HE?</p><p>One plotted against the other, yep.  -Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/27/2011 2:17:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jan 27, 2011 at 4:40 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; At 01:07 PM 1/27/2011, you wrote:<br/>&gt; &gt;<br/>&gt; &gt;What is it, the difference between this and normal HE?<br/>&gt;<br/>&gt; One plotted against the other, yep. -Carl</p><p>I would assume that stems from this:</p><p>&gt; I have opted to give them 1/n*d heights. This is because I left<br/>&gt; something out about #2 - to actually make it equivalent to Paul&apos;s HE<br/>&gt; calculation, you don&apos;t actually give them 1/sqrt(n*d) heights, you<br/>&gt; give them something like N/sqrt(n*d) * log(N/sqrt(n*d)) heights. This<br/>&gt; is complicated and, since it&apos;s all pretty much arbitrary, and this is<br/>&gt; basically still a scaled version of 1/n*d, why not just do that? I<br/>&gt; think that 1/log(n*d+1) might be a better estimate for what&apos;s going<br/>&gt; on, but might as well keep it simple.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/27/2011 11:21:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jan 27, 2011 at 5:17 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; On Thu, Jan 27, 2011 at 4:40 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; At 01:07 PM 1/27/2011, you wrote:<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;What is it, the difference between this and normal HE?<br/>&gt;&gt;<br/>&gt;&gt; One plotted against the other, yep. -Carl<br/>&gt;<br/>&gt; I would assume that stems from this:<br/>&gt;<br/>&gt;&gt; I have opted to give them 1/n*d heights. This is because I left<br/>&gt;&gt; something out about #2 - to actually make it equivalent to Paul&apos;s HE<br/>&gt;&gt; calculation, you don&apos;t actually give them 1/sqrt(n*d) heights, you<br/>&gt;&gt; give them something like N/sqrt(n*d) * log(N/sqrt(n*d)) heights. This<br/>&gt;&gt; is complicated and, since it&apos;s all pretty much arbitrary, and this is<br/>&gt;&gt; basically still a scaled version of 1/n*d, why not just do that? I<br/>&gt;&gt; think that 1/log(n*d+1) might be a better estimate for what&apos;s going<br/>&gt;&gt; on, but might as well keep it simple.</p><p>So what are you getting at here, exactly? That this entire approach is<br/>invalid unless I work out the Tenney series heights?</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/27/2011 11:40:56 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>What you have doesn&apos;t seem to correspond to harmonic entropy,<br/>or exp(entropy), etc.  That&apos;s what you claimed.  That&apos;s all I&apos;m<br/>saying, no more no less.   -Carl</p><p>&gt;So what are you getting at here, exactly? That this entire approach is<br/>&gt;invalid unless I work out the Tenney series heights?<br/>&gt;<br/>&gt;-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/27/2011 11:54:12 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Jan 28, 2011 at 2:40 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; What you have doesn&apos;t seem to correspond to harmonic entropy,<br/>&gt; or exp(entropy), etc. That&apos;s what you claimed. That&apos;s all I&apos;m<br/>&gt; saying, no more no less. -Carl</p><p>I never claimed that the 3 examples I just posted were supposed to be<br/>1-for-1 equivalent to harmonic entropy. In fact, my specific claim was<br/>this:</p><p>&gt; I have opted to give them 1/n*d heights. This is because I left<br/>&gt; something out about #2 - to actually make it equivalent to Paul&apos;s HE<br/>&gt; calculation, you don&apos;t actually give them 1/sqrt(n*d) heights, you<br/>&gt; give them something like N/sqrt(n*d) * log(N/sqrt(n*d)) heights. This<br/>&gt; is complicated and, since it&apos;s all pretty much arbitrary, and this is<br/>&gt; basically still a scaled version of 1/n*d, why not just do that? I<br/>&gt; think that 1/log(n*d+1) might be a better estimate for what&apos;s going<br/>&gt; on, but might as well keep it simple.</p><p>Given that, I&apos;m not sure why you took the results from this and<br/>plotted them against HE. For me to make it actually yield a cent for<br/>cent equivalent to HE, I&apos;d have to figure out the Tenney series widths<br/>and work out the convolution integral and go from there.</p><p>But my real question is, there some reason why the use of the Tenney<br/>series is supposed to be more psychoacoustically valid than other<br/>series?</p><p>-Mike</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/28/2011 12:05:07 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:</p><p>&gt; But my real question is, there some reason why the use of<br/>&gt; the Tenney series is supposed to be more<br/>&gt; psychoacoustically valid than other series?</p><p>It&apos;s a pretty good bet.  We&apos;ve done the best research we<br/>can and we&apos;re happy with it.  Of course, we&apos;re not<br/>psychoacousticians, and we don&apos;t have a budget for<br/>properly controlled experiments.</p><p>It&apos;s also a result that comes out of Harmonic Entropy.  To<br/>an extent, you get out what you feed in.  I&apos;m remembering<br/>way back for this, but you have to specify a threshold for<br/>intervals.  I worked out that if you set that threshold<br/>according to Tenney harmonic distance, you get consonances<br/>ranked by Tenney harmonic distance.  You can also set the<br/>threshold according to the size of the numerator and get<br/>get consonances ranked by either the numerator or<br/>denominator.  Or something.  Like I said, it&apos;s a while ago,<br/>and I can&apos;t even remember how I worked it out.  But<br/>Harmonic Entropy is still measuring facts about the<br/>distribution of rationals, and there&apos;ll be a whole load of<br/>rankings you can&apos;t get out of it.</p><p>                          Graham</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/28/2011 12:07:20 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:</p><p>&gt;&gt; What you have doesn&apos;t seem to correspond to harmonic entropy,<br/>&gt;&gt; or exp(entropy), etc. That&apos;s what you claimed. That&apos;s all I&apos;m<br/>&gt;&gt; saying, no more no less. -Carl<br/>&gt;<br/>&gt;I never claimed that the 3 examples I just posted were supposed to be<br/>&gt;1-for-1 equivalent to harmonic entropy.</p><p>I asked specifically for the equivalent of s=1.0 in one cent<br/>increments and this is what you gave me!</p><p>You&apos;ve been claiming it&apos;s identical to HE for eons already.<br/>In fact, you&apos;ve called it a way to speed up HE computations.<br/>In fact, you just announced that you&apos;re going to start calling<br/>it HE instead of DC.  WTF are we supposed to think?</p><p>-Carl</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/28/2011 12:10:07 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;It&apos;s also a result that comes out of Harmonic Entropy.  To<br/>&gt;an extent, you get out what you feed in.  I&apos;m remembering<br/>&gt;way back for this, but you have to specify a threshold for<br/>&gt;intervals.  I worked out that if you set that threshold<br/>&gt;according to Tenney harmonic distance, you get consonances<br/>&gt;ranked by Tenney harmonic distance.  You can also set the<br/>&gt;threshold according to the size of the numerator and get<br/>&gt;get consonances ranked by either the numerator or<br/>&gt;denominator.  Or something.  Like I said, it&apos;s a while ago,<br/>&gt;and I can&apos;t even remember how I worked it out.  But<br/>&gt;Harmonic Entropy is still measuring facts about the<br/>&gt;distribution of rationals, and there&apos;ll be a whole load of<br/>&gt;rankings you can&apos;t get out of it.</p><p>According to Paul, if you set the threshold according to<br/>denominator, you still get consonances ranked by Tenney harmonic<br/>distance.  And same for sum of numerator and denominator.  -Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/28/2011 12:15:07 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Jan 28, 2011 at 3:05 AM, Graham Breed &lt;<a href="mailto:gbreed@gmail.com">gbreed@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt; But my real question is, there some reason why the use of<br/>&gt; &gt; the Tenney series is supposed to be more<br/>&gt; &gt; psychoacoustically valid than other series?<br/>&gt;<br/>&gt; It&apos;s a pretty good bet. We&apos;ve done the best research we<br/>&gt; can and we&apos;re happy with it. Of course, we&apos;re not<br/>&gt; psychoacousticians, and we don&apos;t have a budget for<br/>&gt; properly controlled experiments.</p><p>Alright, I&apos;ll work it out for the Tenney series. I&apos;m still not<br/>convinced that it&apos;s that miraculous though.</p><p>&gt; It&apos;s also a result that comes out of Harmonic Entropy. To<br/>&gt; an extent, you get out what you feed in. I&apos;m remembering<br/>&gt; way back for this, but you have to specify a threshold for<br/>&gt; intervals. I worked out that if you set that threshold<br/>&gt; according to Tenney harmonic distance, you get consonances<br/>&gt; ranked by Tenney harmonic distance.</p><p>I think that what it is is 1/sqrt(n*d) rather than 1/log(n*d), but<br/>yeah. However, if you do this with a Farey series, you just get 1/d<br/>though, I believe.</p><p>&gt; But Harmonic Entropy is still measuring facts about the<br/>&gt; distribution of rationals, and there&apos;ll be a whole load of<br/>&gt; rankings you can&apos;t get out of it.</p><p>Right, but as per this discussion we just had, it seems more like it&apos;s<br/>measuring facts about the series you use. If you use a Farey series,<br/>you get 1/d widths, and if you use a Tenney series, you get<br/>1/sqrt(n*d) widths. If you use Gene&apos;s ?(x) function, you get something<br/>else, and if you use the Stern-Brocot tree, you end up with something<br/>that looks completely ridiculous, despite that it has the unimodular<br/>property. According to Gene, at least, if you measure the actual<br/>distribution of the rationals with respect to dx, they&apos;re completely<br/>even. Most of what I&apos;ve seen in my own research on this shows that<br/>they are completely even, but that different subsets of them can be<br/>distributed differently (e.g. Farey series).</p><p>That being said, I decided to run the model such that the consonances<br/>really do end up with n*d-scaled minima, which you could probably<br/>reverse engineer to work out some series that gives you widths<br/>proportional to that. But I&apos;ll figure out equivalents for Farey,<br/>Tenney series, etc.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/28/2011 12:18:15 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Jan 28, 2011 at 3:07 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt;<br/>&gt; &gt;&gt; What you have doesn&apos;t seem to correspond to harmonic entropy,<br/>&gt; &gt;&gt; or exp(entropy), etc. That&apos;s what you claimed. That&apos;s all I&apos;m<br/>&gt; &gt;&gt; saying, no more no less. -Carl<br/>&gt; &gt;<br/>&gt; &gt;I never claimed that the 3 examples I just posted were supposed to be<br/>&gt; &gt;1-for-1 equivalent to harmonic entropy.<br/>&gt;<br/>&gt; I asked specifically for the equivalent of s=1.0 in one cent<br/>&gt; increments and this is what you gave me!</p><p>Sorry, I thought you had been following the recent developments in the<br/>thread. We went through a huge discussion about the sqrt(n*d) widths<br/>being arbitrary and I thought you were on the same page. Plus, you<br/>sent something offlist that says that as long as the function is<br/>reasonably well-behaved, that would be enough to satisfy you. But for<br/>proof of concept, I&apos;ll work out the Tenney-series equivalent version.</p><p>The one I sent ends up making the minima lower in entropy and smushes<br/>the maxima together more, which is what your graph is showing.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/28/2011 12:22:32 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; I asked specifically for the equivalent of s=1.0 in one cent<br/>&gt;&gt; increments and this is what you gave me!<br/>&gt;<br/>&gt;Sorry, I thought you had been following the recent developments in the<br/>&gt;thread. We went through a huge discussion about the sqrt(n*d) widths<br/>&gt;being arbitrary and I thought you were on the same page.</p><p>Nope, I haven&apos;t been reading it.  Since it lead you to conclude<br/>that sqrt(n*d) is arbitrary I apparently didn&apos;t miss much.</p><p>&gt;Plus, you<br/>&gt;sent something offlist that says that as long as the function is<br/>&gt;reasonably well-behaved, that would be enough to satisfy you.</p><p>It depends on the claims made.  If you say you&apos;ve got something<br/>that does something, great.  If you say you&apos;ve got something that<br/>you&apos;re calling &quot;harmonic entropy&quot; because it is harmonic entropy,<br/>then show us the gravy.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/28/2011 8:15:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Jan 28, 2011 at 3:22 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; I asked specifically for the equivalent of s=1.0 in one cent<br/>&gt; &gt;&gt; increments and this is what you gave me!<br/>&gt; &gt;<br/>&gt; &gt;Sorry, I thought you had been following the recent developments in the<br/>&gt; &gt;thread. We went through a huge discussion about the sqrt(n*d) widths<br/>&gt; &gt;being arbitrary and I thought you were on the same page.<br/>&gt;<br/>&gt; Nope, I haven&apos;t been reading it. Since it lead you to conclude that sqrt(n*d) is arbitrary I apparently didn&apos;t miss much.</p><p>I notice that you never do seem to miss much. I&apos;ll have to acquire that skill.</p><p>&gt; It depends on the claims made. If you say you&apos;ve got something<br/>&gt; that does something, great. If you say you&apos;ve got something that<br/>&gt; you&apos;re calling &quot;harmonic entropy&quot; because it is harmonic entropy,<br/>&gt; then show us the gravy.</p><p>If you really want to do that kind of a plot, then you need to tell me<br/>what you&apos;re plotting the results against. I&apos;m working right now with<br/>n*d &lt; 10000, mediant-to-mediant widths. I can&apos;t find a clear<br/>explanation anywhere of exactly how the sqrt(n*d) widths are set up to<br/>compare.</p><p>If you plot Paul&apos;s n*d&lt;10000 against his n*d&lt;5000, or his sqrt(n*d)<br/>widths against his mediant-to-mediant widths, I would expect that you<br/>also, unsurprisingly, do not get a line.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/28/2011 8:41:16 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Jan 28, 2011 at 11:15 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; I can&apos;t find a clear explanation anywhere of exactly how the sqrt(n*d) widths are set up to<br/>&gt; compare.</p><p>Paul just wrote this on my facebook wall:</p><p>&quot;Each probability slice&apos;s area is simply assumed to be *proportional*<br/>to width times Gaussian height at the corresponding ratio. Then one<br/>normalizes all the areas so that they sum to one.&quot;</p><p>I think I&apos;m going to go shoot myself in the face now.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/28/2011 10:17:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:</p><p>&gt;If you really want to do that kind of a plot, then you need to tell me<br/>&gt;what you&apos;re plotting the results against.</p><p>The only thing that should matter is s, which as I said is 1%.<br/>(In DC do you represent this as the variance of the Gaussian you<br/>convolve with?)</p><p>&gt;I&apos;m working right now with n*d &lt; 10000, mediant-to-mediant widths.<br/>&gt;I can&apos;t find a clear explanation anywhere of exactly how the sqrt(n*d)<br/>&gt;widths are set up to compare.</p><p>I divide the Gaussian&apos;s height at n/d by sqrt(n*d) and then<br/>normalize the sum of these areas to 1.  See here for the<br/>triadic version: <a href="http://groups.yahoo.com/group/tuning/message/93468">http://groups.yahoo.com/group/tuning/message/93468</a></p><p>Since you&apos;re after the heights of the impulses and there is<br/>no Gaussian yet, I dunoo... you tell me.  In our offlist chat<br/>months ago you produced a breakdown of the contribution of each<br/>impulse at a given point, and one of the columns was<br/>normalized to 1...</p><p>&gt;If you plot Paul&apos;s n*d&lt;10000 against his n*d&lt;5000, or his sqrt(n*d)<br/>&gt;widths against his mediant-to-mediant widths, I would expect that<br/>&gt;you also, unsurprisingly, do not get a line.</p><p>You certainly do.</p><p>-Carl</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/28/2011 10:19:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Paul just wrote this on my facebook wall:<br/>&gt;<br/>&gt;&quot;Each probability slice&apos;s area is simply assumed to be *proportional*<br/>&gt;to width times Gaussian height at the corresponding ratio. Then one<br/>&gt;normalizes all the areas so that they sum to one.&quot;<br/>&gt;<br/>&gt;I think I&apos;m going to go shoot myself in the face now.</p><p>Er, don&apos;t do that.  I&apos;m still not clear how you&apos;ll perform this<br/>normalization and retain the speedup, since it seemingly requires<br/>you &apos;pause&apos; the computation for each incoming dyad...  -Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 12:31:34 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sat, Jan 29, 2011 at 1:17 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt;<br/>&gt; &gt;If you really want to do that kind of a plot, then you need to tell me<br/>&gt; &gt;what you&apos;re plotting the results against.<br/>&gt;<br/>&gt; The only thing that should matter is s, which as I said is 1%.<br/>&gt; (In DC do you represent this as the variance of the Gaussian you<br/>&gt; convolve with?)</p><p>Right. Except it turns out that the convolution isn&apos;t that much of a<br/>speedup, anyway. What does seem to be a real speedup comes from the<br/>concept I sent you offlist a long time ago, which is the value of a<br/>Gaussian of some mean m taken at a point p equals the value of a<br/>Gaussian at mean p taken at a point m. If I start with a vector of<br/>zeroes and manually add Gaussians, it&apos;s still really fast.</p><p>The &quot;true HE&quot; equivalent is to add slightly altered versions of each<br/>Gaussian around each interval, rather than just add stock versions of<br/>each Gaussian. I&apos;m not sure how to model this as a convolution, but<br/>since manually adding them is already pretty fast, and since there are<br/>other speedups, I don&apos;t think it matters.</p><p>Read the thread. :)</p><p>&gt; &gt;I&apos;m working right now with n*d &lt; 10000, mediant-to-mediant widths.<br/>&gt; &gt;I can&apos;t find a clear explanation anywhere of exactly how the sqrt(n*d)<br/>&gt; &gt;widths are set up to compare.<br/>&gt;<br/>&gt; I divide the Gaussian&apos;s height at n/d by sqrt(n*d) and then<br/>&gt; normalize the sum of these areas to 1. See here for the<br/>&gt; triadic version: <a href="http://groups.yahoo.com/group/tuning/message/93468">http://groups.yahoo.com/group/tuning/message/93468</a></p><p>Not dealing with triads yet.</p><p>&gt; Since you&apos;re after the heights of the impulses and there is<br/>&gt; no Gaussian yet, I dunoo... you tell me. In our offlist chat<br/>&gt; months ago you produced a breakdown of the contribution of each<br/>&gt; impulse at a given point, and one of the columns was<br/>&gt; normalized to 1...</p><p>Right. Well, I have to work it all out again now that the rect<br/>functions are going to disappear, but the way it works is that you<br/>distribute the complexity of each point by -plogp, with normalized<br/>p&apos;s, rather than just p.</p><p>A really good -approximation- for this is just to add p together,<br/>which means that you can get a speedup by performing a convolution.<br/>This doesn&apos;t seem to be as much of a speedup, as previously mentioned,<br/>but it might come in handy when it comes to calculating tetrads and<br/>such.</p><p>At one point I thought that the two converged at infinity, and then I<br/>didn&apos;t think so. Then I did again, and then I didn&apos;t, and now in light<br/>of what Paul has said, I think that it does again. I&apos;m losing interest<br/>in this aspect of it, though.</p><p>&gt; &gt;If you plot Paul&apos;s n*d&lt;10000 against his n*d&lt;5000, or his sqrt(n*d)<br/>&gt; &gt;widths against his mediant-to-mediant widths, I would expect that<br/>&gt; &gt;you also, unsurprisingly, do not get a line.<br/>&gt;<br/>&gt; You certainly do.</p><p>Not in Paul&apos;s code that he sent me, which uses mediant-to-mediant widths.</p><p>&gt; Er, don&apos;t do that. I&apos;m still not clear how you&apos;ll perform this<br/>&gt; normalization and retain the speedup, since it seemingly requires<br/>&gt; you &apos;pause&apos; the computation for each incoming dyad... -Carl</p><p>The main speedup doesn&apos;t seem to be coming from the convolution anymore.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 9:29:51 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Right. Except it turns out that the convolution isn&apos;t that much of a<br/>&gt;speedup, anyway. What does seem to be a real speedup comes from the<br/>&gt;concept I sent you offlist a long time ago, which is the value of a<br/>&gt;Gaussian of some mean m taken at a point p equals the value of a<br/>&gt;Gaussian at mean p taken at a point m. If I start with a vector of<br/>&gt;zeroes and manually add Gaussians, it&apos;s still really fast.</p><p>I don&apos;t see much room for a speedup there, since you still have<br/>to do something for each incoming dyad.</p><p>&gt;&gt; I divide the Gaussian&apos;s height at n/d by sqrt(n*d) and then<br/>&gt;&gt; normalize the sum of these areas to 1. See here for the<br/>&gt;&gt; triadic version: <a href="http://groups.yahoo.com/group/tuning/message/93468">http://groups.yahoo.com/group/tuning/message/93468</a><br/>&gt;<br/>&gt;Not dealing with triads yet.</p><p>There&apos;s very little difference in the formulas, shown there.</p><p>&gt;A really good -approximation- for this is just to add p together,</p><p>That&apos;s what we discussed months ago -- whether the absolute sum<br/>tends to be greater when it is dominated by one Gaussian.  Until<br/>you run it and compare we won&apos;t know how true it is.</p><p>&gt;which means that you can get a speedup by performing a convolution.<br/>&gt;This doesn&apos;t seem to be as much of a speedup, as previously mentioned,<br/>&gt;but it might come in handy when it comes to calculating tetrads and<br/>&gt;such.</p><p>The real problem is the need to perform some task for each incoming<br/>interval, the number of which is 1200^(n-1) for n-ads.  Something<br/>like McLoed pitch will get around that but I don&apos;t see DC doing it.</p><p>&gt;&gt; &gt;If you plot Paul&apos;s n*d&lt;10000 against his n*d&lt;5000, or his sqrt(n*d)<br/>&gt;&gt; &gt;widths against his mediant-to-mediant widths, I would expect that<br/>&gt;&gt; &gt;you also, unsurprisingly, do not get a line.<br/>&gt;&gt;<br/>&gt;&gt; You certainly do.<br/>&gt;<br/>&gt;Not in Paul&apos;s code that he sent me, which uses mediant-to-mediant widths.</p><p>Paul ran such comparisons endlessly, which is the basis for his<br/>claim that HE has only a single free variable, s.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 5:32:20 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sat, Jan 29, 2011 at 12:29 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;Right. Except it turns out that the convolution isn&apos;t that much of a<br/>&gt; &gt;speedup, anyway. What does seem to be a real speedup comes from the<br/>&gt; &gt;concept I sent you offlist a long time ago, which is the value of a<br/>&gt; &gt;Gaussian of some mean m taken at a point p equals the value of a<br/>&gt; &gt;Gaussian at mean p taken at a point m. If I start with a vector of<br/>&gt; &gt;zeroes and manually add Gaussians, it&apos;s still really fast.<br/>&gt;<br/>&gt; I don&apos;t see much room for a speedup there, since you still have<br/>&gt; to do something for each incoming dyad.</p><p>The speedup is that after you realize that the Gaussian centered an<br/>incoming dyad d, evaluated at a just interval i, is the same thing as<br/>the Gaussian centered around the just interval and evaluated at the<br/>dyad. So you can just put a Gaussian around each interval i, add them<br/>together and sum them at each point d and get the probability at each<br/>dyad, which can be represented by a convolution...</p><p>Or so I thought. It turns out, however, that you have to do what you<br/>actually have to do is do the above three different times with<br/>slightly different basis vectors, add them together and sum them at<br/>each point d, and then take the log of that, then multiply it by<br/>another pre-convolved vector, and then another, divide the last one by<br/>1/sqrt(2*pi*c^2), etc.</p><p>Which is still nlogn time, assuming I haven&apos;t screwed up the proof,<br/>which I very well may have, and which I&apos;ll get on my website some time<br/>when I no longer have a real life.</p><p>But in case you didn&apos;t get it from my other thread, I am losing<br/>interest in this part of the project. And I&apos;m going to start computing<br/>triads whether the results yield a 1 to 1 correspondence with HE or<br/>not. You can pretend that it has nothing to do with HE, or that it&apos;s<br/>just an HE approximation, or &quot;HE-inspired,&quot; or a &quot;freehand drawing,&quot;<br/>or whatever derogatory language you&apos;d like to call the end model.</p><p>&gt; &gt;A really good -approximation- for this is just to add p together,<br/>&gt;<br/>&gt; That&apos;s what we discussed months ago -- whether the absolute sum<br/>&gt; tends to be greater when it is dominated by one Gaussian. Until<br/>&gt; you run it and compare we won&apos;t know how true it is.</p><p>I&apos;ve run it and compared loads of times. The tabular data I just sent<br/>you was precisely that; n*d heights but seeded with a Farey series.<br/>Took me like 0.5 seconds to compute. It doesn&apos;t look exactly like HE.</p><p>&gt; The real problem is the need to perform some task for each incoming<br/>&gt; interval, the number of which is 1200^(n-1) for n-ads. Something<br/>&gt; like McLoed pitch will get around that but I don&apos;t see DC doing it.</p><p>If McLeod pitch has to perform an autocorrelation for every dyad, then<br/>you&apos;re going to be at nlogn time for every dyad that you measure.<br/>Manually adding a Gaussian puts you at linear time for every dyad that<br/>you measure.</p><p>&gt; Paul ran such comparisons endlessly, which is the basis for his<br/>&gt; claim that HE has only a single free variable, s.</p><p>Not looking like that&apos;s the case for mediant-to-mediant widths.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 5:57:17 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;So you can just put a Gaussian around each interval i, add them<br/>&gt;together and sum them at each point d and get the probability at each<br/>&gt;dyad,</p><p>...an entropy at each dyad.  That&apos;s the kicker.</p><p>&gt;It turns out, however, that you have to do what you<br/>&gt;actually have to do is do the above three different times with<br/>&gt;slightly different basis vectors, add them together and sum them at<br/>&gt;each point d, and then take the log of that, then multiply it by<br/>&gt;another pre-convolved vector, and then another, divide the last one by<br/>&gt;1/sqrt(2*pi*c^2), etc.</p><p>Sounds complicated.</p><p>&gt;And I&apos;m going to start computing<br/>&gt;triads whether the results yield a 1 to 1 correspondence with HE or<br/>&gt;not. You can pretend that it has nothing to do with HE, or that it&apos;s<br/>&gt;just an HE approximation, or &quot;HE-inspired,&quot; or a &quot;freehand drawing,&quot;<br/>&gt;or whatever derogatory language you&apos;d like to call the end model.</p><p>Freehand drawing sounds about right.</p><p>&gt;&gt; Paul ran such comparisons endlessly, which is the basis for his<br/>&gt;&gt; claim that HE has only a single free variable, s.<br/>&gt;<br/>&gt;Not looking like that&apos;s the case for mediant-to-mediant widths.</p><p>Not sure what you mean.  Plots speak louder than words.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 5:59:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sat, Jan 29, 2011 at 8:57 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;And I&apos;m going to start computing<br/>&gt; &gt;triads whether the results yield a 1 to 1 correspondence with HE or<br/>&gt; &gt;not. You can pretend that it has nothing to do with HE, or that it&apos;s<br/>&gt; &gt;just an HE approximation, or &quot;HE-inspired,&quot; or a &quot;freehand drawing,&quot;<br/>&gt; &gt;or whatever derogatory language you&apos;d like to call the end model.<br/>&gt;<br/>&gt; Freehand drawing sounds about right.</p><p>Right, but the second part is that while you can sit from your chair<br/>and arbitrate about conceptual validity of some model that I spend<br/>hours on, that&apos;s really just your humble opinion :)</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 6:50:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Furthermore, let&apos;s go back to here:</p><p>Carl wrote:<br/>&gt;<br/>&gt; Paul ran such comparisons endlessly, which is the basis for his<br/>&gt; claim that HE has only a single free variable, s.</p><p>It has at least two free variables, them being s, and the series that<br/>you choose to seed the model with.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 6:55:48 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:</p><p>&gt;&gt; Freehand drawing sounds about right.<br/>&gt;<br/>&gt;Right, but the second part is that while you can sit from your chair<br/>&gt;and arbitrate about conceptual validity of some model that I spend<br/>&gt;hours on, that&apos;s really just your humble opinion :)</p><p>I didn&apos;t mean it as an insult.  Convolving with a set of impulses<br/>based on the rationals is a good idea, but it is also a broad brush.<br/>From your webpage on the matter, it isn&apos;t clear you&apos;ve succeeded in<br/>excluding any possible convolution fitting that description.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 7:07:14 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sat, Jan 29, 2011 at 9:55 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt;<br/>&gt; &gt;&gt; Freehand drawing sounds about right.<br/>&gt; &gt;<br/>&gt; &gt;Right, but the second part is that while you can sit from your chair<br/>&gt; &gt;and arbitrate about conceptual validity of some model that I spend<br/>&gt; &gt;hours on, that&apos;s really just your humble opinion :)<br/>&gt;<br/>&gt; I didn&apos;t mean it as an insult. Convolving with a set of impulses<br/>&gt; based on the rationals is a good idea, but it is also a broad brush.<br/>&gt; From your webpage on the matter, it isn&apos;t clear you&apos;ve succeeded in<br/>&gt; excluding any possible convolution fitting that description.</p><p>What do you mean by &quot;excluding any possible convolution?&quot; And the<br/>webpage is a bit out of date right now, in light of what happened last<br/>night. I&apos;ve also started considering that it might end up being<br/>simpler to work it out for the mediant-to-mediant version after all,<br/>but I can&apos;t delve into that right now.</p><p>The main point I&apos;m getting at, which is something that you missed, is<br/>in my followup message, where I assert that HE is a function of two<br/>parameters: s, and a measure you use to figure out the distribution of<br/>the rationals. This is what you missed last week, and props to Gene<br/>for elucidating on it. This is outside of the convolution model now,<br/>this is just a result in better understanding HE.</p><p>So if you use a Farey series, you get a certain measure for the<br/>distribution of the rationals, if you use a Tenney series, you get a<br/>different measure, etc. If you use Gene&apos;s ?(x) function, you get<br/>another measure, if you use the Stern-Brocot tree, you get yet another<br/>measure. If you use dx as a measure, you end up with all the widths<br/>being even.</p><p>I think that for any measure that you CHOOSE to use, you can probably<br/>reverse engineer a series that corresponds to it or converges to it in<br/>the limit.</p><p>Does that all make sense?</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 7:18:19 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:</p><p>&gt;The main point I&apos;m getting at, which is something that you missed, is<br/>&gt;in my followup message, where I assert that HE is a function of two<br/>&gt;parameters: s, and a measure you use to figure out the distribution of<br/>&gt;the rationals.</p><p>I didn&apos;t miss it.  You missed the part where I said &quot;plots<br/>speak louder than words&quot;.</p><p>&gt;So if you use a Farey series, you get a certain measure for the<br/>&gt;distribution of the rationals, if you use a Tenney series, you get a<br/>&gt;different measure, etc.</p><p>The &quot;distribution of the rationals&quot; isn&apos;t what we&apos;re measuring.</p><p>&gt;If you use Gene&apos;s ?(x) function, you get<br/>&gt;another measure, if you use the Stern-Brocot tree, you get yet another<br/>&gt;measure. If you use dx as a measure, you end up with all the widths<br/>&gt;being even.</p><p>I think you&apos;re talking about what happens when you use one of<br/>these in DC, not HE.</p><p>&gt;I think that for any measure that you CHOOSE to use, you can probably<br/>&gt;reverse engineer a series that corresponds to it or converges to it in<br/>&gt;the limit.</p><p>In the case of HE, Paul showed that the relative entropy at the<br/>rationals is always proportional to Tenney height, whether Farey or<br/>Tenney or Mann series are CHOSEN.  But even if he hadn&apos;t, Tenney<br/>is the right choice for other reasons.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 7:31:05 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sat, Jan 29, 2011 at 10:18 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt;<br/>&gt; &gt;The main point I&apos;m getting at, which is something that you missed, is<br/>&gt; &gt;in my followup message, where I assert that HE is a function of two<br/>&gt; &gt;parameters: s, and a measure you use to figure out the distribution of<br/>&gt; &gt;the rationals.<br/>&gt;<br/>&gt; I didn&apos;t miss it. You missed the part where I said &quot;plots<br/>&gt; speak louder than words&quot;.</p><p>There are plenty of plots that compare HE measured with a Farey series<br/>to HE measured with a Tenney series, and you already know how they<br/>differ.</p><p>&gt; &gt;So if you use a Farey series, you get a certain measure for the<br/>&gt; &gt;distribution of the rationals, if you use a Tenney series, you get a<br/>&gt; &gt;different measure, etc.<br/>&gt;<br/>&gt; The &quot;distribution of the rationals&quot; isn&apos;t what we&apos;re measuring.</p><p>It is in HE.</p><p>&gt; &gt;If you use Gene&apos;s ?(x) function, you get<br/>&gt; &gt;another measure, if you use the Stern-Brocot tree, you get yet another<br/>&gt; &gt;measure. If you use dx as a measure, you end up with all the widths<br/>&gt; &gt;being even.<br/>&gt;<br/>&gt; I think you&apos;re talking about what happens when you use one of<br/>&gt; these in DC, not HE.</p><p>I&apos;m talking about HE.</p><p>&gt; &gt;I think that for any measure that you CHOOSE to use, you can probably<br/>&gt; &gt;reverse engineer a series that corresponds to it or converges to it in<br/>&gt; &gt;the limit.<br/>&gt;<br/>&gt; In the case of HE, Paul showed that the relative entropy at the<br/>&gt; rationals is always proportional to Tenney height, whether Farey or<br/>&gt; Tenney or Mann series are CHOSEN. But even if he hadn&apos;t, Tenney<br/>&gt; is the right choice for other reasons.</p><p>Farey series have the widths proportional to 1/d, Tenney series have<br/>the widths proportional to 1/sqrt(n*d). The Farey series has a curve<br/>where all of the minima at 1/1, 2/1, 3/1, 4/1, 5/1, 6/1, 7/1, 8/1, etc<br/>have relatively similar entropy, and the whole curve slopes down. I<br/>know you&apos;ve seen this before, so I&apos;m not sure what you&apos;re saying here.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 9:34:43 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; I didn&apos;t miss it. You missed the part where I said &quot;plots<br/>&gt;&gt; speak louder than words&quot;.<br/>&gt;<br/>&gt;There are plenty of plots that compare HE measured with a Farey series<br/>&gt;to HE measured with a Tenney series, and you already know how they<br/>&gt;differ.</p><p>Yes- minimally.</p><p>&gt;&gt; &gt;So if you use a Farey series, you get a certain measure for the<br/>&gt;&gt; &gt;distribution of the rationals, if you use a Tenney series, you get a<br/>&gt;&gt; &gt;different measure, etc.<br/>&gt;&gt;<br/>&gt;&gt; The &quot;distribution of the rationals&quot; isn&apos;t what we&apos;re measuring.<br/>&gt;<br/>&gt;It is in HE.</p><p>No, it measures the entropy of the distribution.</p><p>&gt;&gt; &gt;If you use Gene&apos;s ?(x) function, you get<br/>&gt;&gt; &gt;another measure, if you use the Stern-Brocot tree, you get yet another<br/>&gt;&gt; &gt;measure. If you use dx as a measure, you end up with all the widths<br/>&gt;&gt; &gt;being even.<br/>&gt;&gt;<br/>&gt;&gt; I think you&apos;re talking about what happens when you use one of<br/>&gt;&gt; these in DC, not HE.<br/>&gt;<br/>&gt;I&apos;m talking about HE.</p><p>Plots speak louder than words.</p><p>&gt;&gt; &gt;I think that for any measure that you CHOOSE to use, you can probably<br/>&gt;&gt; &gt;reverse engineer a series that corresponds to it or converges to it in<br/>&gt;&gt; &gt;the limit.<br/>&gt;&gt;<br/>&gt;&gt; In the case of HE, Paul showed that the relative entropy at the<br/>&gt;&gt; rationals is always proportional to Tenney height, whether Farey or<br/>&gt;&gt; Tenney or Mann series are CHOSEN. But even if he hadn&apos;t, Tenney<br/>&gt;&gt; is the right choice for other reasons.<br/>&gt;<br/>&gt;Farey series have the widths proportional to 1/d, Tenney series have<br/>&gt;the widths proportional to 1/sqrt(n*d).</p><p>I said *entropy*.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 10:13:34 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 12:34 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/> &gt;&gt; &gt;So if you use a Farey series, you get a certain measure for the<br/>&gt; &gt;&gt; &gt;distribution of the rationals, if you use a Tenney series, you get a<br/>&gt; &gt;&gt; &gt;different measure, etc.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; The &quot;distribution of the rationals&quot; isn&apos;t what we&apos;re measuring.<br/>&gt; &gt;<br/>&gt; &gt;It is in HE.<br/>&gt;<br/>&gt; No, it measures the entropy of the distribution.</p><p>The distribution doesn&apos;t have an entropy. It measures the entropy of a<br/>dyad by integrating a Gaussian over each interval&apos;s &quot;domain,&quot; and each<br/>interval&apos;s &quot;domain&quot; is equal to its &quot;width,&quot; and its width is<br/>completely dependent on what series you use.</p><p>&gt; &gt;&gt; &gt;If you use Gene&apos;s ?(x) function, you get<br/>&gt; &gt;&gt; &gt;another measure, if you use the Stern-Brocot tree, you get yet another<br/>&gt; &gt;&gt; &gt;measure. If you use dx as a measure, you end up with all the widths<br/>&gt; &gt;&gt; &gt;being even.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; I think you&apos;re talking about what happens when you use one of<br/>&gt; &gt;&gt; these in DC, not HE.<br/>&gt; &gt;<br/>&gt; &gt;I&apos;m talking about HE.<br/>&gt;<br/>&gt; Plots speak louder than words.</p><p>Here&apos;s Paul&apos;s Tenney plot:<br/><a href="http://launch.groups.yahoo.com/group/harmonic_entropy/files/dyadic/default.gif">http://launch.groups.yahoo.com/group/harmonic_entropy/files/dyadic/default.gif</a><br/>Here&apos;s his Farey plot:<br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif</a></p><p>You should see the Stern-Brocot version, which doesn&apos;t even look<br/>remotely close to either of these.</p><p>&gt; &gt;Farey series have the widths proportional to 1/d, Tenney series have<br/>&gt; &gt;the widths proportional to 1/sqrt(n*d).<br/>&gt;<br/>&gt; I said *entropy*.</p><p>The entropy results from the widths that you use.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/29/2011 10:30:02 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 1:13 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; You should see the Stern-Brocot version, which doesn&apos;t even look<br/>&gt; remotely close to either of these.</p><p>I should add to this that I can come up with a Stern-Brocot plot if<br/>people want, but I realized what was wrong about Paul&apos;s code not<br/>correlating to itself. Paul sent me the mediant-to-mediant version,<br/>but the memory footprint on this is so extensive that it actually<br/>won&apos;t run. I noticed this a long time ago and set it to only use<br/>ratios from a Tenney series that are within 4 octaves of the lowest<br/>and highest dyad that you want. So if we&apos;re going from 0 to 1200 cents<br/>in 1 cent increments, it uses only ratios in which n*d &lt; N and 1/8 &lt;<br/>n/d &lt; 8/1.</p><p>This is a lot faster to calculate but could be the reason that the<br/>curves don&apos;t correlate properly. So I&apos;ll have to tweak the code a bit<br/>and see how it works then.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/29/2011 11:06:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Plots speak louder than words.<br/>&gt;<br/>&gt;Here&apos;s his Farey plot:<br/>&gt; <a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif</a></p><p>That&apos;s not &quot;Paul&apos;s Farey plot&quot;.</p><p>&gt;You should see the Stern-Brocot version, which doesn&apos;t even look<br/>&gt;remotely close to either of these.</p><p>Great, where is it?</p><p>&gt;&gt; &gt;Farey series have the widths proportional to 1/d, Tenney series have<br/>&gt;&gt; &gt;the widths proportional to 1/sqrt(n*d).<br/>&gt;&gt;<br/>&gt;&gt; I said *entropy*.<br/>&gt;<br/>&gt;The entropy results from the widths that you use.</p><p>Show me.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 12:42:30 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 2:06 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; Plots speak louder than words.<br/>&gt; &gt;<br/>&gt; &gt;Here&apos;s his Farey plot:<br/>&gt; &gt; <a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/harment.gif</a><br/>&gt;<br/>&gt; That&apos;s not &quot;Paul&apos;s Farey plot&quot;.</p><p>You&apos;ll have to stop &quot;being&quot; &quot;so cryptic.&quot; This is &quot;the Farey series HE<br/>plot&quot; in his &quot;files folder&quot; on &quot;tuning.&quot; This is what he told me to<br/>look at, this is exactly what he described the Farey series version as<br/>looking like, and this is why he told me he moved onto using the<br/>&quot;Tenney series.&quot;</p><p>Here&apos;s a message from him on Facebook, reprinted with his permission,<br/>about this very subject:</p><p>&gt; If we look at a Farey series, where the numerators are limited by N, the &quot;widths&quot; then go as 1/d rather than 1/sqrt(n*d) -- and I proved that rigorously in the original version of my 22 paper. The result is a harmonic entropy curve that has an overall downward slope. But other than the overall slope it looks incredibly similar to the curve deriving from a &quot;Tenney&quot; series; in particular, the &quot;dips&quot; at the major simple ratios seem to maintain the same width and even depth relative to the &quot;crests&quot; or any other features immediately in their vicinity.</p><p>But let&apos;s back up here. I&apos;m working under the belief that the choice<br/>of series affects the resultant curve. This is backed up by Paul&apos;s own<br/>statements, the curves I&apos;ve seen and now posted comparing the two, the<br/>discussion we had this last week about number theory and the<br/>distribution of the rationals, my own understanding of the calculus<br/>involved in the entropy summation, and pretty much every tidbit of<br/>information on the models I&apos;ve ever found at all.</p><p>You came in asserting that whatever results I posted should correlate<br/>perfectly to whatever random HE dataset you decide to plot them<br/>against, because the choice of series doesn&apos;t matter, and because the<br/>choice of n*d doesn&apos;t matter, and because the use of<br/>mediant-to-mediant widths vs sqrt(n*d) doesn&apos;t matter. That&apos;s quite an<br/>extraordinary claim to make, and I&apos;d like to see some proof of it.</p><p>&gt; &gt;You should see the Stern-Brocot version, which doesn&apos;t even look<br/>&gt; &gt;remotely close to either of these.<br/>&gt;<br/>&gt; Great, where is it?<br/>//<br/>&gt; Show me.</p><p>See followup message.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/30/2011 1:04:03 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;You&apos;ll have to stop &quot;being&quot; &quot;so cryptic.&quot; This is &quot;the Farey series HE<br/>&gt;plot&quot; in his &quot;files folder&quot; on &quot;tuning.&quot; This is what he told me to<br/>&gt;look at, this is exactly what he described the Farey series version as<br/>&gt;looking like, and this is why he told me he moved onto using the<br/>&gt;&quot;Tenney series.&quot;</p><p>He probably intended it as an exaggerated example to show the trend<br/>in the curve -- the Farey order is only 100!</p><p>&gt;Here&apos;s a message from him on Facebook, reprinted with his permission,<br/>&gt;about this very subject:<br/>&gt;<br/>&gt;If we look at a Farey series, where the numerators are limited by N,<br/>&gt;the &quot;widths&quot; then go as 1/d rather than 1/sqrt(n*d)</p><p>The mediant-mediant widths, not the widths of anything on the<br/>resulting entropy curve.</p><p>&gt;You came in asserting that whatever results I posted should correlate<br/>&gt;perfectly to whatever random HE dataset</p><p>It&apos;s odd that you think this.  You drew the comparison to HE, not me.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 1:24:21 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 4:04 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;You&apos;ll have to stop &quot;being&quot; &quot;so cryptic.&quot; This is &quot;the Farey series HE<br/>&gt; &gt;plot&quot; in his &quot;files folder&quot; on &quot;tuning.&quot; This is what he told me to<br/>&gt; &gt;look at, this is exactly what he described the Farey series version as<br/>&gt; &gt;looking like, and this is why he told me he moved onto using the<br/>&gt; &gt;&quot;Tenney series.&quot;<br/>&gt;<br/>&gt; He probably intended it as an exaggerated example to show the trend<br/>&gt; in the curve -- the Farey order is only 100!</p><p>Here are some other Farey series graphs:</p><p><a href="http://sonic-arts.org/td/erlich/entropy.htm">http://sonic-arts.org/td/erlich/entropy.htm</a><br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_006.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_006.jpg</a><br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_015.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_015.jpg</a></p><p>Here&apos;s one where N goes from 80 to 405:<br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a></p><p>Here&apos;s exp(entropy), showing that the curve actually gets steeper as N<br/>gets higher:<br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg</a></p><p>This would mean if you plotted them against one another you wouldn&apos;t get a line.</p><p>&gt; &gt;Here&apos;s a message from him on Facebook, reprinted with his permission,<br/>&gt; &gt;about this very subject:<br/>&gt; &gt;<br/>&gt; &gt;If we look at a Farey series, where the numerators are limited by N,<br/>&gt; &gt;the &quot;widths&quot; then go as 1/d rather than 1/sqrt(n*d)<br/>&gt;<br/>&gt; The mediant-mediant widths, not the widths of anything on the<br/>&gt; resulting entropy curve.</p><p>Why did you cut out the next sentence? &quot;The result is a harmonic<br/>entropy curve that has an overall downward slope.&quot;</p><p>&gt; &gt;You came in asserting that whatever results I posted should correlate<br/>&gt; &gt;perfectly to whatever random HE dataset<br/>&gt;<br/>&gt; It&apos;s odd that you think this. You drew the comparison to HE, not me.</p><p>I drew the comparison to HE based off of a bunch of calculus that I<br/>had done. You came in asserting that the curves should correlate<br/>perfectly to HE, despite the fact that what I was claiming at the time<br/>was that they&apos;d converge as N -&gt; Infinity. However, I decided to play<br/>along, since I wanted to see how close the models really would get.<br/>OK, fine, burden of proof is on me, and it turns out that there was an<br/>error in my derivation anyway, which I admitted.</p><p>But to continue to strive for a solution, I asked you what data I&apos;m<br/>supposed to be correlating to, and you said that you weren&apos;t going to<br/>tell me because it doesn&apos;t matter, because the curve looks pretty much<br/>the same no matter what N is set to, no matter what series you use, no<br/>matter what widths you use, etc. This is a pretty novel claim and it<br/>isn&apos;t supported by the charts I&apos;ve seen and posted, by what Paul has<br/>said about the use of sqrt(n*d) widths, or by my current understanding<br/>of the entropy calculation, or my running an optimized version of<br/>Paul&apos;s code, which may or may not be accurate, but generally reflected<br/>the charts.</p><p>You then said that the choice of series in particular doesn&apos;t matter,<br/>and that HE has only one free parameter - s. Since the entire point of<br/>my work, and the main result of last&apos;s weeks discussion that you<br/>missed is that the choice of series is all-important and dictates the<br/>end result, and that the end result can be efficiently approximated by<br/>picking reasonable heights and convolving with a Gaussian, this is yet<br/>another pretty bold claim to make. It again doesn&apos;t seem to match up<br/>with anything I&apos;ve heard or seen, or what Paul has said, or what the<br/>math itself would seem to suggest, although I could have always<br/>screwed up the math again.</p><p>So now I ask you, since you&apos;re claiming that the only way my model is<br/>validated is if it matches up to all HE curves, since they&apos;re all<br/>apparently correlated, and since you&apos;re also claiming that the choice<br/>of series doesn&apos;t do anything to the end curve, for some proof of<br/>these statements. It&apos;s good to hold my work to a high standard, but I<br/>don&apos;t think these specific claims are true and I don&apos;t think Paul does<br/>either.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/30/2011 2:20:49 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; He probably intended it as an exaggerated example to show the trend<br/>&gt;&gt; in the curve -- the Farey order is only 100!<br/>&gt;<br/>&gt;Here are some other Farey series graphs:<br/>&gt;<br/>&gt;<a href="http://sonic-arts.org/td/erlich/entropy.htm">http://sonic-arts.org/td/erlich/entropy.htm</a><br/>&gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_006.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_006.jpg</a><br/>&gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_015.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/ent_015.jpg</a></p><p>N=167 is the highest those go.  That&apos;s starting to look reasonable<br/>but note he says it&apos;s too low right on it!</p><p>&gt;Here&apos;s one where N goes from 80 to 405:<br/>&gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a></p><p>Labeled &quot;discordance&quot;.</p><p>&gt;Here&apos;s exp(entropy), showing that the curve actually gets steeper as N<br/>&gt;gets higher:<br/>&gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg</a></p><p>That&apos;s probably exp amplification at the higher starting points.</p><p>&gt;&gt; &gt;If we look at a Farey series, where the numerators are limited by N,<br/>&gt;&gt; &gt;the &quot;widths&quot; then go as 1/d rather than 1/sqrt(n*d)<br/>&gt;&gt;<br/>&gt;&gt; The mediant-mediant widths, not the widths of anything on the<br/>&gt;&gt; resulting entropy curve.<br/>&gt;<br/>&gt;Why did you cut out the next sentence? &quot;The result is a harmonic<br/>&gt;entropy curve that has an overall downward slope.&quot;</p><p>I know about the slope.  Why do you keep talking about mediant<br/>to mediant widths as if they came out in the result?</p><p>&gt;But to continue to strive for a solution, I asked you what data I&apos;m<br/>&gt;supposed to be correlating to, and you said that you weren&apos;t going to<br/>&gt;tell me because it doesn&apos;t matter,</p><p>I said I thought the standard harmonic entropy formulation is a<br/>good target.  That&apos;s n*d &lt; 10000, 1 &lt;= n/d &lt;= 2, 1/sqrt(n*d) widths,<br/>s = 1%.  I even showed that your thing doesn&apos;t match up to it.<br/>What else would you like me to do?</p><p>&gt;You then said that the choice of series in particular doesn&apos;t matter,<br/>&gt;and that HE has only one free parameter - s. Since the entire point of<br/>&gt;my work, and the main result of last&apos;s weeks discussion that you<br/>&gt;missed is that the choice of series is all-important and dictates the<br/>&gt;end result,</p><p>I&apos;m waiting for evidence of this.</p><p>&gt;and that the end result can be efficiently approximated by<br/>&gt;picking reasonable heights and convolving with a Gaussian,</p><p>Evidence of this?</p><p>&gt;So now I ask you, since you&apos;re claiming that the only way my model is<br/>&gt;validated is if it matches up to all HE curves,</p><p>I didn&apos;t say that.  I said you could alternatively demonstrate<br/>why your model makes sense -- why you&apos;re not free to choose any<br/>convolution integral you want, or how such a choice doesn&apos;t change<br/>the result.  You replied by saying you think you need to do three<br/>convolutions and cobble them together, or something.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 3:09:51 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 5:20 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;Here&apos;s one where N goes from 80 to 405:<br/>&gt; &gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a><br/>&gt;<br/>&gt; Labeled &quot;discordance&quot;.</p><p>Yes...?</p><p>&gt; &gt;Here&apos;s exp(entropy), showing that the curve actually gets steeper as N<br/>&gt; &gt;gets higher:<br/>&gt; &gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel2.jpg</a><br/>&gt;<br/>&gt; That&apos;s probably exp amplification at the higher starting points.</p><p>OK, good point.</p><p>&gt; &gt;Why did you cut out the next sentence? &quot;The result is a harmonic<br/>&gt; &gt;entropy curve that has an overall downward slope.&quot;<br/>&gt;<br/>&gt; I know about the slope. Why do you keep talking about mediant<br/>&gt; to mediant widths as if they came out in the result?</p><p>You said that the minima of harmonic entropy are scaled resulting to<br/>Tenney height, but now you&apos;re saying you knew about the slope? Look at<br/>the Farey picture - what has a lower entropy, 2/1 or 1/1? What has<br/>lower entropy, the maxima right before 2/1, or 4/3? Look on the N=405<br/>plot at the top here:<br/><a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a></p><p>While I never claimed that the Farey series curve has no valid<br/>interpretation, which it obviously does, I am saying that the choice<br/>of series biases the curve at the end of the day.</p><p>&gt; &gt;But to continue to strive for a solution, I asked you what data I&apos;m<br/>&gt; &gt;supposed to be correlating to, and you said that you weren&apos;t going to<br/>&gt; &gt;tell me because it doesn&apos;t matter,<br/>&gt;<br/>&gt; I said I thought the standard harmonic entropy formulation is a<br/>&gt; good target. That&apos;s n*d &lt; 10000, 1 &lt;= n/d &lt;= 2, 1/sqrt(n*d) widths,<br/>&gt; s = 1%. I even showed that your thing doesn&apos;t match up to it.<br/>&gt; What else would you like me to do?</p><p>You never told me n*d &lt; 10000 or 1/sqrt(n*d) widths.</p><p>&gt; &gt;You then said that the choice of series in particular doesn&apos;t matter,<br/>&gt; &gt;and that HE has only one free parameter - s. Since the entire point of<br/>&gt; &gt;my work, and the main result of last&apos;s weeks discussion that you<br/>&gt; &gt;missed is that the choice of series is all-important and dictates the<br/>&gt; &gt;end result,<br/>&gt;<br/>&gt; I&apos;m waiting for evidence of this.</p><p>I&apos;m shoving it in your face. Go look at the Tenney series results and<br/>then go look at the Farey series results. The entire curve changes.<br/>And if you&apos;d be patient, you&apos;ll have Stern-Brocot tree results soon as<br/>well.</p><p>&gt; &gt;and that the end result can be efficiently approximated by<br/>&gt; &gt;picking reasonable heights and convolving with a Gaussian,<br/>&gt;<br/>&gt; Evidence of this?</p><p>Here&apos;s a plot with heights that I think are reasonable; 1/(n*d) heights:</p><p><a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/ndheights.png">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/ndheights.png</a></p><p>Here&apos;s it with atan(4/nd) heights, which smooths out the curve a bit:</p><p><a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/atan-4nd-heights.png">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/atan-4nd-heights.png</a></p><p>This is not going to give a 1-1 correspondence to HE, so I&apos;m not going<br/>to bother to give you plots (the first one is the one I gave you plots<br/>of).</p><p>If you think that there is no utility in the fact that this model<br/>generates a curve like that and can be computed in under a second,<br/>then I don&apos;t really know what to tell you.</p><p>&gt; &gt;So now I ask you, since you&apos;re claiming that the only way my model is<br/>&gt; &gt;validated is if it matches up to all HE curves,<br/>&gt;<br/>&gt; I didn&apos;t say that. I said you could alternatively demonstrate<br/>&gt; why your model makes sense -- why you&apos;re not free to choose any<br/>&gt; convolution integral you want, or how such a choice doesn&apos;t change<br/>&gt; the result. You replied by saying you think you need to do three<br/>&gt; convolutions and cobble them together, or something.</p><p>You can choose any convolution integral that you want. You can also<br/>choose any series that you want in HE, and you&apos;ll get out what you put<br/>in.</p><p>In DC, you choose a basis function that consists of impulses weighted<br/>sensibly according to complexity, so that when a Gaussian is convolved<br/>with it, the model produces sensible results. This is analogous to<br/>what you guys did when you switched from the Farey to the Tenney<br/>series - made it produce more sensible results. And when I figure out<br/>how to run Paul&apos;s code, I&apos;ll post the Stern-Brocot tree example so you<br/>can see for yourself how dumb it looks.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 3:11:33 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 6:09 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; This is not going to give a 1-1 correspondence to HE, so I&apos;m not going<br/>&gt; to bother to give you plots (the first one is the one I gave you plots<br/>&gt; of).</p><p>Sorry, I meant tabular data. The first one is the one I already gave<br/>you tabular data of.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/30/2011 12:30:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;Here&apos;s one where N goes from 80 to 405:<br/>&gt;&gt; &gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a><br/>&gt;&gt;<br/>&gt;&gt; Labeled &quot;discordance&quot;.<br/>&gt;<br/>&gt;Yes...?</p><p>So, what is it?</p><p>&gt;You said that the minima of harmonic entropy are scaled resulting to<br/>&gt;Tenney height, but now you&apos;re saying you knew about the slope?</p><p>Yep, I&apos;ve written several times recently about the slope on several<br/>of the lists.</p><p>&gt;&gt; &gt;You then said that the choice of series in particular doesn&apos;t matter,<br/>&gt;&gt; &gt;and that HE has only one free parameter - s. Since the entire point of<br/>&gt;&gt; &gt;my work, and the main result of last&apos;s weeks discussion that you<br/>&gt;&gt; &gt;missed is that the choice of series is all-important and dictates the<br/>&gt;&gt; &gt;end result,<br/>&gt;&gt;<br/>&gt;&gt; I&apos;m waiting for evidence of this.<br/>&gt;<br/>&gt;I&apos;m shoving it in your face. Go look at the Tenney series results and<br/>&gt;then go look at the Farey series results. The entire curve changes.<br/>&gt;And if you&apos;d be patient, you&apos;ll have Stern-Brocot tree results soon as<br/>&gt;well.</p><p>The entropies of the minima are proportional to log(n*d) regardless<br/>of the series used.  That is Paul&apos;s result.  You have a similar<br/>result for DC?</p><p>&gt;&gt; &gt;and that the end result can be efficiently approximated by<br/>&gt;&gt; &gt;picking reasonable heights and convolving with a Gaussian,<br/>&gt;&gt;<br/>&gt;&gt; Evidence of this?<br/>&gt;<br/>&gt;Here&apos;s a plot with heights that I think are reasonable; 1/(n*d) heights:<br/>&gt;<br/>&gt;<a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/ndh">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/ndh</a><br/>&gt;eights.png</p><p>Looks vaguely reasonable.  Hard to tell much it.</p><p>&gt;Here&apos;s it with atan(4/nd) heights, which smooths out the curve a bit:</p><p>Why bother?  Just get a pencil and connect the dots.</p><p>&gt;If you think that there is no utility in the fact that this model<br/>&gt;generates a curve like that and can be computed in under a second,<br/>&gt;then I don&apos;t really know what to tell you.</p><p>I&apos;ve said it&apos;s a good idea.</p><p>&gt;In DC, you choose a basis function that consists of impulses weighted<br/>&gt;sensibly according to complexity, so that when a Gaussian is convolved<br/>&gt;with it, the model produces sensible results. This is analogous to<br/>&gt;what you guys did when you switched from the Farey to the Tenney<br/>&gt;series - made it produce more sensible results. And when I figure out<br/>&gt;how to run Paul&apos;s code, I&apos;ll post the Stern-Brocot tree example so you<br/>&gt;can see for yourself how dumb it looks.</p><p>Why would you use the Stern-Brocot tree??</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 12:56:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 3:30 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;Here&apos;s one where N goes from 80 to 405:<br/>&gt; &gt;&gt; &gt;<a href="http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg">http://launch.groups.yahoo.com/group/tuning/files/PaulErlich/manuel.jpg</a><br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Labeled &quot;discordance&quot;.<br/>&gt; &gt;<br/>&gt; &gt;Yes...?<br/>&gt;<br/>&gt; So, what is it?</p><p>Isn&apos;t it just entropy? He calls entropy discordance a lot of times on<br/>these plots, and the numbers on the side are generally the same range<br/>as the nats this model generates as a measure of entropy.</p><p>&gt; &gt;You said that the minima of harmonic entropy are scaled resulting to<br/>&gt; &gt;Tenney height, but now you&apos;re saying you knew about the slope?<br/>&gt;<br/>&gt; Yep, I&apos;ve written several times recently about the slope on several<br/>&gt; of the lists.</p><p>OK...? So then you get my point?</p><p>&gt; The entropies of the minima are proportional to log(n*d) regardless<br/>&gt; of the series used.</p><p>This is a false statement. Please stop making it. I responded to this<br/>statement in my last message, and you cut the statement out, didn&apos;t<br/>respond to it, and just said the same thing again. It&apos;s getting absurd<br/>now.</p><p>15/1 has a lower entropy than 3/2 if a Farey series is used.</p><p>&gt; You have a similar result for DC?</p><p>That&apos;s the whole point of DC. Rather than picking a series to<br/>indirectly engineer the curve to have a certain slope and/or shape,<br/>and picking a value of N high enough to give the curve linear behavior<br/>and avoid clipping, you just give it perfectly linear behavior from<br/>the outset with the convolution and pick the minima that you want.</p><p>&gt; &gt;Here&apos;s it with atan(4/nd) heights, which smooths out the curve a bit:<br/>&gt;<br/>&gt; Why bother? Just get a pencil and connect the dots.</p><p>Troll.</p><p>&gt; I&apos;ve said it&apos;s a good idea.</p><p>You&apos;ve said it in nothing but the most unflattering derogatory terms<br/>that you can think of, which although you might find amusing, is not<br/>as amusing from my vantage point after spending countless hours on<br/>this.</p><p>You left for a week, and in that week we had a huge discussion about<br/>number theory, did a huge investigation into why exactly DC and HE<br/>yield such similar results, investigated the distribution of different<br/>subsets of the rationals, figured out why different series yield<br/>different curves, etc. You came back a week later, missed the entire<br/>thing, arrogantly asserted you &quot;apparently didn&apos;t miss much,&quot; and now<br/>you&apos;re telling me that this is an arbitrary freehand drawing? Jesus<br/>Carl, WTF? That is absolutely ridiculous.</p><p>&gt; &gt;In DC, you choose a basis function that consists of impulses weighted<br/>&gt; &gt;sensibly according to complexity, so that when a Gaussian is convolved<br/>&gt; &gt;with it, the model produces sensible results. This is analogous to<br/>&gt; &gt;what you guys did when you switched from the Farey to the Tenney<br/>&gt; &gt;series - made it produce more sensible results. And when I figure out<br/>&gt; &gt;how to run Paul&apos;s code, I&apos;ll post the Stern-Brocot tree example so you<br/>&gt; &gt;can see for yourself how dumb it looks.<br/>&gt;<br/>&gt; Why would you use the Stern-Brocot tree??</p><p>Why wouldn&apos;t you? It generates simpler rationals first and more<br/>complex ones after, has the unimodular property, and generally has all<br/>of the properties that people like about the Farey and Tenney series.<br/>The problem is that there are so few rationals around the simple<br/>ratios that the curve ends up looking ridiculous. It looks like a<br/>&quot;distorted&quot; or &quot;clipped&quot; version of the Tenney series version.</p><p>I initially thought to use it because it&apos;s a lot quicker to compute<br/>than the Tenney series version, and it messed the curve up so bad that<br/>it was unusable. You had 1/1 with a ridiculously, unrealistically<br/>large field of attraction, etc.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/30/2011 1:40:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; Labeled &quot;discordance&quot;.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Yes...?<br/>&gt;&gt;<br/>&gt;&gt; So, what is it?<br/>&gt;<br/>&gt;Isn&apos;t it just entropy? He calls entropy discordance a lot of times on<br/>&gt;these plots, and the numbers on the side are generally the same range<br/>&gt;as the nats this model generates as a measure of entropy.</p><p>I don&apos;t know.  He did a lot of weird experiments over the years.</p><p>&gt;&gt; Yep, I&apos;ve written several times recently about the slope on several<br/>&gt;&gt; of the lists.<br/>&gt;<br/>&gt;OK...? So then you get my point?</p><p>No...</p><p>&gt;&gt; The entropies of the minima are proportional to log(n*d) regardless<br/>&gt;&gt; of the series used.<br/>&gt;<br/>&gt;This is a false statement. Please stop making it. I responded to this<br/>&gt;statement in my last message, and you cut the statement out, didn&apos;t<br/>&gt;respond to it, and just said the same thing again. It&apos;s getting absurd<br/>&gt;now.</p><p>Paul made this claim.  I haven&apos;t seen anything from you that<br/>debunks it.</p><p>&gt;15/1 has a lower entropy than 3/2 if a Farey series is used.</p><p>Troll.</p><p>&gt;You left for a week, and in that week we had a huge discussion about<br/>&gt;number theory, did a huge investigation into why exactly DC and HE<br/>&gt;yield such similar results, investigated the distribution of different<br/>&gt;subsets of the rationals, figured out why different series yield<br/>&gt;different curves, etc.</p><p>I wasn&apos;t gone, I just thought it was bullshit.  If you&apos;ve reached<br/>any conclusions in this business I doubt anyone here can name one.</p><p>&gt;&gt; Why would you use the Stern-Brocot tree??<br/>&gt;<br/>&gt;Why wouldn&apos;t you?  It generates simpler rationals first and more<br/>&gt;complex ones after, has the unimodular property, and generally has all<br/>&gt;of the properties that people like about the Farey and Tenney series.</p><p>Here is the relation of the subtree between 1/1 and 2/1 to the<br/>Tenney series</p><p>&gt; (df-dyads 2 (df-limit 2 10))<br/>(3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 12))<br/>(4/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 15))<br/>(4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 20))<br/>(5/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 28))<br/>(5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 30))<br/>(6/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 35))<br/>(6/5 7/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 40))<br/>(6/5 7/5 8/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 42))<br/>(7/6 6/5 7/5 8/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 45))<br/>(7/6 6/5 7/5 8/5 9/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 56))<br/>(8/7 7/6 6/5 7/5 8/5 9/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 63))<br/>(8/7 9/7 7/6 6/5 7/5 8/5 9/5 5/4 7/4 4/3 5/3 3/2 2)<br/>&gt; (df-dyads 2 (df-limit 2 66))<br/>(8/7 9/7 7/6 11/6 6/5 7/5 8/5 9/5 5/4 7/4 4/3 5/3 3/2 2)</p><p>1/1                                                                    2/1<br/>                                     3/2<br/>                        4/3          3/2<br/>                        4/3          3/2          5/3<br/>               5/4      4/3          3/2          5/3<br/>               5/4      4/3          3/2          5/3   7/4<br/>          6/5  5/4      4/3          3/2          5/3   7/4<br/>          6/5  5/4      4/3   7/5    3/2          5/3   7/4<br/>          6/5  5/4      4/3   7/5    3/2    8/5   5/3   7/4<br/>     7/6  6/5  5/4      4/3   7/5    3/2    8/5   5/3   7/4<br/>     7/6  6/5  5/4      4/3   7/5    3/2    8/5   5/3   7/4   9/5<br/> 8/7 7/6  6/5  5/4      4/3   7/5    3/2    8/5   5/3   7/4   9/5<br/> 8/7 7/6  6/5  5/4 9/7  4/3   7/5    3/2    8/5   5/3   7/4   9/5<br/> 8/7 7/6  6/5  5/4 9/7  4/3   7/5    3/2    8/5   5/3   7/4   9/5  11/6</p><p>So it&apos;s almost close enough to consider using, but probably<br/>not quite.  I dunno, I haven&apos;t tried it.</p><p>&gt;The problem is that there are so few rationals around the simple<br/>&gt;ratios that the curve ends up looking ridiculous. It looks like a<br/>&gt;&quot;distorted&quot; or &quot;clipped&quot; version of the Tenney series version.</p><p>It sounds like you did something wrong - I don&apos;t expect that big<br/>of a difference.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 2:08:02 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 4:40 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; The entropies of the minima are proportional to log(n*d) regardless<br/>&gt; &gt;&gt; of the series used.<br/>&gt; &gt;<br/>&gt; &gt;This is a false statement. Please stop making it. I responded to this<br/>&gt; &gt;statement in my last message, and you cut the statement out, didn&apos;t<br/>&gt; &gt;respond to it, and just said the same thing again. It&apos;s getting absurd<br/>&gt; &gt;now.<br/>&gt;<br/>&gt; Paul made this claim. I haven&apos;t seen anything from you that<br/>&gt; debunks it.</p><p>The fact that this is true stems because the entropy equation is being used<br/>with a series that generates simple rationals first and tends to cluster<br/>rationals together in packs of complex ratios. If you come up with a series<br/>that doesn&apos;t behave like that, you aren&apos;t going to get the entropy curve<br/>that you all know and love, because the simple ratios won&apos;t have the largest<br/>widths and eat up most of the Gaussian.</p><p>The fact that the slope occurs happens because the reciprocal of the Farey<br/>series that Paul is using will always have the highest rationals distributed<br/>the most sparsely.</p><p>&gt; &gt;You left for a week, and in that week we had a huge discussion about<br/>&gt; &gt;number theory, did a huge investigation into why exactly DC and HE<br/>&gt; &gt;yield such similar results, investigated the distribution of different<br/>&gt; &gt;subsets of the rationals, figured out why different series yield<br/>&gt; &gt;different curves, etc.<br/>&gt;<br/>&gt; I wasn&apos;t gone, I just thought it was bullshit.</p><p>The main conclusion was that your claim that the rationals tend to cluster<br/>away from simple ratios, even in the limit, isn&apos;t true. It depends on what<br/>measure of &quot;width&quot; you use. If you use the Farey measure of width, you get a<br/>setup where the rationals end up having larger relative widths in the<br/>infinite limit, if you use the Tenney measure of width, you get a different<br/>setup, and if you use the Stern-Brocot tree, you get a different setup, and<br/>if you use ?(x), you get a different setup. And if you use dx, it&apos;s even.</p><p>&gt; So it&apos;s almost close enough to consider using, but probably<br/>&gt; not quite. I dunno, I haven&apos;t tried it.</p><p>Why are we comparing it to the Tenney series? I don&apos;t understand.</p><p>&gt; &gt;The problem is that there are so few rationals around the simple<br/>&gt; &gt;ratios that the curve ends up looking ridiculous. It looks like a<br/>&gt; &gt;&quot;distorted&quot; or &quot;clipped&quot; version of the Tenney series version.<br/>&gt;<br/>&gt; It sounds like you did something wrong - I don&apos;t expect that big<br/>&gt; of a difference.</p><p>We&apos;ll find out once I fix Paul&apos;s code.</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 2:22:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 5:08 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; The fact that this is true stems because the entropy equation is being used<br/>&gt; with a series that generates simple rationals first and tends to cluster<br/>&gt; rationals together in packs of complex ratios. If you come up with a series<br/>&gt; that doesn&apos;t behave like that, you aren&apos;t going to get the entropy curve<br/>&gt; that you all know and love, because the simple ratios won&apos;t have the largest<br/>&gt; widths and eat up most of the Gaussian.</p><p>I should also add that the choice of mediants is also responsible for<br/>a lot of it, which means that the &quot;midpoint&quot; is placed closer to the<br/>more complex ratio. The use of mean-to-mean widths ends up working out<br/>the same anyway for the series&apos; that we&apos;re used to. Not going to be<br/>the case for this one:</p><p><a href="http://mathworld.wolfram.com/Calkin-WilfTree.html">http://mathworld.wolfram.com/Calkin-WilfTree.html</a></p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/30/2011 3:03:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Jan 30, 2011 at 5:22 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt;<br/>wrote:<br/>&gt; Not going to be the case for this one:<br/>&gt;<br/>&gt; <a href="http://mathworld.wolfram.com/Calkin-WilfTree.html">http://mathworld.wolfram.com/Calkin-WilfTree.html</a></p><p>Sorry, it appears I was misunderstanding the Calkin-Wilf tree - I didn&apos;t<br/>realize that you had to concatenate the previous entries each time as well.<br/>But if a suitable pathological example is needed in which a series can be<br/>used to seed HE that will still completely enumerate the rationals, has the<br/>unimodular property, and still screws up the resultant entropy of the curve<br/>anyway, I can probably work something out :)</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 4:20:43 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;<br/>&gt;&gt; &gt;&gt; The entropies of the minima are proportional to log(n*d) regardless<br/>&gt;&gt; &gt;&gt; of the series used.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;This is a false statement. Please stop making it. I responded to this<br/>&gt;&gt; &gt;statement in my last message, and you cut the statement out, didn&apos;t<br/>&gt;&gt; &gt;respond to it, and just said the same thing again. It&apos;s getting absurd<br/>&gt;&gt; &gt;now.<br/>&gt;&gt;<br/>&gt;&gt; Paul made this claim. I haven&apos;t seen anything from you that<br/>&gt;&gt; debunks it.<br/>&gt;<br/>&gt;The fact that this is true stems because the entropy equation is being used with a series that generates simple rationals first and tends to cluster rationals together in packs of complex ratios. If you come up with a series that doesn&apos;t behave like that, you aren&apos;t going to get the entropy curve that you all know and love,</p><p>By &quot;series&quot;, you honestly thought I meant any possible number series?</p><p>&gt;The main conclusion was that your claim that the rationals tend to cluster away from simple ratios, even in the limit, isn&apos;t true. It depends on what measure of &quot;width&quot; you use.</p><p>Width?  There&apos;s no width, there&apos;s distance, log(a)-log(b).  At the<br/>limit the rationals are dense, so that&apos;s the opposite of what I said<br/>during the time you claim I wasn&apos;t here.</p><p>&gt;Why are we comparing it to the Tenney series? I don&apos;t understand.</p><p>Because the Tenney series models how we hear.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 4:46:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 7:20 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;The fact that this is true stems because the entropy equation is being used with a series that generates simple rationals first and tends to cluster rationals together in packs of complex ratios. If you come up with a series that doesn&apos;t behave like that, you aren&apos;t going to get the entropy curve that you all know and love,<br/>&gt;<br/>&gt; By &quot;series&quot;, you honestly thought I meant any possible number series?</p><p>Yes. You said that HE has one free parameter, s, and that me picking<br/>n*d heights and convolving was just a freehand drawing. I&apos;m saying<br/>that no, it doesn&apos;t have one free parameter: it depends on you picking<br/>a series that meets whatever psychoacoustic criteria you&apos;re trying to<br/>fulfill.</p><p>Paul has made the claim that HE spontaneously predicts the dissonance<br/>and consonance of intervals based on the distribution of the rational<br/>numbers, and I&apos;m saying no, it spontaneously predicts the dissonance<br/>and consonance of intervals based on the distribution of a series that<br/>you choose based a priori on what intervals are consonant and<br/>dissonant. How in the hell does that make it not a freehand drawing?<br/>You guys are indirectly engineering a &quot;freehand drawing&quot; that I am<br/>trying to draw directly, and much more quickly.</p><p>The widths translate over to heights, and the use of a Gaussian<br/>spreads everything out Gaussian-wise. An almost indistinguishably<br/>similar thing happens in DC, and I have yet to fully resolve whether<br/>or not it approaches a real convolution in the limit (talking to Gene<br/>off-list about this now). If not, it&apos;s some kind of nonlinear<br/>quasi-convolution, which only makes a difference at very small<br/>intervals. In any case the two obviously don&apos;t seem to yield very<br/>different results, and to answer your question, yes, the maxima and<br/>minima end up being the same in DC as they are in HE.</p><p>So why, in DC, can&apos;t you convolve with any kernel you want? Well, you<br/>can, and you can also choose whatever series you want in HE, but in<br/>both cases it makes sense to pick one that matches some kind of<br/>psychoacoustically reasonable criterion. So please stop pretending<br/>otherwise in HE.</p><p>&gt; &gt;The main conclusion was that your claim that the rationals tend to cluster away from simple ratios, even in the limit, isn&apos;t true. It depends on what measure of &quot;width&quot; you use.<br/>&gt;<br/>&gt; Width? There&apos;s no width, there&apos;s distance, log(a)-log(b).</p><p>This is either a word game or I don&apos;t understand what you mean here.</p><p>&gt; At the limit the rationals are dense, so that&apos;s the opposite of what I said<br/>&gt; during the time you claim I wasn&apos;t here.</p><p>They are dense, but you can still claim that there are differing<br/>relative distances between &quot;pairs of adjacent rationals&quot; based on<br/>whether you&apos;re using dx as a measure of width, or something else. If<br/>you look at the Farey numbers, and you observe their distribution as<br/>the limit of N goes to infinity, it does not approach a perfectly even<br/>distribution.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 4:55:03 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Paul has made the claim that HE spontaneously predicts the dissonance<br/>&gt;and consonance of intervals based on the distribution of the rational<br/>&gt;numbers, and I&apos;m saying no, it spontaneously predicts the dissonance<br/>&gt;and consonance of intervals based on the distribution of a series that<br/>&gt;you choose based a priori on what intervals are consonant and<br/>&gt;dissonant.</p><p>For any list of rationals bounded by some reasonable measure of<br/>their complexity, the distance between them when ordered by size<br/>will be a function of that same measure of complexity.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 5:01:13 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 7:55 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt; their complexity</p><p>This is not a negligible restriction to place on the model.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 5:37:48 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;I should also add that the choice of mediants is also responsible for<br/>&gt;a lot of it, which means that the &quot;midpoint&quot; is placed closer to the<br/>&gt;more complex ratio.</p><p>The fact that mediants generally give you the simplest rational between<br/>to others should tell you something.</p><p>Mediants also aren&apos;t needed.  You can just take the distance all the<br/>way to neighbors on either side.  For the Farey series, that&apos;ll be<br/>the same as using the previous-order series with mediants, but with<br/>Tenney or Mann it won&apos;t.</p><p>-Carl</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 5:38:31 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt;&gt; their complexity<br/>&gt;<br/>&gt;This is not a negligible restriction to place on the model.</p><p>It&apos;s the whole point of the model -- hardly a free parameter.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 5:40:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 8:38 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt; &gt;&gt; their complexity<br/>&gt; &gt;<br/>&gt; &gt;This is not a negligible restriction to place on the model.<br/>&gt;<br/>&gt; It&apos;s the whole point of the model -- hardly a free parameter.</p><p>Well then there&apos;s your answer about why you can&apos;t use any convolution<br/>kernel you want in DC.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 5:45:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt;&gt; &gt;&gt; their complexity<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;This is not a negligible restriction to place on the model.<br/>&gt;&gt;<br/>&gt;&gt; It&apos;s the whole point of the model -- hardly a free parameter.<br/>&gt;<br/>&gt;Well then there&apos;s your answer about why you can&apos;t use any convolution<br/>&gt;kernel you want in DC.</p><p>That was my point- you seemed to be using any kernel you wanted.<br/>Assuming you agree to desist on that front, I&apos;m still worried about<br/>the convolution integral.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 5:47:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 8:45 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;Well then there&apos;s your answer about why you can&apos;t use any convolution<br/>&gt; &gt;kernel you want in DC.<br/>&gt;<br/>&gt; That was my point- you seemed to be using any kernel you wanted.</p><p>I always picked kernels that order intervals by complexity. Even the<br/>atan(4/(n*d)) one was ordered like that, but your response was &quot;why<br/>not just connect the dots?&quot; Something fishy&apos;s going on here.</p><p>&gt; Assuming you agree to desist on that front, I&apos;m still worried about<br/>&gt; the convolution integral.</p><p>Worried about it how?</p><p>-Mike</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 7:07:02 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 7:55 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt; their complexity, the distance between them when ordered by size<br/>&gt; will be a function of that same measure of complexity.</p><p>In fact, let&apos;s go back to this. I don&apos;t think that this is true. What<br/>happens if you use Tenney-Euclidean complexity, rather than just<br/>Tenney height? You&apos;ll get 15/8 appearing before 9/8, which is I doubt<br/>what you want. 1/n+1/d, which is the same thing as (n*d)/(n+d), also<br/>yields some pretty gnarly results.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 7:35:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 07:07 PM 1/31/2011, you wrote:<br/>&gt;On Mon, Jan 31, 2011 at 7:55 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; For any list of rationals bounded by some reasonable measure of<br/>&gt;&gt; their complexity, the distance between them when ordered by size<br/>&gt;&gt; will be a function of that same measure of complexity.<br/>&gt;<br/>&gt;In fact, let&apos;s go back to this. I don&apos;t think that this is true. What<br/>&gt;happens if you use Tenney-Euclidean complexity, rather than just<br/>&gt;Tenney height? You&apos;ll get 15/8 appearing before 9/8, which is I doubt<br/>&gt;what you want. 1/n+1/d, which is the same thing as (n*d)/(n+d), also<br/>&gt;yields some pretty gnarly results.</p><p>I didn&apos;t say any reasonable complexity gives good agreement with<br/>psychoacoustics, I said it gives lists of rationals that are<br/>distributed unevenly.  In this sense, the rationals do have the<br/>property I claimed.    -Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 7:42:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 10:35 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; I didn&apos;t say any reasonable complexity gives good agreement with<br/>&gt; psychoacoustics, I said it gives lists of rationals that are<br/>&gt; distributed unevenly. In this sense, the rationals do have the<br/>&gt; property I claimed. -Carl</p><p>1/n + 1/d also completely breaks the model. So if your goal is to<br/>tweak the model to give good results with psychoacoustics, then there<br/>are two parameters you&apos;re tweaking: s, and a method of enumerate the<br/>rationals by complexity. Not every method will work. These are two<br/>parameters.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 7:45:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 07:42 PM 1/31/2011, you wrote:<br/>&gt;On Mon, Jan 31, 2011 at 10:35 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; I didn&apos;t say any reasonable complexity gives good agreement with<br/>&gt;&gt; psychoacoustics, I said it gives lists of rationals that are<br/>&gt;&gt; distributed unevenly. In this sense, the rationals do have the<br/>&gt;&gt; property I claimed. -Carl<br/>&gt;<br/>&gt;1/n + 1/d also completely breaks the model.</p><p>I should say.  n/1 + d/1 doesn&apos;t though.</p><p>&gt;So if your goal is to<br/>&gt;tweak the model to give good results with psychoacoustics, then there<br/>&gt;are two parameters you&apos;re tweaking: s, and a method of enumerate the<br/>&gt;rationals by complexity. Not every method will work.</p><p>Every method should work reasonably well in an entropy-based model.<br/>I dunno about DC.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>1/31/2011 7:51:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Jan 31, 2011 at 10:45 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;1/n + 1/d also completely breaks the model.<br/>&gt;<br/>&gt; I should say. n/1 + d/1 doesn&apos;t though.</p><p>Oops. (n*d)/(n+d) might though.</p><p>&gt; &gt;So if your goal is to<br/>&gt; &gt;tweak the model to give good results with psychoacoustics, then there<br/>&gt; &gt;are two parameters you&apos;re tweaking: s, and a method of enumerate the<br/>&gt; &gt;rationals by complexity. Not every method will work.<br/>&gt;<br/>&gt; Every method should work reasonably well in an entropy-based model.<br/>&gt; I dunno about DC.</p><p>We&apos;re talking about strict HE now. So if I can come up with a series<br/>that bounds things by some measure of complexity other than n*d, n+d,<br/>and d that doesn&apos;t work reasonably well, that means I can claim it&apos;s<br/>two parameters, right?</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>1/31/2011 7:58:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;We&apos;re talking about strict HE now. So if I can come up with a series<br/>&gt;that bounds things by some measure of complexity other than n*d, n+d,<br/>&gt;and d that doesn&apos;t work reasonably well, that means I can claim it&apos;s<br/>&gt;two parameters, right?</p><p>That depends on how many posts you force me to reply to between<br/>now and then.  :P</p><p>-Carl</p></div>