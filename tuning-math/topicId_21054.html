<a href="/tuning-math">back to list</a><h1>Deprecating weighted coordinates and the two kinds of Tp complexity</h1><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/3/2012 12:26:42 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I reached some clarity on the TE complexity issues we were talking<br/>about on the tuning list; below is what I&apos;ve worked out.</p><p>The context is, we&apos;re noting that 2.9.5 81/80 (13&amp;19) has a TE<br/>complexity equal to half that of 2.3.5 81/80 (12&amp;19), but that the<br/>contorted and insane 2.9.5 81/80 (12&amp;19) is equal to 2.3.5 81/80.<br/>However, the weighted monzo for 81/80 is the same in all these cases<br/>regardless of subgroup, which is |-4 6.340 -2.322&gt;. What gives; how<br/>can the TE complexity differ?</p><p>On Fri, Nov 2, 2012 at 11:38 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; This looks<br/>&gt; like a thing with the way that Gene&apos;s &quot;dual&quot; is defined on the wiki;<br/>&gt; it isn&apos;t invariant under a change of basis. The L2 norms of the<br/>&gt; multimonzo and multival associated with a temperament will be<br/>&gt; identical if the coordinates aren&apos;t weighted, but once you weight<br/>&gt; things, one will be a multiple of the other.</p><p>This is a lot messier than I thought; in general the weighted &quot;dual&quot;<br/>ends up being a scaled version of what you want. So let&apos;s compare<br/>these two multivals:</p><p>1) take 81/80, put that into a weighted monzo, and then get the dual<br/>of it; you now have &lt;&lt;2.322 6.340 4||<br/>2) take &lt;12 19 28| and &lt;7 11 16|, weight those vals, and then take the<br/>wedge product; you now have &lt;&lt;0.631 1.723 1.087||</p><p>These are clearly not equal. However, the former is a multiple of the<br/>latter; specifically it&apos;s equal to the latter times log2(2) * log2(3)<br/>* log2(5), which is the determinant of the monzo weighting matrix.<br/>Now, if all we care about is ranking all of the temperaments within a<br/>single prime limit by TE complexity, then this fact doesn&apos;t matter at<br/>all, because this scaling doesn&apos;t change the rankings, but should we<br/>ever care to venture outside of this comfort zone and look at the<br/>behavior of this function across limits and subgroups, we&apos;ll need to<br/>get a better handle on what this means.</p><p>I was going to try to fix this &quot;problem&quot; by creating a refinement of<br/>the dual which can handle weighted coordinates, and which I was going<br/>to call the Tenney dual. This is simple enough to do, but it didn&apos;t<br/>take long before I realized that this wasn&apos;t going to solve the<br/>original problem. The problem is that the entire concept of weighted<br/>coordinates MAKES NO SENSE for the vast majority of subgroups. It&apos;s<br/>not just that working out Tenney height is more complicated in these<br/>subgroups, it&apos;s that the entire concept of weighting coordinates at<br/>all falls apart and becomes useless.</p><p>The whole thing is a nice mathematical trick for if you&apos;re working in<br/>a full-limit group (or a few special subgroups), because it simplifies<br/>things so you can just weight tuning maps or whatever and compute raw<br/>&quot;post-weighted&quot; distances to the JIP. However, there&apos;s literally no<br/>incentive to weight the axes if your axes are 2.9.15; this is stupid<br/>and evil and makes nothing easier. The whole thing is basically<br/>&quot;deprecated&quot; now that we&apos;re looking at subgroups in their full<br/>generality (but it can serve an indirect mathematical purpose as an<br/>intermediary in certain calculations).</p><p>Things get much more clear if you get rid of the idea of weighting the<br/>coordinates of monzos and vals directly. Instead, just keep the<br/>coordinates unweighted and natural, and add all of this<br/>complexification in the definition of the -norm-, not the coordinate<br/>system. For full-limits, this basically just means that rather than<br/>scaling the axes and using the normal Lp norm, we&apos;re going to keep the<br/>axes normal but used a scaled Lp norm. For more complex subgroups, the<br/>norm -won&apos;t- just be a scaled Lp norm, but will have to be obtained by<br/>a more elaborate transformation. If we&apos;re talking about a norm on<br/>monzos to start, then what I&apos;m describing here is what I called the<br/>&quot;Tp&quot; norm on the wiki (T stands for Tenney rather than L for<br/>Lebesgue); the T1 norm is always Tenney height no matter what subgroup<br/>you&apos;re in, and likewise with the T2 norm and TE height, etc.</p><p>This makes things much clearer. Once you have a norm, you<br/>automatically get a bunch of &quot;induced&quot; norms related to it. For<br/>instance, using the Tp norm automatically gives you a dual norm on the<br/>dual space, and it also induces norms on the exterior algebra of that<br/>vector space, etc. From this perspective, it&apos;s now obvious what&apos;s<br/>going on with TE complexity if you just think about Tp complexity in<br/>general. So say you have some (unweighted!) multival V and its dual M,<br/>representing the kernel. Then the induced norm on M is NOT (!!) the<br/>same as the induced norm on V, its dual! So the norm of the multimonzo<br/>representing some temperament&apos;s kernel is NOT the same as the norm of<br/>the multival representing the temperament itself.</p><p>Therefore, there are two types of Tp complexity - kernel complexity<br/>and &quot;mapping complexity&quot; or something, whatever we want to call it,<br/>and they&apos;re emphatically not the same. They actually have two<br/>different musical interpretations as well, though that&apos;s a subject for<br/>a different post. This means that it&apos;s a total coincidence that we<br/>even have this scenario where the TE complexity from the multival<br/>agrees with the TE complexity from the multimonzo, because this is<br/>usually NOT the case.</p><p>For instance, let&apos;s go back to our variables V and M from above, and<br/>say you&apos;re using the naive unweighted L1 norm on unweighted monzos<br/>(just to keep the math simple for now). Then the norm on M is also the<br/>L1 norm, but the norm on V is the Linf norm, since it&apos;s dual to M and<br/>lives in the dual space. These are the two complexity measures; the<br/>former is the direct complexity of the kernel and the latter has a<br/>really nice interpretation in terms of how much simple intervals are<br/>&quot;made more complex&quot; by the temperament. It&apos;s -ONLY- if you&apos;re using<br/>the L2 norm on M that the norm on M and the norm on V agree, since the<br/>L2 norm is dual to itself.</p><p>If instead we use the real T1 and T2 norms above, a similar thing<br/>happens. You still get the T1 norm on M and then you get a dual norm<br/>on V which is Linf-based just like T1 is L1-based. In the case of the<br/>T2 norm, the dual norm on V is still L2-based, but the scaling ends up<br/>working in a different way, so that one is a scaled version of the<br/>other. But note that the two are usually even further off!</p><p>So the fact that these two types of TE complexity differ is related to<br/>the fact that there are two types of Tp complexity in general. For TE<br/>complexity you get lucky and the complexity you get by looking at the<br/>multival is just a scaled version of the complexity you get by looking<br/>at the multimonzo; for other Tp norms these diverge and are even<br/>further apart from one another.</p><p>-Mike</p></div><h3>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/5/2012 11:38:14 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; I reached some clarity on the TE complexity issues we were talking<br/>&gt; about on the tuning list; below is what I&apos;ve worked out.<br/>&gt;<br/>&gt; The context is, we&apos;re noting that 2.9.5 81/80 (13&amp;19) has a TE<br/>&gt; complexity equal to half that of 2.3.5 81/80 (12&amp;19), but that the<br/>&gt; contorted and insane 2.9.5 81/80 (12&amp;19) is equal to 2.3.5 81/80.<br/>&gt; However, the weighted monzo for 81/80 is the same in all these cases<br/>&gt; regardless of subgroup, which is |-4 6.340 -2.322&gt;. What gives; how<br/>&gt; can the TE complexity differ?<br/>&gt;<br/>&gt; On Fri, Nov 2, 2012 at 11:38 PM, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; This looks<br/>&gt; &gt; like a thing with the way that Gene&apos;s &quot;dual&quot; is defined on the wiki;<br/>&gt; &gt; it isn&apos;t invariant under a change of basis. The L2 norms of the<br/>&gt; &gt; multimonzo and multival associated with a temperament will be<br/>&gt; &gt; identical if the coordinates aren&apos;t weighted, but once you weight<br/>&gt; &gt; things, one will be a multiple of the other.<br/>&gt;<br/>&gt; This is a lot messier than I thought; in general the weighted &quot;dual&quot;<br/>&gt; ends up being a scaled version of what you want. So let&apos;s compare<br/>&gt; these two multivals:<br/>&gt;<br/>&gt; 1) take 81/80, put that into a weighted monzo, and then get the dual<br/>&gt; of it; you now have &lt;&lt;2.322 6.340 4||<br/>&gt; 2) take &lt;12 19 28| and &lt;7 11 16|, weight those vals, and then take the<br/>&gt; wedge product; you now have &lt;&lt;0.631 1.723 1.087||<br/>&gt;<br/>&gt; These are clearly not equal. However, the former is a multiple of the<br/>&gt; latter; specifically it&apos;s equal to the latter times log2(2) * log2(3)<br/>&gt; * log2(5), which is the determinant of the monzo weighting matrix.<br/>&gt; Now, if all we care about is ranking all of the temperaments within a<br/>&gt; single prime limit by TE complexity, then this fact doesn&apos;t matter at<br/>&gt; all, because this scaling doesn&apos;t change the rankings, but should we<br/>&gt; ever care to venture outside of this comfort zone and look at the<br/>&gt; behavior of this function across limits and subgroups, we&apos;ll need to<br/>&gt; get a better handle on what this means.<br/>&gt;<br/>&gt; I was going to try to fix this &quot;problem&quot; by creating a refinement of<br/>&gt; the dual which can handle weighted coordinates, and which I was going<br/>&gt; to call the Tenney dual. This is simple enough to do, but it didn&apos;t<br/>&gt; take long before I realized that this wasn&apos;t going to solve the<br/>&gt; original problem. The problem is that the entire concept of weighted<br/>&gt; coordinates MAKES NO SENSE for the vast majority of subgroups. It&apos;s<br/>&gt; not just that working out Tenney height is more complicated in these<br/>&gt; subgroups, it&apos;s that the entire concept of weighting coordinates at<br/>&gt; all falls apart and becomes useless.<br/>&gt;<br/>&gt; The whole thing is a nice mathematical trick for if you&apos;re working in<br/>&gt; a full-limit group (or a few special subgroups), because it simplifies<br/>&gt; things so you can just weight tuning maps or whatever and compute raw<br/>&gt; &quot;post-weighted&quot; distances to the JIP. However, there&apos;s literally no<br/>&gt; incentive to weight the axes if your axes are 2.9.15; this is stupid<br/>&gt; and evil and makes nothing easier. The whole thing is basically<br/>&gt; &quot;deprecated&quot; now that we&apos;re looking at subgroups in their full<br/>&gt; generality (but it can serve an indirect mathematical purpose as an<br/>&gt; intermediary in certain calculations).<br/>&gt;<br/>&gt; Things get much more clear if you get rid of the idea of weighting the<br/>&gt; coordinates of monzos and vals directly. Instead, just keep the<br/>&gt; coordinates unweighted and natural, and add all of this<br/>&gt; complexification in the definition of the -norm-, not the coordinate<br/>&gt; system. For full-limits, this basically just means that rather than<br/>&gt; scaling the axes and using the normal Lp norm, we&apos;re going to keep the<br/>&gt; axes normal but used a scaled Lp norm. For more complex subgroups, the<br/>&gt; norm -won&apos;t- just be a scaled Lp norm, but will have to be obtained by<br/>&gt; a more elaborate transformation. If we&apos;re talking about a norm on<br/>&gt; monzos to start, then what I&apos;m describing here is what I called the<br/>&gt; &quot;Tp&quot; norm on the wiki (T stands for Tenney rather than L for<br/>&gt; Lebesgue); the T1 norm is always Tenney height no matter what subgroup<br/>&gt; you&apos;re in, and likewise with the T2 norm and TE height, etc.<br/>&gt;<br/>&gt; This makes things much clearer. Once you have a norm, you<br/>&gt; automatically get a bunch of &quot;induced&quot; norms related to it. For<br/>&gt; instance, using the Tp norm automatically gives you a dual norm on the<br/>&gt; dual space, and it also induces norms on the exterior algebra of that<br/>&gt; vector space, etc. From this perspective, it&apos;s now obvious what&apos;s<br/>&gt; going on with TE complexity if you just think about Tp complexity in<br/>&gt; general. So say you have some (unweighted!) multival V and its dual M,<br/>&gt; representing the kernel. Then the induced norm on M is NOT (!!) the<br/>&gt; same as the induced norm on V, its dual! So the norm of the multimonzo<br/>&gt; representing some temperament&apos;s kernel is NOT the same as the norm of<br/>&gt; the multival representing the temperament itself.<br/>&gt;<br/>&gt; Therefore, there are two types of Tp complexity - kernel complexity<br/>&gt; and &quot;mapping complexity&quot; or something, whatever we want to call it,<br/>&gt; and they&apos;re emphatically not the same. They actually have two<br/>&gt; different musical interpretations as well, though that&apos;s a subject for<br/>&gt; a different post. This means that it&apos;s a total coincidence that we<br/>&gt; even have this scenario where the TE complexity from the multival<br/>&gt; agrees with the TE complexity from the multimonzo, because this is<br/>&gt; usually NOT the case.<br/>&gt;<br/>&gt; For instance, let&apos;s go back to our variables V and M from above, and<br/>&gt; say you&apos;re using the naive unweighted L1 norm on unweighted monzos<br/>&gt; (just to keep the math simple for now). Then the norm on M is also the<br/>&gt; L1 norm, but the norm on V is the Linf norm, since it&apos;s dual to M and<br/>&gt; lives in the dual space. These are the two complexity measures; the<br/>&gt; former is the direct complexity of the kernel and the latter has a<br/>&gt; really nice interpretation in terms of how much simple intervals are<br/>&gt; &quot;made more complex&quot; by the temperament. It&apos;s -ONLY- if you&apos;re using<br/>&gt; the L2 norm on M that the norm on M and the norm on V agree, since the<br/>&gt; L2 norm is dual to itself.<br/>&gt;<br/>&gt; If instead we use the real T1 and T2 norms above, a similar thing<br/>&gt; happens. You still get the T1 norm on M and then you get a dual norm<br/>&gt; on V which is Linf-based just like T1 is L1-based. In the case of the<br/>&gt; T2 norm, the dual norm on V is still L2-based, but the scaling ends up<br/>&gt; working in a different way, so that one is a scaled version of the<br/>&gt; other. But note that the two are usually even further off!<br/>&gt;<br/>&gt; So the fact that these two types of TE complexity differ is related to<br/>&gt; the fact that there are two types of Tp complexity in general. For TE<br/>&gt; complexity you get lucky and the complexity you get by looking at the<br/>&gt; multival is just a scaled version of the complexity you get by looking<br/>&gt; at the multimonzo; for other Tp norms these diverge and are even<br/>&gt; further apart from one another.</p><p>I&apos;m confused about the difference in meaning or interpretation of T1 multimonzo complexity vs T1 val complexity (these are not duals, they just seem like they&apos;re very similar). If you take a bunch of good temperaments (e.g. the Middle Path 5-limit list) and plot them with T1 multimonzo complexity on one axis and T1 multival complexity on the other axis, what does the scatter plot look like? What does the axis orthogonal to the simple-complex axis represent?</p><p>Keenan</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2012 12:28:51 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;This is a lot messier than I thought; in general the weighted &quot;dual&quot;<br/>&gt;ends up being a scaled version of what you want. So let&apos;s compare<br/>&gt;these two multivals:<br/>&gt;<br/>&gt;1) take 81/80, put that into a weighted monzo, and then get the dual<br/>&gt;of it; you now have &lt;&lt;2.322 6.340 4||<br/>&gt;2) take &lt;12 19 28| and &lt;7 11 16|, weight those vals, and then take the<br/>&gt;wedge product; you now have &lt;&lt;0.631 1.723 1.087||</p><p>Indeed.</p><p>&gt;Things get much more clear if you get rid of the idea of weighting the<br/>&gt;coordinates of monzos and vals directly. Instead, just keep the<br/>&gt;coordinates unweighted and natural, and add all of this<br/>&gt;complexification in the definition of the -norm-, not the coordinate<br/>&gt;system. For full-limits, this basically just means that rather than<br/>&gt;scaling the axes and using the normal Lp norm, we&apos;re going to keep the<br/>&gt;axes normal but used a scaled Lp norm. For more complex subgroups, the<br/>&gt;norm -won&apos;t- just be a scaled Lp norm, but will have to be obtained by<br/>&gt;a more elaborate transformation.</p><p>Ok!</p><p>&gt;If we&apos;re talking about a norm on<br/>&gt;monzos to start, then what I&apos;m describing here is what I called the<br/>&gt;&quot;Tp&quot; norm on the wiki (T stands for Tenney rather than L for<br/>&gt;Lebesgue); the T1 norm is always Tenney height no matter what subgroup<br/>&gt;you&apos;re in, and likewise with the T2 norm and TE height, etc.</p><p>Lost me here. I thought the &apos;problem&apos; we&apos;re trying to solve is<br/>the complexity of 2.3.5 meantone being different than 2.9.5<br/>meantone.</p><p>&gt;If instead we use the real T1 and T2 norms above, a similar thing<br/>&gt;happens. You still get the T1 norm on M and then you get a dual norm<br/>&gt;on V which is Linf-based just like T1 is L1-based. In the case of the<br/>&gt;T2 norm, the dual norm on V is still L2-based, but the scaling ends up<br/>&gt;working in a different way, so that one is a scaled version of the<br/>&gt;other. But note that the two are usually even further off!<br/>&gt;So the fact that these two types of TE complexity differ is related to<br/>&gt;the fact that there are two types of Tp complexity in general. For TE<br/>&gt;complexity you get lucky and the complexity you get by looking at the<br/>&gt;multival is just a scaled version of the complexity you get by looking<br/>&gt;at the multimonzo; for other Tp norms these diverge and are even<br/>&gt;further apart from one another.</p><p>What happens if we apply the weighting after getting the multival?<br/>e.g. &lt;&lt;1 2 4|| vs &lt;&lt;1 4 4||. What are weighting coefficients here?</p><p>-Carl</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2012 1:59:03 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I wrote:<br/>&gt; What happens if we apply the weighting after getting the multival?<br/>&gt; e.g. &lt;&lt;1 2 4|| vs &lt;&lt;1 4 4||. What are weighting coefficients here?</p><p>I think the weights are 1/p1, 1/p2, and 1/p0p2 where pi is the<br/>base-2 log of the (0-indexed) ith prime in the smonzo. Right?</p><p>If so, then for TE complexity I get 0.87 and 1.27 respectively.<br/>Drat.</p><p>I get 0.62 and 1.23 if I weight the reduced vals before wedging.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2012 9:20:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;m trying to keep up here... promised too many responses to too many<br/>people (and I still haven&apos;t forgotten your explanation of<br/>thermoeconomics on FB). Anyways, enjoy...</p><p>On Wed, Nov 7, 2012 at 3:28 AM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;If we&apos;re talking about a norm on<br/>&gt; &gt;monzos to start, then what I&apos;m describing here is what I called the<br/>&gt; &gt;&quot;Tp&quot; norm on the wiki (T stands for Tenney rather than L for<br/>&gt; &gt;Lebesgue); the T1 norm is always Tenney height no matter what subgroup<br/>&gt; &gt;you&apos;re in, and likewise with the T2 norm and TE height, etc.<br/>&gt;<br/>&gt; Lost me here. I thought the &apos;problem&apos; we&apos;re trying to solve is<br/>&gt; the complexity of 2.3.5 meantone being different than 2.9.5<br/>&gt; meantone.</p><p>The reason I&apos;m going into more abstract details about the structure of<br/>the Tp norm is that it&apos;s necessary to see why the 2.3.5 vs 2.9.5 issue<br/>arises to begin with. The essential topic being discussed here is:<br/>given the Tp norm being placed on the vector space of monzos M, what<br/>are the induced norms on various vector spaces associated with M, and<br/>what complexity measures do they represent? These spaces would include<br/>V, the dual space to M, as well as &Lambda;(M) and &Lambda;(V), the exterior algebra<br/>on M and V. Our overall strategy here is to first impose a norm on M<br/>representing intervallic complexity, and to then start chaining<br/>together &quot;induced&quot; norms on these various related spaces in standard<br/>linear-algebraic ways and see what we come up with.</p><p>&Lambda;&#x207F;(M), in simple terms, is the vector space of n-dimensional<br/>multimonzos, and &Lambda;&#x207F;(V) is the vector space of n-dimensional multivals.<br/>For any space of multimonzos &Lambda;&#x207F;(M), the space of &quot;dual&quot; or<br/>&quot;complement&quot; multivals exists in &Lambda;&#x1D48;&#x207B;&#x207F;(V), where d is the<br/>dimensionality of M (e.g. the rank of the subgroup M represents). And<br/>what we&apos;re trying to do specifically is to compare the norm induced on<br/>&Lambda;&#x207F;(M), where each multimonzo represents the kernel of some<br/>temperament, with the one induced on &Lambda;&#x1D48;&#x207B;&#x207F;(V), where each multival is<br/>dual to a multimonzo in the former space, and see how their behaviors<br/>change as the subgroup changes.</p><p>&gt; What happens if we apply the weighting after getting the multival?<br/>&gt; e.g. &lt;&lt;1 2 4|| vs &lt;&lt;1 4 4||. What are weighting coefficients here?</p><p>I&apos;ll do the math out for 2.3.5 and 2.9.5 so you can compare.</p><p>So for instance, if we&apos;re looking at 2.3.5 meantone, then the kernel<br/>|-4 4 -1&gt; exists in &Lambda;&sup1;(M) = M, the space of monzos, and its dual<br/>multival &lt;&lt;1 4 4|| exists in &Lambda;&sup3;&#x207B;&sup1;(V) = &Lambda;&sup2;(V), the space of bivals.<br/>Then the question we&apos;re asking ourselves is, if we equip M with a Tp<br/>norm, how do the norms on M and &Lambda;&sup2;(V) differ? Well, in general, they<br/>differ a lot. Here&apos;s the Tp norm on M for |-4 4 -1&gt;:</p><p>|| |-4 4 -1&gt; || = (|log2(2)&middot;(-4)|^p + |log2(3)&middot;(4)|^p + |log2(5)&middot;(-1)|^p)^(1/p)</p><p>So for p=1 (Tenney Height) you get 12.661, for p=2 (Tenney-Euclidean<br/>height) you get 7.848, for p=Inf you get 6.340, etc.</p><p>You can see that in this case, since the subgroup is &quot;neat&quot;, the Tp<br/>norm just reduces to a simple weighted Lp norm. There is hence a dual<br/>weighted Lq norm induced on V, where 1/p + 1/q = 1, but this time the<br/>weighting is backwards: we&apos;re weighting things by 1/log2(2),<br/>1/log2(3), etc rather than their reciprocals as we did with monzos.<br/>Then, from there, there&apos;s also a sort of higher-dimensional weighted<br/>Lq norm induced on &Lambda;&sup2;(V), which is as follows:</p><p>|| &lt;&lt;1 4 4|| || = (|1/(log2(2)log2(3))&middot;(1)|^q +<br/>|1/(log2(2)log2(5))&middot;(4)|^q + |1/(log2(3)log2(5))&middot;(4)|^q)^(1/q)<br/>(Note that the weights now look like 1/(log2(2)log2(3)), which is the<br/>product of the individual weights of the basis vals you wedge to get<br/>the basis bivals.)</p><p>So for p=1 you get q=Inf, which works out to 1.723. If p=2 then q=2,<br/>which works out to 2.132. If p=Inf then q=1, which works out to 3.441.</p><p>The key thing to note here is that these things are all NOT the same,<br/>even in the case of p=2/q=2. Even in that case, you still do not<br/>actually get the exact same thing if you take the induced norm on the<br/>monzo representing the kernel vs the induced norm on the multival<br/>representing the mapping, because the weightings are different, and<br/>the weightings are an inherent part of the Tp norm. If we were using<br/>-unweighted- Lp norms, this situation wouldn&apos;t arise, and the L2 norm<br/>would give you the same exact thing for |-4 4 -1&gt; as it would for &lt;&lt;1<br/>4 4||. But this isn&apos;t the case here.</p><p>However, look what happens if you multiply these multival norm results<br/>by log2(2)&middot;log2(3)&middot;log2(5): for q=Inf you get 6.340, for q=2 you get<br/>7.848, and for q=1 you get 12.661. If we look at this scaled norm,<br/>then the p=2/q=2 norms on multimonzos and multivals agree at 7.848.<br/>Setting p=1 on multimonzos gives you q=Inf on multivals - in this<br/>case, the result for multivals is the same as if you&apos;d set p=Inf on<br/>multimonzos, which is 6.340. And if you set p=Inf on multimonzos, you<br/>get q=1 on multivals, and now the scaled multival norm agrees with p=1<br/>on multimonzos at 12.661.</p><p>Is this scaled norm &quot;correct,&quot; with the unscaled norm being messed up<br/>somehow? No, I don&apos;t think so. There are simply two different norms<br/>here, one for the temperament&apos;s kernel and another for its multival,<br/>both of which are naturally induced from the original norm on monzos.<br/>In the case of TE (T2) and -only- TE, you can sort of force them to be<br/>the same by doing this scaling trick above, but in general they won&apos;t<br/>even coincide with a simple constant scaling factor and they<br/>shouldn&apos;t. They have two different linear-algebraic interpretations<br/>and tell you two different things about the temperament, and even in<br/>the case of T2 the scaling changes differently when the subgroup<br/>changes, as we&apos;ll see now.</p><p>So let&apos;s do the whole thing again for the 2.9.5 subgroup, assuming we<br/>want the non-contorted 81/80 temperament generated by 2/1 and 9/8<br/>(e.g. there&apos;s a deliberate generator change). Then the kernel is now<br/>|-4 2 -1&gt;, and the bival is &lt;&lt;1 2 4||. Since this is still a &quot;nice&quot;<br/>subgroup, we can continue to just weight the axes without worrying<br/>about anything more complicated. Let&apos;s see what we get:</p><p>|| |-4 2 -1&gt; || = (|log2(2)&middot;(-4)|^p + |log2(9)&middot;(2)|^p + |log2(5)&middot;(-1)|^p)^(1/p)</p><p>So for p=1 (Tenney Height) you get 12.661, for p=2 (Tenney-Euclidean<br/>height) you get 7.848, for p=Inf you get 6.340, etc. As you can see,<br/>these values are all completely identical to the 2.3.5 81/80 version.<br/>And for the bival:</p><p>|| &lt;&lt;1 2 4|| || = (|1/(log2(2)log2(9))&middot;(1)|^q +<br/>|1/(log2(2)log2(5))&middot;(2)|^q + |1/(log2(9)log2(5))&middot;(4)|^q)^(1/q)</p><p>So for p=1 you get q=Inf, which works out to 0.861. If p=2 then q=2,<br/>which works out to 1.066. If p=Inf then q=1, which works out to 1.720.</p><p>As before, if we multiply these latter values by<br/>log2(2)&middot;log2(9)&middot;log2(5), things align with the values for 2.9.5 |-4 2<br/>-1&gt;. For p=1/q=Inf you get 6.340, for p=2/q=2 you get 7.848, for<br/>p=Inf/q=1 you get 12.661. However, if we don&apos;t do this scaling, these<br/>values are exactly half of the values for the 2.3.5 81/80 bival. You<br/>can go compare these with the values above.</p><p>For posterity&apos;s sake, we might as well compare the 2.9.5 contorted and<br/>insane 12&amp;19 temperament. In this case, the kernel is still |-4 2 -1&gt;,<br/>so you know the kernel complexity is the same as above. However, the<br/>multival is now &lt;&lt;2 4 8||. Let&apos;s see what we get:</p><p>|| &lt;&lt;2 4 8|| || = (|1/(log2(2)log2(9))&middot;(2)|^q +<br/>|1/(log2(2)log2(5))&middot;(4)|^q + |1/(log2(9)log2(5))&middot;(8)|^q)^(1/q)</p><p>For p=1/q=Inf you get 1.723, for p=2/q=2, you get 2.132, for<br/>p=Inf/q=1, you get 3.441. These values are now the same as the 2.3.5<br/>81/80 values. But, now, if you scale it by log2(2)&middot;log2(9)&middot;log2(5),<br/>you get p=1/q=Inf: 12.680, p=2/q=2: 15.695, p=Inf/q=1: 25.324. Now<br/>everything is twice the complexity of the 2.3.5 81/80 temperament.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/8/2012 11:16:18 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I wrote:</p><p>&gt;&gt; What happens if we apply the weighting after getting the multival?<br/>&gt;&gt; e.g. &lt;&lt;1 2 4|| vs &lt;&lt;1 4 4||. What are weighting coefficients here?<br/>&gt;<br/>&gt;I think the weights are 1/p1, 1/p2, and 1/p0p2 where pi is the<br/>&gt;base-2 log of the (0-indexed) ith prime in the smonzo. Right?</p><p>Gene: are these the right weights?</p><p>&gt;If so, then for TE complexity I get 0.87 and 1.27 respectively.<br/>&gt;Drat.<br/>&gt;<br/>&gt;I get 0.62 and 1.23 if I weight the reduced vals before wedging.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/8/2012 11:23:21 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Nov 7, 2012 at 4:59 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; I wrote:<br/>&gt; &gt; What happens if we apply the weighting after getting the multival?<br/>&gt; &gt; e.g. &lt;&lt;1 2 4|| vs &lt;&lt;1 4 4||. What are weighting coefficients here?<br/>&gt;<br/>&gt; I think the weights are 1/p1, 1/p2, and 1/p0p2 where pi is the<br/>&gt; base-2 log of the (0-indexed) ith prime in the smonzo. Right?</p><p>No, they&apos;re going to be 1/p0p1, 1/p0p2, 1/p1p2. If your basis vectors<br/>are e0, e1, and e2, then the first coefficient is for e0^e1, the<br/>second is e0^e2, and the third is e1^e2. Each coefficient represents a<br/>combination of ordinary grade-1 basis vectors. Since the wedge product<br/>is bilinear and respects (ka)^b = a^(kb) = k(a^b) for some scalar k,<br/>the weight for any basis element of the bivector will be the product<br/>of the weights of the basis vectors you wedge to get that bivector.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/8/2012 1:07:57 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:</p><p>&gt;&gt; I think the weights are 1/p1, 1/p2, and 1/p0p2 where pi is the<br/>&gt;&gt; base-2 log of the (0-indexed) ith prime in the smonzo. Right?<br/>&gt;<br/>&gt;No, they&apos;re going to be 1/p0p1, 1/p0p2, 1/p1p2. If your basis vectors<br/>&gt;are e0, e1, and e2, then the first coefficient is for e0^e1, the<br/>&gt;second is e0^e2, and the third is e1^e2. Each coefficient represents a<br/>&gt;combination of ordinary grade-1 basis vectors. Since the wedge product<br/>&gt;is bilinear and respects (ka)^b = a^(kb) = k(a^b) for some scalar k,<br/>&gt;the weight for any basis element of the bivector will be the product<br/>&gt;of the weights of the basis vectors you wedge to get that bivector.</p><p>Thanks. p0p2 was a typo, which I then followed when I did the<br/>calculation. And quite right, I shouldn&apos;t assume p0 = log2(2).<br/>So here&apos;s a correction and a few more things to look at</p><p>2.3.5 81/80<br/> weighted wedgie 1.2311451463736638<br/> weighted wedgie, 1/(p0+p1) etc. 0.9394665497911078<br/> wedged weighted vals 0.5043897296092386<br/> wedged weighted vals, 1/(p0+p1) etc. 0.297447306060962<br/>2.9.5 81/80<br/> weighted wedgie 0.6155725731868319<br/> weighted wedgie, 1/(p0+p1) etc. 0.5628742631174743<br/> wedged weighted vals 0.16389775164911985<br/> wedged weighted vals, 1/(p0+p1) etc. 0.14249183178107935</p><p>These are all RMS. &quot;1/(p0+p1)&quot; means I add the weights instead<br/>of multiplying them (or use the log of the product of the two<br/>basis elements).</p><p>Can you reproduce any of these numbers?</p><p>The only one of these with a theoretical justification known to<br/>me is plain &quot;weighted wedgie&quot;, and that gives the &apos;unwanted&apos;<br/>result that the 2.9.5-based temperament is half as complex. And<br/>none of the rest helps.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/8/2012 2:03:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Nov 8, 2012 at 4:07 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; 2.3.5 81/80<br/>&gt; weighted wedgie 1.2311451463736638<br/>&gt; weighted wedgie, 1/(p0+p1) etc. 0.9394665497911078<br/>&gt; wedged weighted vals 0.5043897296092386<br/>&gt; wedged weighted vals, 1/(p0+p1) etc. 0.297447306060962</p><p>&gt; 2.9.5 81/80<br/>&gt; weighted wedgie 0.6155725731868319<br/>&gt; weighted wedgie, 1/(p0+p1) etc. 0.5628742631174743<br/>&gt; wedged weighted vals 0.16389775164911985<br/>&gt; wedged weighted vals, 1/(p0+p1) etc. 0.14249183178107935<br/>&gt;<br/>&gt; These are all RMS. &quot;1/(p0+p1)&quot; means I add the weights instead<br/>&gt; of multiplying them (or use the log of the product of the two<br/>&gt; basis elements).<br/>&gt;<br/>&gt; Can you reproduce any of these numbers?</p><p>The figures I gave don&apos;t use RMS, they&apos;re using the T2/weighted L2<br/>norm, which is root-sum-squared. I&apos;d rather not use RMS because it<br/>complicates the algebra involved. But for the sake of comparison, if I<br/>weight &lt;&lt;1 4 4|| and take RMS instead of the L2 norm, I get<br/>1.23114514637367, which is what you had above. If I first weight the<br/>vals, take the wedge product and then take the RMS, I also get<br/>1.23114514637367.</p><p>The weighting on the multival is defined so that these two values are<br/>supposed to be identical, which they are for my calculation but not<br/>yours. You must be doing something different for your &quot;wedged weighted<br/>vals&quot; calculation.</p><p>For 2.9.5 81/80, the RMS I get is 0.615572573186832. If I weight the<br/>vals beforehand and then take the wedge product and RMS I still also<br/>get 0.615572573186825.</p><p>I don&apos;t get the &quot;wedged weighted vals, 1/(p0+p1) etc.&quot; thing. How are<br/>you weighting the individual vals so as to get that result when you<br/>wedge them?</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/8/2012 2:41:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; 2.3.5 81/80<br/>&gt;&gt; weighted wedgie 1.2311451463736638<br/>&gt;&gt; weighted wedgie, 1/(p0+p1) etc. 0.9394665497911078<br/>&gt;&gt; wedged weighted vals 0.5043897296092386<br/>&gt;&gt; wedged weighted vals, 1/(p0+p1) etc. 0.297447306060962<br/>&gt;<br/>&gt;&gt; 2.9.5 81/80<br/>&gt;&gt; weighted wedgie 0.6155725731868319<br/>&gt;&gt; weighted wedgie, 1/(p0+p1) etc. 0.5628742631174743<br/>&gt;&gt; wedged weighted vals 0.16389775164911985<br/>&gt;&gt; wedged weighted vals, 1/(p0+p1) etc. 0.14249183178107935<br/>&gt;&gt;<br/>&gt;&gt; These are all RMS. &quot;1/(p0+p1)&quot; means I add the weights instead<br/>&gt;&gt; of multiplying them (or use the log of the product of the two<br/>&gt;&gt; basis elements).<br/>&gt;&gt;<br/>&gt;&gt; Can you reproduce any of these numbers?<br/>&gt;<br/>&gt;The figures I gave don&apos;t use RMS, they&apos;re using the T2/weighted L2<br/>&gt;norm, which is root-sum-squared. I&apos;d rather not use RMS because it<br/>&gt;complicates the algebra involved. But for the sake of comparison, if I<br/>&gt;weight &lt;&lt;1 4 4|| and take RMS instead of the L2 norm, I get<br/>&gt;1.23114514637367, which is what you had above. If I first weight the<br/>&gt;vals, take the wedge product and then take the RMS, I also get<br/>&gt;1.23114514637367.</p><p>Crap, yeah, I used the wedgie weighting on the vals by mistake.</p><p>&gt;I don&apos;t get the &quot;wedged weighted vals, 1/(p0+p1) etc.&quot; thing. How are<br/>&gt;you weighting the individual vals so as to get that result when you<br/>&gt;wedge them?</p><p>Totally incorrectly. Here is the corrected table</p><p>2.3.5 81/80<br/> weighted wedgie 1.2311451463736638<br/> wedged weighted vals 1.2311451463736638<br/> weighted wedgie, 1/(p0+p1) etc. 0.9394665497911078<br/>2.9.5 81/80<br/> weighted wedgie 0.6155725731868319<br/> wedged weighted vals 0.6155725731868319<br/> weighted wedgie, 1/(p0+p1) etc. 0.5628742631174743</p><p>Thanks for looking at this.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/8/2012 2:56:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Nov 8, 2012 at 5:41 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Crap, yeah, I used the wedgie weighting on the vals by mistake.</p><p>There you go. So then, to address this:</p><p>&gt; The only one of these with a theoretical justification known to<br/>&gt; me is plain &quot;weighted wedgie&quot;, and that gives the &apos;unwanted&apos;<br/>&gt; result that the 2.9.5-based temperament is half as complex. And<br/>&gt; none of the rest helps.</p><p>So weighted wedgie and wedged weighted vals are the same thing, as you<br/>know. What you&apos;ve computed here is basically 1/sqrt(3) times the<br/>natural norm induced on multivals that you get if you put the TE norm<br/>on monzos. This is indeed one thing with some sort of theoretical<br/>justification; what its exact interpretation is is something that I&apos;m<br/>still figuring out. That&apos;ll probably be my next post on this topic.</p><p>The thing which you were advocating for, however, is slightly<br/>different. Try taking various types of weighted norm of |-4 4 -1&gt;<br/>under both subgroups and you&apos;ll see it works out to be the same. This<br/>is a second type of complexity which is also theoretically justified.</p><p>If we go back to the old school and start using regular Tenney height<br/>instead of TE, then the difference between these two norms is<br/>heightened. The equivalent of your norm is the weighted L1 norm of the<br/>multimonzo representing the kernel, whereas the equivalent of the<br/>thing you calculated before would be the weighted Linf norm of the<br/>wedgie.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/8/2012 3:16:45 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 02:56 PM 2012/11/08, you wrote:<br/>&gt;On Thu, Nov 8, 2012 at 5:41 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; Crap, yeah, I used the wedgie weighting on the vals by mistake.<br/>&gt;<br/>&gt;There you go. So then, to address this:<br/>&gt;<br/>&gt;&gt; The only one of these with a theoretical justification known to<br/>&gt;&gt; me is plain &quot;weighted wedgie&quot;, and that gives the &apos;unwanted&apos;<br/>&gt;&gt; result that the 2.9.5-based temperament is half as complex. And<br/>&gt;&gt; none of the rest helps.<br/>&gt;<br/>&gt;So weighted wedgie and wedged weighted vals are the same thing, as you<br/>&gt;know. What you&apos;ve computed here is basically 1/sqrt(3) times the<br/>&gt;natural norm induced on multivals that you get if you put the TE norm<br/>&gt;on monzos. This is indeed one thing with some sort of theoretical<br/>&gt;justification; what its exact interpretation is is something that I&apos;m<br/>&gt;still figuring out. That&apos;ll probably be my next post on this topic.</p><p>It&apos;s just Euclidean harmonic distance, right? It isn&apos;t quite as<br/>good as Tenney harmonic distance, but neither is it bad. I don&apos;t<br/>think the choice of L1 or L2 is terribly significant here...</p><p>&gt;The thing which you were advocating for, however, is slightly<br/>&gt;different. Try taking various types of weighted norm of |-4 4 -1&gt;<br/>&gt;under both subgroups and you&apos;ll see it works out to be the same. This<br/>&gt;is a second type of complexity which is also theoretically justified.</p><p>You mean the multimonzo complexity? I don&apos;t want to be accused<br/>of advocating it. I pointed out it was the same under different<br/>subgroups... I agree that reaching a better understanding of the<br/>differences between val-based and comma-based complexity is a<br/>good goal.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/8/2012 6:06:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Nov 8, 2012 at 6:16 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; At 02:56 PM 2012/11/08, Mike wrote:<br/>&gt; &gt;So weighted wedgie and wedged weighted vals are the same thing, as you<br/>&gt; &gt;know. What you&apos;ve computed here is basically 1/sqrt(3) times the<br/>&gt; &gt;natural norm induced on multivals that you get if you put the TE norm<br/>&gt; &gt;on monzos. This is indeed one thing with some sort of theoretical<br/>&gt; &gt;justification; what its exact interpretation is is something that I&apos;m<br/>&gt; &gt;still figuring out. That&apos;ll probably be my next post on this topic.<br/>&gt;<br/>&gt; It&apos;s just Euclidean harmonic distance, right? It isn&apos;t quite as<br/>&gt; good as Tenney harmonic distance, but neither is it bad. I don&apos;t<br/>&gt; think the choice of L1 or L2 is terribly significant here...</p><p>No, I mean the norm on multivals is one thing with some sort of<br/>theoretical justification. The specific thing it&apos;s measuring is hidden<br/>somewhere in this page:<br/><a href="http://en.wikipedia.org/wiki/Operator_norm">http://en.wikipedia.org/wiki/Operator_norm</a></p><p>Multivals are bounded linear operators, so there&apos;s a very interesting<br/>musical interpretation for some of that, which I&apos;ll post about soon<br/>once I have it really nailed down well.</p><p>Basically, it tells you something like, in its worst-case scenario,<br/>how much the temperament increases the complexity of its intervals,<br/>using a very particular definition of &quot;complexity increase&quot; that works<br/>across ranks. But the devil is in the details here.</p><p>I agree that L1 vs L2 doesn&apos;t make much of a difference.</p><p>&gt; &gt;The thing which you were advocating for, however, is slightly<br/>&gt; &gt;different. Try taking various types of weighted norm of |-4 4 -1&gt;<br/>&gt; &gt;under both subgroups and you&apos;ll see it works out to be the same. This<br/>&gt; &gt;is a second type of complexity which is also theoretically justified.<br/>&gt;<br/>&gt; You mean the multimonzo complexity? I don&apos;t want to be accused<br/>&gt; of advocating it. I pointed out it was the same under different<br/>&gt; subgroups... I agree that reaching a better understanding of the<br/>&gt; differences between val-based and comma-based complexity is a<br/>&gt; good goal.</p><p>Well, Paul&apos;s an advocate of it, for whatever reason. I think it&apos;s<br/>useful in its own way.</p><p>-Mike</p></div><h3>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/8/2012 9:43:56 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;&gt; &gt;So weighted wedgie and wedged weighted vals are the same thing, as you<br/>&gt;&gt; &gt;know. What you&apos;ve computed here is basically 1/sqrt(3) times the<br/>&gt;&gt; &gt;natural norm induced on multivals that you get if you put the TE norm<br/>&gt;&gt; &gt;on monzos. This is indeed one thing with some sort of theoretical<br/>&gt;&gt; &gt;justification; what its exact interpretation is is something that I&apos;m<br/>&gt;&gt; &gt;still figuring out. That&apos;ll probably be my next post on this topic.<br/>&gt;&gt;<br/>&gt;&gt; It&apos;s just Euclidean harmonic distance, right? It isn&apos;t quite as<br/>&gt;&gt; good as Tenney harmonic distance, but neither is it bad. I don&apos;t<br/>&gt;&gt; think the choice of L1 or L2 is terribly significant here...<br/>&gt;<br/>&gt;No, I mean the norm on multivals is one thing with some sort of<br/>&gt;theoretical justification.</p><p>Thanks for clarifying - I thought the 2nd half of the paragraph<br/>referred to the TE norm on monzos.</p><p>-Carl</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/8/2012 10:07:55 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Sorry Keenan, I missed this post.</p><p>On Tue, Nov 6, 2012 at 2:38 AM, Keenan Pepper &lt;<a href="mailto:keenanpepper@gmail.com">keenanpepper@gmail.com</a>&gt;<br/>wrote:<br/>&gt;<br/>&gt; I&apos;m confused about the difference in meaning or interpretation of T1<br/>&gt; multimonzo complexity vs T1 val complexity (these are not duals, they just<br/>&gt; seem like they&apos;re very similar).</p><p>First off, assuming that you put the T1 norm on monzos, then you&apos;ll<br/>naturally induce up a norm on multimonzos which we might as well also<br/>just call the T1 norm for multimonzos. For &quot;nice&quot; subgroups it&apos;s going<br/>to be a weighted L1 norm, where the weights for each basis multivector<br/>of the exterior power are the products of the weights of the basis<br/>vectors you have to wedge to get it. Then, the natural norm induced on<br/>multivals is NOT a T1 norm, but something kind of like a Tinf norm,<br/>but where you&apos;re using 1/log2(p) weighting rather than log2(p)<br/>weighting. Maybe I&apos;ll call this the Tinf* norm from now on.</p><p>I&apos;m not sure if, when you say &quot;T1 multival complexity,&quot; you mean the<br/>complexity on multivals which is ultimately derived from the T1 norm<br/>on monzos (e.g. the result is the Tinf* norm on multivals), or you<br/>mean putting the T1 norm on multivals itself. I&apos;ll assume the former.</p><p>One interpretation of the T1 norm of the kernel of a temperament is in<br/>how efficient the temperament is, overall, in using tempered intervals<br/>to represent a wide array of simple JI intervals. If the kernel is<br/>simple then it &quot;appears often&quot; between simple JI intervals, or is<br/>reachable by very small and simple circuits using only simple JI<br/>intervals; therefore, if you temper a very simple kernel out then we<br/>can expect to see, as a rule, less notes standing in for more JI<br/>intervals.</p><p>The interpretation of the Tinf* norm on the multival for the<br/>temperament is a bit more complex. Multivals are linear functionals on<br/>same-grade multimonzos, and in that capacity they&apos;re also bounded<br/>linear operators; the norm in this case is telling you what the bound<br/>is. So if V is a multival and M is a multimonzo of the same grade,<br/>then ||V|| = sup |V(M)|/||M||. It&apos;s a bit mysterious and pinning down<br/>the meaning of that is what I&apos;m working on now.</p><p>&gt; If you take a bunch of good temperaments<br/>&gt; (e.g. the Middle Path 5-limit list) and plot them with T1 multimonzo<br/>&gt; complexity on one axis and T1 multival complexity on the other axis, what<br/>&gt; does the scatter plot look like? What does the axis orthogonal to the<br/>&gt; simple-complex axis represent?</p><p>So assuming my interpretation before was correct, and that T1 multival<br/>complexity means the Linf-based complexity on multivals which is<br/>derived from the original T1 norm on monzos, then I&apos;m not sure. I<br/>haven&apos;t done it yet. I&apos;ve been wondering about things like the ratio<br/>of the two quantities as well.</p><p>-Mike</p></div><h3>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/9/2012 8:51:32 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; I&apos;m not sure if, when you say &quot;T1 multival complexity,&quot; you mean the<br/>&gt; complexity on multivals which is ultimately derived from the T1 norm<br/>&gt; on monzos (e.g. the result is the Tinf* norm on multivals), or you<br/>&gt; mean putting the T1 norm on multivals itself. I&apos;ll assume the former.</p><p>What if we&apos;re talking about the latter? Is there any actual difference between using the T1 norm on multimonzos and using the T1 norm (not Tinf*) on multivals?</p><p>Keenan</p></div><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/9/2012 11:22:59 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, Nov 9, 2012 at 11:51 AM, Keenan Pepper &lt;<a href="mailto:keenanpepper@gmail.com">keenanpepper@gmail.com</a>&gt;<br/>wrote:<br/>&gt;<br/>&gt; What if we&apos;re talking about the latter? Is there any actual difference<br/>&gt; between using the T1 norm on multimonzos and using the T1 norm (not Tinf*)<br/>&gt; on multivals?</p><p>Do you mean the T1 norm here and not the T1* norm? Meaning you want us<br/>to weight vals with log2(p) weighting instead of 1/log2(p)?</p><p>-Mike</p></div>