<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning-math A weird structural issue with infinite-limit JI</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning-math">back to list</a><h1>A weird structural issue with infinite-limit JI</h1><h3><a id=21045 href="#21045">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>10/21/2012 9:36:18 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>So let&apos;s say we want to deal with infinite-limit JI, which is just the<br/>multiplicative group of Q+ in general. By the prime factorization<br/>theorem, Q+ is isomorphic to the direct sum of an infinite number of<br/>Z&apos;s. Since it&apos;s the direct sum and not the direct product, and only<br/>finitely many nonzero elements are allowed in the resulting monzo, Q+<br/>is still a free abelian group. |1 0 0 ...&gt;, |0 1 0 ...&gt;, |0 0 1 ...&gt;,<br/>etc serve as a (Hamel) basis for this group.</p><p>However, vals are a different story. The group Hom(Q+,Z) admits vals<br/>with infinitely many nonzero elements, so now we&apos;re looking at a group<br/>which is the direct -product- of an infinite number of Z&apos;s. This is<br/>NOT a free abelian group, but a &quot;Baer-Specker group&quot; which has no<br/>basis. However, if one considers Schauder bases that admit infinite<br/>summations, then &lt;1 0 0 ...|, &lt;0 1 0 ...|, &lt;0 0 1 ...|, etc serve as a<br/>Schauder basis for this group.</p><p>Much of the work done in the past few years has involved embedding<br/>these groups into vector spaces, so that we can define norms, tuning<br/>maps, have the Hahn-Banach theorem to work with, etc. If we embed the<br/>group of infinite-limit JI monzos into a vector space, we find that |1<br/>0 0 ...&gt;, |0 1 0 ...&gt;, |0 0 1 ...&gt; is a (Hamel) basis for this space.<br/>The dual space of this space is R^N, the direct -product- of a<br/>countably infinite number of R&apos;s, and so we get &lt;1 0 0 ...|, &lt;0 1 0<br/>...|, &lt;0 0 1 ...| as a Schauder basis for this space. However, these<br/>vectors don&apos;t serve as a Hamel basis for the dual space, as the JIP,<br/>for instance, isn&apos;t expressible as a finite linear combination of<br/>these vectors.</p><p>However, it is a theorem of ZFC that a Hamel basis exists for R^N,<br/>though it must be uncountable. Can a construction be given of such a<br/>basis?</p><p>-Mike</p></div><h3><a id=21046 href="#21046">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>10/22/2012 7:44:09 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; However, it is a theorem of ZFC that a Hamel basis exists for R^N,<br/>&gt; though it must be uncountable. Can a construction be given of such a<br/>&gt; basis?</p><p>Can I use Zorn&apos;s Lemma?</p></div><h3><a id=21047 href="#21047">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>10/22/2012 7:56:58 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Oct 22, 2012 at 10:44 AM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt;<br/>&gt; wrote:<br/>&gt;<br/>&gt; &gt; However, it is a theorem of ZFC that a Hamel basis exists for R^N,<br/>&gt; &gt; though it must be uncountable. Can a construction be given of such a<br/>&gt; &gt; basis?<br/>&gt;<br/>&gt; Can I use Zorn&apos;s Lemma?</p><p>I&apos;m hoping to find a way to express your Hamel basis in terms of the<br/>Schauder basis &lt;1 0 0 0 ...|, &lt;0 1 0 0 ...|, &lt;0 0 1 0 ...|, &lt;0 0 0 1<br/>...|, etc. Put another way, what I really want to know is what the<br/>actual vals are in this Hamel basis, i.e. how they map 2/1, 3/1, etc.</p><p>So, if by using Zorn&apos;s lemma you just mean you&apos;re going to lay out the<br/>standard proof that every vector space has a basis without expressing<br/>it in terms of the standard Schauder basis, then no :)</p><p>-Mike</p></div><h3><a id=21048 href="#21048">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>10/22/2012 9:11:21 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; So, if by using Zorn&apos;s lemma you just mean you&apos;re going to lay out the<br/>&gt; standard proof that every vector space has a basis without expressing<br/>&gt; it in terms of the standard Schauder basis, then no :)</p><p>You could use Zorn&apos;s Lemma starting from the Schauder basis. This whole thing strikes me as a pretty dubious enterprise, though.</p></div><h3><a id=21086 href="#21086">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/11/2012 12:19:43 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Oct 22, 2012 at 12:11 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt;<br/>&gt; wrote:<br/>&gt;<br/>&gt; &gt; So, if by using Zorn&apos;s lemma you just mean you&apos;re going to lay out the<br/>&gt; &gt; standard proof that every vector space has a basis without expressing<br/>&gt; &gt; it in terms of the standard Schauder basis, then no :)<br/>&gt;<br/>&gt; You could use Zorn&apos;s Lemma starting from the Schauder basis. This whole<br/>&gt; thing strikes me as a pretty dubious enterprise, though.</p><p>Sorry, I missed this reply. Maybe this particular approach is dubious,<br/>but my goal with this was to eventually figure out something like<br/>tuning optimizations in infinite-limit JI, and this involves putting a<br/>norm on vals and tuning maps. But vals can have infinitely many<br/>nonzero coordinates, so how does something like the L2 norm even work?<br/>I figured I needed to work out some sort of basis first, but maybe<br/>not.</p><p>I guess we need to restrict our attention to the continuous dual<br/>space, and that Keenan&apos;s work earlier showing that infinite-limit TOP<br/>error is bounded for GPV&apos;s shows that most (all?) GPV&apos;s are in the<br/>continuous dual space.</p><p>-Mike</p></div><h3><a id=21087 href="#21087">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/11/2012 2:26:04 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;my goal with this was to eventually figure out something like<br/>&gt;tuning optimizations in infinite-limit JI, and this involves putting a<br/>&gt;norm on vals and tuning maps. But vals can have infinitely many<br/>&gt;nonzero coordinates, so how does something like the L2 norm even work?<br/>&gt;I figured I needed to work out some sort of basis first, but maybe<br/>&gt;not.<br/>&gt;I guess we need to restrict our attention to the continuous dual<br/>&gt;space, and that Keenan&apos;s work earlier showing that infinite-limit TOP<br/>&gt;error is bounded for GPV&apos;s shows that most (all?) GPV&apos;s are in the<br/>&gt;continuous dual space.</p><p>What do you mean by &apos;being in the continuous dual space&apos;?</p><p>Keenan&apos;s TOP-FP vals show that the weighted TOP error of vals<br/>rapidly stops changing. What about the weighted complexity?<br/>I think the weighted error works because you get to pick the best<br/>approximation to each new prime whilst the weights are going down.<br/>But the complexity terms aren&apos;t bounded to anything like &apos;half a<br/>step&apos;, are they? With infinitely many primes to map, it seems like<br/>the complexity must get infinitely large.</p><p>For any average complexity like L2 though, the prime number<br/>theorem does mean we&apos;ll have to consider absolutely huge numbers<br/>to change it. With an infinite val like &lt;1 2 3 4...|, some large<br/>prime p will have mapping approximately p/log(p) and weight<br/>1/log(p), and will contribute only log(p)/p to the average.<br/>That means its influence is 1/log(p). Since we&apos;re interested in<br/>only the simplest temperaments, vals not much worse than &lt;1 2 3...|<br/>seem reasonable. Hm.</p><p>-Carl</p></div><h3><a id=21088 href="#21088">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/11/2012 2:41:21 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I wrote:<br/>&gt;For any average complexity like L2 though, the prime number<br/>&gt;theorem does mean we&apos;ll have to consider absolutely huge numbers<br/>&gt;to change it. With an infinite val like &lt;1 2 3 4...|, some large<br/>&gt;prime p will have mapping approximately p/log(p) and weight<br/>&gt;1/log(p), and will contribute only log(p)/p to the average.<br/>&gt;That means its influence is 1/log(p).</p><p>Or more closely, p/log^3(p) for L2. -Carl</p></div><h3><a id=21089 href="#21089">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/12/2012 12:47:21 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Sun, Nov 11, 2012 at 5:26 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; What do you mean by &apos;being in the continuous dual space&apos;?</p><p>Well, first off, I screwed up in what I said - it&apos;s not that the val<br/>is in the continuous dual space, it&apos;s that the optimum error map for<br/>every GPV seems to be in the continuous dual space.</p><p>But basically, for any finite-dimensional vector space, there&apos;s only<br/>one dual space - you know, the dual space. This space is the set of<br/>linear functionals mapping from vectors to scalars (aka tuning<br/>maps/vals) and they all have this lovely property of being &quot;bounded<br/>linear operators,&quot; which is basically what powers all of tuning<br/>optimization now.</p><p>When you take the difference from some tuning map to the JIP to get an<br/>error map e, there is a bound on |&lt;e|m&gt;|/||m|| over all monzos m. Note<br/>that |&lt;e|m&gt;| is the error of that monzo and ||m|| is its complexity,<br/>so this is weighted error we&apos;re looking at. So the max weighted error<br/>of every error map is bounded. More generally, the max &quot;weighted<br/>mapping&quot; of every tuning map is bounded.</p><p>For finite dimensional vector spaces, this gets messy. There are now<br/>two dual spaces: the algebraic dual space and the &quot;continuous&quot; dual<br/>space. So in our case, our vector space is the set of all monzos with<br/>only finitely many nonzero coordinates: e.g. |1 2 3 4 0 0 0 ...&gt; is<br/>fine, but |1 1 1 1 1 1 1 1 ...&gt; is not. If you work the usual dual<br/>space out by looking at the set of linear functionals that map<br/>infinite-dimensional monzos to scalars, this is called the &quot;algebraic<br/>dual space&quot;; it consists of all vals which can have infinitely many<br/>nonzero coordinates: so &lt;12 19 28 ... round(12*log2(p)) ...| is fine<br/>for all p.</p><p>But not all of these vals have that nice bounded property. The ones<br/>which do form the &quot;continuous dual space,&quot; a subspace which you can<br/>read about here:</p><p><a href="http://en.wikipedia.org/wiki/Dual_space#Continuous_dual_space">http://en.wikipedia.org/wiki/Dual_space#Continuous_dual_space</a></p><p>This continuous dual space is a subspace of the algebraic dual space.<br/>For this space and only this space does the concept of a dual norm<br/>make sense. The choice norm you put on monzos affects which tuning<br/>maps get identified as being in this privileged continuous dual space.<br/>For instance, it looks like T1-weighted error is bounded for all GPV&apos;s<br/>- but is T2-weighted error bounded for these vals? (I think it was<br/>shown previously that it&apos;s not; TE error doesn&apos;t converge as the limit<br/>goes up.)</p><p>So basically, the continuous dual space is more well-behaved for our<br/>purposes where the infinite-limit is involved. And that&apos;s all I&apos;ve<br/>pieced together so far, so I don&apos;t want to say anything else yet and<br/>lead anyone in the wrong direction.</p><p>&gt; Keenan&apos;s TOP-FP vals show that the weighted TOP error of vals<br/>&gt; rapidly stops changing. What about the weighted complexity?</p><p>That&apos;s a good question, maybe Keenan can answer.</p><p>&gt; I think the weighted error works because you get to pick the best<br/>&gt; approximation to each new prime whilst the weights are going down.<br/>&gt; But the complexity terms aren&apos;t bounded to anything like &apos;half a<br/>&gt; step&apos;, are they? With infinitely many primes to map, it seems like<br/>&gt; the complexity must get infinitely large.</p><p>If we&apos;re talking about &quot;T1 complexity,&quot; meaning just the weighted Linf<br/>norm of the val, I think it should be bounded for GPV&apos;s, since the<br/>weighted error of a prime can never be more than<br/>round(n*log2(p))/log2(p), where n is the raw and unrounded number of<br/>steps to the octave that you&apos;re putting into the GPV calculation.</p><p>To see that this value is bounded across all p, note that<br/>round(n*log2(p)) &lt; n*log2(p)+1, so round(n*log2(p))/log2(p) &lt;<br/>n+1/log2(p). The p which maximizes the right hand side is p=2, which<br/>means round(n*log2(p))/log2(p) &lt; n+1.</p><p>So in other words, once you put the n-EDO GPV into weighted<br/>coordinates, the weighted values must be less than n+1. So yeah, for<br/>weighted T1 complexity, things are good. For T2 complexity or anything<br/>that involves taking an infinite sum of some kind, this is definitely<br/>going to snap.</p><p>&gt; For any average complexity like L2 though, the prime number<br/>&gt; theorem does mean we&apos;ll have to consider absolutely huge numbers<br/>&gt; to change it. With an infinite val like &lt;1 2 3 4...|, some large<br/>&gt; prime p will have mapping approximately p/log(p) and weight<br/>&gt; 1/log(p), and will contribute only log(p)/p to the average.<br/>&gt; That means its influence is 1/log(p). Since we&apos;re interested in<br/>&gt; only the simplest temperaments, vals not much worse than &lt;1 2 3...|<br/>&gt; seem reasonable. Hm.</p><p>But even if you divide by log(p), L2 breaks here. L2 isn&apos;t RMS, it&apos;s<br/>root-sum-squared, so you basically just have the sum-squared of an<br/>infinite number of positive values inside of a square root. You could<br/>perhaps try to make it RMS, but that requires you to divide by &quot;the<br/>square root of infinity.&quot; Maybe there&apos;s a better way to formalize the<br/>means of infinite data sets that I don&apos;t know of. (Or maybe<br/>nonstandard analysis might be useful here.)</p><p>So far, it looks like L1 is the name of the game for infinite-dimensional JI.</p><p>-Mike</p></div><h3><a id=21090 href="#21090">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/12/2012 12:40:52 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:</p><p>&gt; But even if you divide by log(p), L2 breaks here. L2<br/>&gt; isn&apos;t RMS, it&apos;s root-sum-squared, so you basically just<br/>&gt; have the sum-squared of an infinite number of positive<br/>&gt; values inside of a square root. You could perhaps try to<br/>&gt; make it RMS, but that requires you to divide by &quot;the<br/>&gt; square root of infinity.&quot; Maybe there&apos;s a better way to<br/>&gt; formalize the means of infinite data sets that I don&apos;t<br/>&gt; know of. (Or maybe nonstandard analysis might be useful<br/>&gt; here.)</p><p>An RMS may converge in the limit of the prime limit<br/>approaching infinity.  It depends on the weighting.  You<br/>may be able to predict how much you&apos;d expect it to increase<br/>for random errors (within half a scale step for equal<br/>temperaments) and normalize it to be relative to that.</p><p>                   Graham</p></div><h3><a id=21091 href="#21091">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/12/2012 1:00:27 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;&gt; I think the weighted error works because you get to pick the best<br/>&gt;&gt; approximation to each new prime whilst the weights are going down.<br/>&gt;&gt; But the complexity terms aren&apos;t bounded to anything like &apos;half a<br/>&gt;&gt; step&apos;, are they? With infinitely many primes to map, it seems like<br/>&gt;&gt; the complexity must get infinitely large.<br/>&gt;<br/>&gt;If we&apos;re talking about &quot;T1 complexity,&quot; meaning just the weighted Linf<br/>&gt;norm of the val, I think it should be bounded for GPV&apos;s, since the<br/>&gt;weighted error of a prime can never be more than<br/>&gt;round(n*log2(p))/log2(p), where n is the raw and unrounded number of<br/>&gt;steps to the octave that you&apos;re putting into the GPV calculation.</p><p>Derr, quite right. Each element is approximately n. Forget<br/>everything I said. -Carl</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            