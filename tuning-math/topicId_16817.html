<a href="/tuning-math">back to list</a><h1>Volume of a matrix</h1><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>10/17/2007 5:41:28 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Here&apos;s a paper about the volume of an m x n matrix:</p><p><a href="http://benisrael.net/VOLUME.pdf">http://benisrael.net/VOLUME.pdf</a></p><p>This kind of volume of the weighted mapping of a regular temperament is proportional to what I call scalar complexity.  (You could define the complexity to be equal to this volume, but I have my reasons for normalizing.)  I already knew it was a volume because of the exterior algebra definition, but this paper confirms it.</p><p>I don&apos;t think the paper is wonderfully informative in itself.  But you may find it useful as background.  It uses both matrix and exterior algebra.  It mentions linear least squares approximations, singular value decompositions (a method of solving linear least squares problems that may be particularly relevant to us) and Cramer&apos;s rule (which is tantalizingly close to the determinant form of the TOP-RMS error that I&apos;m trying to prove).  There&apos;s also a section on weighted least squares, which is the exact equivalent to prime-weighted RMS temperament optimizatons.</p><p>It turns out that simple badness is a similar volume involving the covariance matrix of the weighted mapping. You can look up &quot;covariance matrix&quot; in either Mathworld or Wikipedia.  The Wikipedia article has a little bit relating it to the variance.  From my PDF, you&apos;ll see that the badness of an equal temperament is the standard deviation of the weighted mapping.  In higher dimensions it&apos;s the volume of the natural generalization of the standard deviation. You can also see the scalar complexity as a generalization of RMS to higher dimensions.  So really, the TOP-RMS error of any regular temperament class is simply the generalization of STD(M)/RMS(M).  Except I can&apos;t find this mentioned in the literature on linear least squares optimizations, and there&apos;s a simple identity that that it depends on, which I can&apos;t prove.  In maxima syntax:</p><p>expand(determinant(I - X.transpose(X)) - 1 + transpose(X).X);</p><p>is always zero, where I is the identity matrix and X is *any* column vector (verified in 10 dimensions).  If it isn&apos;t generally true it at least is for all cases we care about.  And it neatly ties up the error/complexity theory.</p><p>And, while I&apos;m here, I&apos;ll explain a teaser I left before and nobody picked up on.  The scalar complexity is also proportional to the volume of the matrix of weighted unison vectors (weighting is done by multiplying by the size of the prime -- the dual of the way you weight mappings).  I said it was possible to define a rank 0 temperament, where the number of unison vectors equals the number of primes.  Well, this is close to a periodicity block.  For a true periodicity block you either need to use octave-equivalent coordinates or include the octave in place of a unison vector.  Well, you know that the determinant of the matrix of unison vectors gives you the number of notes in the periodicity block (as Fokker showed).  As you may have guessed by now, this determinant is simply a special case of the scalar complexity.  So Fokker&apos;s determinant is the formula for the scalar complexity of a periodicity block! With proper normalization, it&apos;s independent of the weighting.</p><p>                  Graham</p></div>