<a href="/tuning-math">back to list</a><h1>Complete list of TOP self-consistent vals up to 120</h1><h3><a id=19981 href="#19981">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/5/2011 7:18:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>The following infinite vals are generalized patent vals for their *own* TOP tunings (the tunings with the least TOP-max error). Furthermore, I claim these are the *only* vals below 120 steps per 2/1 with this property.</p><p>All other infinite vals are not patent for their own TOP tunings, which means that the generalized patent val for that tuning is some other val (with, in turn, a possibly different TOP tuning).</p><p>This is great news for people looking to create no-limit temperaments, because we have a well-defined and *discrete* set of vals to work with. Note that you can&apos;t do this for TE, because the TE error doesn&apos;t converge (unless you analytically continue it or something). But TOP is no worse than TE; I remember when everybody was using TOP. Even <a href="http://xenharmonic.wikispaces.com/Tenney-Euclidean+tuning">http://xenharmonic.wikispaces.com/Tenney-Euclidean+tuning</a> says &quot;there are theoretical arguments favoring TOP&quot;.</p><p>Here are the first few in long-winded format:</p><p>First is the tuning, in steps per octave<br/>Next comes the beginning of the infinite val, which is the GPV of that tuning<br/>Next, the flattest and sharpest primes, which determing the TOP tuning<br/>Next, the TOP error in cents per octave</p><p>0.815464876786<br/>Val: [1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, ...] (all of the following should have &quot;...&quot; because they&apos;re infinite vals)<br/>Worst primes: 3 2<br/>Error: 271.553262637</p><p>1.06160631164<br/>Val: [1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5]<br/>Worst primes: 5 3<br/>Error: 226.358709403</p><p>1.14601483711<br/>Val: [1, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6]<br/>Worst primes: 2 5<br/>Error: 152.893137906</p><p>1.86135311615<br/>Val: [2, 3, 4, 5, 6, 7, 8, 8, 8, 9, 9]<br/>Worst primes: 5 2<br/>Error: 89.3845768332</p><p>2.02252493568<br/>Val: [2, 3, 5, 6, 7, 7, 8, 9, 9, 10, 10]<br/>Worst primes: 13 5<br/>Error: 77.6402915263</p><p>2.02734724807<br/>Val: [2, 3, 5, 6, 7, 8, 8, 9, 9, 10, 10]<br/>Worst primes: 3 13<br/>Error: 79.6457464189</p><p>2.26185950714<br/>Val: [2, 4, 5, 6, 8, 8, 9, 10, 10, 11, 11]<br/>Worst primes: 2 3<br/>Error: 138.926139125</p><p>2.76185950714<br/>Val: [3, 4, 6, 8, 10, 10, 11, 12, 12, 13, 14]<br/>Worst primes: 3 2<br/>Error: 103.469633661</p><p>3.00215313236<br/>Val: [3, 5, 7, 8, 10, 11, 12, 13, 14, 15, 15]<br/>Worst primes: 7 3<br/>Error: 60.9545064918</p><p>The few vals with the least TOP errors relative to there size are:</p><p>12.0753641528<br/>Val: [12, 19, 28, 34, 42, 45, 49, 51, 55, 59, 60, 63, 65, 66, 67, 69]<br/>Worst primes: 3 43<br/>Error: 8.71514934392</p><p>25.9262587256<br/>Val: [26, 41, 60, 73, 90, 96, 106, 110, 117, 126, 128, 135, 139, 141, 144]<br/>Worst primes: 31 11<br/>Error: 4.14601940058</p><p>28.9385927663<br/>Val: [29, 46, 67, 81, 100, 107, 118, 123, 131, 141, 143, 151, 155, 157, 161, 166]<br/>Worst primes: 7 29<br/>Error: 3.55831859325</p><p>30.9950566273<br/>Val: [31, 49, 72, 87, 107, 115, 127, 132, 140, 151, 154, 161, 166, 168, 172]<br/>Worst primes: 37 31<br/>Error: 3.47291662817</p><p>45.9931176763<br/>Val: [46, 73, 107, 129, 159, 170, 188, 195, 208, 223, 228, 240, 246, 250, 255]<br/>Worst primes: 29 5<br/>Error: 2.3292364275</p><p>49.9331483721<br/>Val: [50, 79, 116, 140, 173, 185, 204, 212, 226, 243, 247, 260, 268, 271, 277]<br/>Worst primes: 3 41<br/>Error: 2.1556303066</p><p>58.0580443925<br/>Val: [58, 92, 135, 163, 201, 215, 237, 247, 263, 282, 288, 302, 311, 315, 322]<br/>Worst primes: 47 19<br/>Error: 1.81798494881</p><p>58.060825976<br/>Val: [58, 92, 135, 163, 201, 215, 237, 247, 263, 282, 288, 302, 311, 315, 323]<br/>Worst primes: 37 47<br/>Error: 1.8454963235</p><p>80.0639853429<br/>Val: [80, 127, 186, 225, 277, 296, 327, 340, 362, 389, 397, 417, 429, 434, 445, 459, 471, 475, 486]<br/>Worst primes: 43 7<br/>Error: 1.23848578403</p><p>80.0780680651<br/>Val: [80, 127, 186, 225, 277, 296, 327, 340, 362, 389, 397, 417, 429, 435, 445, 459]<br/>Worst primes: 13 43<br/>Error: 1.31233467614</p><p>99.0887013374<br/>Val: [99, 157, 230, 278, 343, 367, 405, 421, 448, 481, 491, 516, 531, 538, 550]<br/>Worst primes: 2 13<br/>Error: 1.07420526719</p><p>102.929503781<br/>Val: [103, 163, 239, 289, 356, 381, 421, 437, 466, 500, 510, 536, 551, 559, 572]<br/>Worst primes: 3 43<br/>Error: 1.02540802112</p><p>And without further ado, here is the complete list (the first entries of the val are omitted, because they are easy to compute):</p><p>0.815464876786 3 2 271.553262637<br/>1.06160631164 5 3 226.358709403<br/>1.14601483711 2 5 152.893137906<br/>1.86135311615 5 2 89.3845768332<br/>2.02252493568 13 5 77.6402915263<br/>2.02734724807 3 13 79.6457464189<br/>2.26185950714 2 3 138.926139125<br/>2.76185950714 3 2 103.469633661<br/>3.00215313236 7 3 60.9545064918<br/>3.08924219134 13 7 45.3013983674<br/>3.12142892656 2 13 46.6820534136<br/>3.87892137107 11 2 37.4574116929<br/>3.91512086522 13 11 40.3931447649<br/>3.91957541892 3 13 41.0239017758<br/>4.2082541375 2 3 59.3844755651<br/>4.7082541375 3 2 74.3577183337<br/>4.89244008369 5 3 38.0173350469<br/>5.04111037214 11 5 30.2334165362<br/>5.09503374662 7 11 25.4678887283<br/>5.17155390331 2 7 39.8071233175<br/>5.79939762748 5 2 41.5082500788<br/>5.85391979759 3 5 35.9865570105<br/>6.15464876786 2 3 30.152577089<br/>6.87797693034 13 2 21.2893536971<br/>6.89706428508 7 13 22.4665552806<br/>7.00748433567 5 7 19.9773957509<br/>7.13086438827 3 5 32.0808959926<br/>7.28557852143 2 3 47.0373388615<br/>7.78557852143 3 2 33.0490243695<br/>8.10104339821 2 3 14.9674643002<br/>8.80676558073 5 2 26.3299052295<br/>8.9258136179 17 5 15.9170836467<br/>8.94254330419 3 17 14.6973965407<br/>9.23197315179 2 3 30.152577089<br/>9.73197315179 3 2 33.0490243695<br/>9.96154007598 11 3 16.0620924255<br/>10.0022216413 19 11 13.8026071395<br/>10.0140720552 5 19 13.0030483831<br/>10.1681186969 2 5 19.8406846213<br/>10.8431078066 7 2 17.3631615044<br/>10.8841143055 3 7 17.4538955075<br/>11.1783677821 2 3 19.147816814<br/>11.814133534 5 2 18.8790619785<br/>11.9068904003 7 5 15.3242252813<br/>11.9813511204 11 7 12.9895107839<br/>12.0156007501 13 11 12.4959500114<br/>12.0697455519 43 13 9.04457150884<br/>12.0753641528 3 43 8.71514934392<br/>12.3092975357 2 3 30.152577089<br/>12.8092975357 3 2 17.8653791517<br/>13.0849107836 5 3 15.0965377561<br/>13.1754866501 2 5 15.9830134368<br/>13.8908249292 5 2 9.43141142868<br/>14.0463904975 3 5 14.1761047257<br/>14.2556921661 2 3 21.5233743624<br/>14.7556921661 3 2 19.8682242361<br/>15.0329985775 17 3 8.72604419045<br/>15.0645177349 7 17 8.26970062154<br/>15.1584545228 2 7 12.5438531415<br/>15.8365581164 7 2 12.3846519494<br/>15.9012836296 3 7 9.6626003236<br/>16.2020867964 2 3 14.9674643002<br/>16.7020867964 3 2 21.4042621525<br/>16.8884205703 7 3 10.4224862638<br/>16.947165373 5 7 10.6764479965<br/>17.1135311615 2 5 7.96079970148<br/>17.8288694405 5 2 11.5182105113<br/>17.8772242695 3 5 14.1761047257<br/>18.1484814268 2 3 9.81777527019<br/>18.8946068553 11 2 6.69353824458<br/>18.95818514 17 11 7.60157553383<br/>18.980861601 7 17 6.44105750666<br/>19.0815403555 3 7 9.6626003236<br/>19.2794111804 2 3 17.3912685036<br/>19.7794111804 3 2 13.3829354755<br/>20.0004368928 5 3 11.3586651613<br/>20.0947003537 7 5 8.78427882639<br/>20.1519048326 2 7 9.04558653928<br/>20.8201746563 43 2 10.364486176<br/>20.9619166067 3 5 8.08521901039<br/>21.2258058107 2 3 12.7659215991<br/>21.7258058107 3 2 15.144801993<br/>21.9055898943 7 3 9.69349731606<br/>21.9851269085 23 7 5.44288105048<br/>21.9978817272 13 23 5.92372774336<br/>22.0620165624 5 13 5.30389053273<br/>22.1975905099 2 5 10.6817274507<br/>22.8567355643 3 2 7.52151690135<br/>23.1722004411 2 3 8.91760494701<br/>23.843605347 5 2 7.87102373348<br/>23.9918843942 7 5 6.3022740123<br/>24.0987096795 3 7 6.14368381324<br/>24.3031301946 2 3 14.9674643002<br/>24.8031301946 3 2 9.52475613255<br/>25.0858466202 7 3 7.23962917926<br/>25.1349753265 5 7 7.43513559971<br/>25.2049584632 2 5 9.75800678892<br/>25.8234587359 7 2 8.2037622884<br/>25.8649471006 11 7 6.41072526889<br/>25.9262587256 31 11 4.14601940058<br/>25.9395628267 5 31 4.57845845767<br/>26.0696949695 3 5 9.27859293758<br/>26.249524825 2 3 11.4070556323<br/>26.749524825 3 2 11.2364691323<br/>26.9159630021 5 3 9.54153792653<br/>27.0078260031 11 5 5.54493303604<br/>27.0709953338 23 11 4.4814757102<br/>27.0954808616 2 23 4.22863999162<br/>27.8751116633 11 2 5.37633735427<br/>27.900098655 3 11 5.98662389979<br/>28.1929075927 5 3 8.46729202306<br/>28.2123264164 2 5 9.0312190475<br/>28.6959194554 3 2 12.7159770621<br/>28.9206626044 29 3 4.2366689025<br/>28.9385927663 7 29 3.55831859325<br/>29.0321593669 5 7 7.30899718766<br/>29.1430029745 2 5 5.88832830799<br/>29.8268492089 3 2 6.96623863384<br/>30.1423140857 2 3 5.66568652866<br/>30.7890178116 5 2 8.22301729866<br/>30.907930893 13 5 3.91283215821<br/>30.9772063141 29 13 3.88084492924<br/>30.9828909743 31 29 3.87235886987<br/>30.9950566273 37 31 3.47291662817<br/>31.006435283 3 37 3.51710309945<br/>31.2732438393 2 3 10.4847648306<br/>31.7732438393 3 2 8.56404194147<br/>32.0237413648 5 3 5.75858013364<br/>32.1503709278 2 5 5.61253597068<br/>32.8657092068 5 2 4.90325496521<br/>32.9852210787 3 5 6.43465966334<br/>33.2196384696 2 3 7.93404671797<br/>33.7196384696 3 2 9.97738563332<br/>33.945395686 11 3 4.41218036121<br/>33.9746661404 7 11 4.76767122537<br/>34.0979449812 2 7 3.4469519347<br/>34.8505682232 3 2 5.14534314027<br/>35.1660331 2 3 5.66568652866<br/>35.87307716 5 2 4.24573022452<br/>36.0592527085 13 5 3.91283215821<br/>36.0874543234 3 13 4.13855858255<br/>36.2969628536 2 3 9.81777527019<br/>36.7969628536 3 2 6.62132297937<br/>37.1124277304 2 3 3.63525871737<br/>37.8789809167 7 2 3.83386502<br/>37.9849771174 3 7 4.08135782953<br/>38.2433574839 2 3 7.63607067811<br/>38.7433574839 3 2 7.94900182341<br/>38.939267474 5 3 5.497091003<br/>39.0957833923 2 5 2.93996080482<br/>39.8111216714 5 2 5.69323306737<br/>39.9007471879 3 5 4.57653724213<br/>40.1897521143 2 3 5.66568652866<br/>40.6897521143 3 2 9.14966161038<br/>40.8842872465 11 3 3.70254914513<br/>40.9265833278 13 11 3.53673332585<br/>40.9664200033 17 13 3.21569137637<br/>40.9991330128 23 17 2.99005531888<br/>41.016156348 5 23 2.98077655636<br/>41.1724747875 2 5 5.0268959079<br/>41.8206818679 3 2 5.14534314027<br/>42.1361467446 2 3 3.87733825218<br/>42.8184896246 5 2 5.08687841047<br/>42.9062591302 7 5 4.51393378384<br/>42.9938135774 23 7 2.99362313969<br/>43.0054227435 3 23 2.85171945742<br/>43.2670764982 2 3 7.40729034169<br/>43.7670764982 3 2 6.38626621893<br/>43.9892833821 7 3 4.79569625166<br/>44.0493500624 5 7 3.2783540856<br/>44.1798427408 2 5 4.88483605978<br/>44.8855246038 17 2 3.06046273562<br/>44.9030308947 5 17 3.010991093<br/>45.0085255506 3 5 5.66594113804<br/>45.2134711286 2 3 5.66568652866<br/>45.7134711286 3 2 7.52151690135<br/>45.8547935832 5 3 5.31447410225<br/>45.9931176763 29 5 2.3292364275<br/>46.0072142888 19 29 2.67287251981<br/>46.0403166363 13 19 2.60199036589<br/>46.080725772 7 13 3.38532780178<br/>46.153467162 2 7 3.99017897787<br/>46.8315707556 7 2 4.31578719331<br/>46.8540752313 3 7 4.23289029617<br/>47.1317381739 5 3 4.78641402707<br/>47.187210694 2 5 4.76088392468<br/>47.8658815362 7 2 3.36235647082<br/>47.9465341029 5 7 3.53984685729<br/>48.0932178878 3 5 3.55700756129<br/>48.2907955125 2 3 7.22611030317<br/>48.7907955125 3 2 5.14534314027<br/>49.1062603893 2 3 2.59666417547<br/>49.8595750633 11 2 3.37969033591<br/>49.9115742042 29 11 2.32348709474<br/>49.9284870443 41 29 2.21821046078<br/>49.9331483721 3 41 2.1556303066<br/>50.2371901429 2 3 5.66568652866<br/>50.7371901429 3 2 6.21579215723<br/>50.962571946 5 3 3.36100996155<br/>51.0940690836 7 5 3.67419537999<br/>51.1469174718 2 7 3.4469519347<br/>51.8250210653 7 2 4.05160899669<br/>51.8437181433 5 7 3.76202588015<br/>51.9240516599 3 5 4.34045944429<br/>52.1835847732 2 3 4.22166719316<br/>52.859331846 7 2 3.19341503028<br/>52.9596939883 17 7 2.60976336014<br/>52.9940154279 11 17 2.15463299331<br/>53.0773031551 13 11 2.5010665002<br/>53.1050665326 5 13 2.97937424475<br/>53.2009962506 3 5 4.57653724213<br/>53.3145145268 2 3 7.07907472279<br/>53.8145145268 3 2 4.13610658412<br/>54.0472642832 5 3 4.7224116182<br/>54.1326231586 2 5 2.93996080482<br/>54.8479614377 5 2 3.32640028955<br/>54.991253124 7 5 2.95348009053<br/>55.0515012812 3 7 3.50099924853<br/>55.2609091571 2 3 5.66568652866<br/>55.7609091571 3 2 5.14534314027<br/>56.0386382219 7 3 2.44352502996<br/>56.1343440563 5 7 3.1294532976<br/>56.2093145538 2 5 4.46860927877<br/>56.8750062149 13 2 2.63723122229<br/>56.9019642028 3 13 2.49453001542<br/>57.2073037875 2 3 4.34847525633<br/>57.7073037875 3 2 6.08649914215<br/>57.87555082 7 3 3.52452473202<br/>57.8862151402 5 7 3.6393402048<br/>58.0259640141 19 5 2.38592487659<br/>58.0580443925 47 19 1.81798494881<br/>58.060825976 37 47 1.8454963235<br/>58.072835322 17 37 1.87330629506<br/>58.1134145121 2 17 2.34192768787<br/>58.786005949 5 2 4.36826515198<br/>58.8395777691 3 5 3.32655045748<br/>59.1536984179 2 3 3.11794708299<br/>59.8969319976 19 2 2.06490714632<br/>59.9328459781 11 19 1.93069951085<br/>59.9841456541 7 11 2.82751153883<br/>60.0315280967 5 7 3.34797123786<br/>60.1165223598 3 5 3.55700756129<br/>60.2846281714 2 3 5.66568652866<br/>60.7846281714 3 2 4.25183474932<br/>61.055807546 7 3 2.8376376882<br/>61.1301572678 11 7 2.69874485977<br/>61.1408715897 2 11 2.76485930344<br/>61.7933739022 5 2 4.01258746116<br/>61.9242701063 3 5 1.8051905553<br/>62.2310228018 2 3 4.45480966989<br/>62.7310228018 3 2 5.14534314027<br/>62.9840823408 17 3 2.07467688344<br/>62.9993086726 5 17 2.29585745812<br/>63.1547270184 2 5 2.93996080482<br/>63.8619525554 3 2 2.59398478971<br/>64.1774174321 2 3 3.31738058482<br/>64.8007418555 5 2 3.68992339513<br/>64.8913406821 11 5 2.60409944607<br/>64.9346469876 7 11 1.93928398106<br/>65.079387665 19 7 1.9642638865<br/>65.0970168102 3 19 2.05082563474<br/>65.3083471857 2 3 5.66568652866<br/>65.8083471857 3 2 3.49474476987<br/>66.0705687551 5 3 3.21575018753<br/>66.1620949717 2 5 2.93996080482<br/>66.8744120718 13 2 2.2535602064<br/>66.8869643997 5 13 2.36993081361<br/>67.032048469 3 5 2.74784245345<br/>67.2547418161 2 3 4.54525838674<br/>67.7547418161 3 2 4.34375237546<br/>68.0353237852 11 3 1.8535594981<br/>68.1096495055 2 11 1.9318761375<br/>68.8739935559 7 2 2.19542566191<br/>68.9377687191 3 7 2.89697069562<br/>69.2011364464 2 3 3.48785797619<br/>69.7011364464 3 2 5.14534314027<br/>69.9014025272 5 3 2.26261759288<br/>70.0769834674 11 5 2.1113151694<br/>70.117217396 13 11 2.14843778372<br/>70.1309600756 2 13 2.24083757719<br/>70.815477762 5 2 3.12681199896<br/>70.8628822411 3 5 3.36565211182<br/>71.1475310768 2 3 2.48831251718<br/>71.8998592884 19 2 1.67133642673<br/>71.9378360094 29 19 1.62292589514<br/>71.9462142174 23 29 1.67086724139<br/>71.9752254404 13 23 1.53179979101<br/>72.0382862152 5 13 1.9206622974<br/>72.1398268317 3 5 3.55700756129<br/>72.2784608304 2 3 4.62313381596<br/>72.7784608304 3 2 3.6528253076<br/>72.9860948643 5 3 3.31717788685<br/>73.0896582839 13 5 2.05812846567<br/>73.1172699249 2 13 1.92463299063<br/>73.8228457153 5 2 2.87966603847<br/>73.90562786 7 5 2.77229434741<br/>73.9549380432 3 7 2.209294669<br/>74.2248554607 2 3 3.63525871737<br/>74.7248554607 3 2 4.41852239265<br/>74.9420749838 7 3 2.21876468495<br/>75.0487187923 5 7 1.77481011211<br/>75.1841988314 2 5 2.93996080482<br/>75.8557852143 3 2 2.28140467291<br/>76.1712500911 2 3 2.69786972174<br/>76.8302136685 5 2 2.65186816531<br/>76.9911738478 11 5 1.55752991875<br/>77.0605305211 7 11 1.86520552031<br/>77.1351947691 3 7 2.51659181798<br/>77.3021798446 2 3 4.69088729842<br/>77.8021798446 3 2 3.05112513429<br/>78.0938732271 5 3 2.17301885453<br/>78.1915667847 2 5 2.93996080482<br/>78.8608941754 7 2 2.11672707014<br/>78.9459028327 5 7 2.00784639408<br/>79.055352941 3 5 2.87090608036<br/>79.248574475 2 3 3.7639714276<br/>79.748574475 3 2 3.78327301753<br/>79.9592443079 7 3 2.53380677345<br/>80.0639853429 43 7 1.23848578403<br/>80.0780680651 13 43 1.31233467614<br/>80.1303659325 2 13 1.95230755692<br/>80.8795042286 3 2 1.78778204801<br/>81.1785655643 5 3 3.12455848399<br/>81.1989347379 2 5 2.93996080482<br/>81.6949691054 3 2 4.4805338392<br/>81.9131069061 11 3 1.57866396088<br/>81.9614783541 5 11 1.94626533592<br/>82.0934378134 7 5 2.42335733041<br/>82.141930111 2 7 2.07343719513<br/>82.8258988589 3 2 2.52241596121<br/>83.1395010338 7 3 2.06726914428<br/>83.1762408916 2 7 2.54266203545<br/>83.8543444852 7 2 2.0844074191<br/>83.9468478948 11 7 1.68747152199<br/>84.0157608418 3 11 1.45834946769<br/>84.2722934893 2 3 3.87733825218<br/>84.7722934893 3 2 3.22331509044<br/>85.0093993363 5 3 2.34492746178<br/>85.129508718 11 5 2.03618861725<br/>85.1370618819 2 11 1.9318761375<br/>85.8523175283 5 2 2.06423042661<br/>85.9708790502 3 5 2.29518506215<br/>86.2186881196 2 3 3.04372229846<br/>86.7186881196 3 2 3.89275095998<br/>86.9596242224 17 3 1.49975492305<br/>86.9807410132 19 17 1.5845160144<br/>87.0079258 7 19 1.28777434641<br/>87.1337127861 5 7 1.88741717902<br/>87.2136706444 2 5 2.93996080482<br/>87.8496178732 3 2 2.05417572107<br/>88.1566703578 7 3 2.36163831674<br/>88.1696912014 2 7 2.30951746455<br/>88.8477947949 7 2 2.05572064544<br/>88.8855838701 5 7 2.24395767736<br/>89.0292001639 13 5 1.6288681522<br/>89.0698431073 3 13 1.46511344257<br/>89.2960125036 2 3 3.9779492312<br/>89.7903620396 5 2 2.80169883188<br/>89.8017128222 3 5 2.80201089943<br/>90.1061300925 13 3 1.55582757476<br/>90.1297717894 2 13 1.72779919602<br/>90.8826755629 17 2 1.54913269895<br/>90.8881785659 11 17 1.60843494506<br/>90.9277200889 13 11 1.68529730409<br/>90.9515453773 7 13 1.56627572501<br/>91.021462207 3 7 2.209294669<br/>91.2424071339 2 3 3.18808512241<br/>91.7424071339 3 2 3.36934084182<br/>91.9249254455 5 3 2.49097064774<br/>92.0331188508 7 5 1.71674603963<br/>92.1288307305 2 7 1.67805100055<br/>92.7977299929 5 2 2.6156244183<br/>92.8864051594 3 5 1.8051905553<br/>93.1888017643 2 3 2.43121612099<br/>93.6888017643 3 2 3.98593936332<br/>93.8455117458 7 3 2.08455194077<br/>93.9056684914 13 7 1.69994728441<br/>93.9513515007 31 13 1.16902509931<br/>93.9639149746 29 31 1.24847125848<br/>93.9790826438 41 29 1.18647411319<br/>93.9801524622 5 41 1.18317921118<br/>94.159083109 2 5 2.02741705357<br/>94.8197315179 3 2 2.28140467291<br/>95.1351963946 2 3 1.70531706164<br/>95.8050979461 5 2 2.44123193481<br/>95.9261500953 19 5 1.43513344328<br/>95.9332849928 7 19 1.42038285178<br/>96.038631531 3 7 1.7156719453<br/>96.2661261482 2 3 3.31738058482<br/>96.7661261482 3 2 2.90027753834<br/>97.0257684717 7 3 1.69951031124<br/>97.0733938235 5 7 2.11594444467<br/>97.1664510623 2 5 2.05566090526<br/>97.8817893413 5 2 1.4492255542<br/>97.9941835222 3 5 2.45000319065<br/>98.2125207786 2 3 2.59666417547<br/>98.7125207786 3 2 3.49474476987<br/>98.9473951632 17 3 1.31677419864<br/>98.9718200793 11 17 1.35371234146<br/>99.0281999737 13 11 1.46667862306<br/>99.0887013374 2 13 1.07420526719<br/>99.8434505321 3 2 1.88153915379<br/>100.117396145 5 3 2.402395749<br/>100.173819016 2 5 2.08220891157<br/>100.889157295 5 2 1.31838990495<br/>101.078875859 3 5 1.54471786673<br/>101.289845163 2 3 3.43385059425<br/>101.789845163 3 2 2.47751437871<br/>102.042937796 7 3 1.97190264957<br/>102.11573135 2 7 1.36000220696<br/>102.897549599 23 2 1.19478531642<br/>102.924666412 43 23 1.06677069704<br/>102.929503781 3 43 1.02540802112<br/>103.236239793 2 3 2.74601004454<br/>103.736239793 3 2 3.05112513429<br/>103.948229917 5 3 1.79142354141<br/>104.111863527 2 5 1.28934616776<br/>104.827201806 5 2 1.97809184379<br/>104.909709631 3 5 2.00595966729<br/>105.182634423 2 3 2.08362634252<br/>105.896767286 7 2 1.16981151102<br/>106.041549147 13 7 1.22434210603<br/>106.075013988 5 13 1.4546011971<br/>106.186654222 3 5 2.1523114006<br/>106.313564177 2 3 3.53931330456<br/>106.813564177 3 2 2.09451851531<br/>107.129029054 2 3 1.44531193512<br/>107.834569759 5 2 1.84093365824<br/>107.960498146 11 5 1.54854339115<br/>107.999616452 3 11 1.229210933<br/>108.259958807 2 3 2.88149535629<br/>108.759958807 3 2 2.64848786801<br/>109.028411801 13 3 1.34756324305<br/>109.068691791 5 13 1.18298950435<br/>109.195922875 2 5 2.15307901792<br/>109.848081197 73 2 1.65958805777<br/>109.924898969 3 7 1.56239595013<br/>110.206353438 2 3 2.24691333375<br/>110.706353438 3 2 3.1829778875<br/>110.863756027 5 3 1.94704500339<br/>110.947493587 7 5 1.80689143056<br/>111.044042537 29 7 1.00068230928<br/>111.065891858 23 29 0.987496726004<br/>111.097779459 2 23 1.05614487388<br/>111.837283191 3 2 1.74593091981<br/>112.140700617 5 3 1.76345092276<br/>112.203290829 2 5 2.17416969228<br/>112.867641132 11 2 1.40722921132<br/>112.904123503 17 11 1.27779482094<br/>112.931094929 31 17 1.03555743978<br/>112.936373351 5 31 1.05314310102<br/>113.092806543 7 5 1.85824264566<br/>113.105155695 3 7 1.79015505874<br/>113.283677821 2 3 3.0049641066<br/>113.783677821 3 2 2.28140467291<br/>114.092292636 7 3 1.11481073946<br/>114.171253531 2 7 1.79996479543<br/>114.849357124 7 2 1.57398748462<br/>114.942068293 3 7 1.17819153198<br/>115.230072452 2 3 2.395962584<br/>115.730072452 3 2 2.79886680265<br/>115.929205234 7 3 1.67553385562<br/>115.987768559 5 7 1.4047112666<br/>116.14133534 2 5 1.46031046908<br/>116.856673619 5 2 1.47181715806<br/>116.933014103 3 5 2.16544175388<br/>117.176467082 2 3 1.807193064<br/>117.787350177 5 2 2.16644475994<br/>117.914606121 13 5 0.92376129355<br/>117.999326209 7 13 0.963537122143<br/>118.122325019 3 7 1.40662081595<br/>118.307396836 2 3 3.11794708299<br/>118.807396836 3 2 1.94536538377<br/>119.056226727 5 3 1.90998861884<br/>119.13530354 7 5 1.63279606943<br/>119.164703841 2 7 1.658583476<br/>119.842807434 7 2 1.57398748462<br/>119.8849526 5 7 1.57019995018<br/>120.017706441 3 5 1.41032452094</p><p>Keenan</p></div><h3><a id=19985 href="#19985">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>11/6/2011 10:52:56 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@...&gt; wrote:<br/>&gt;<br/>&gt; The following infinite vals are generalized patent vals for their *own* TOP tunings (the tunings with the least TOP-max error).</p><p>I&apos;m not clear what you are computing.</p></div><h3><a id=19986 href="#19986">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/6/2011 11:11:58 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;genewardsmith&quot; &lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; The following infinite vals are generalized patent vals for their *own* TOP tunings (the tunings with the least TOP-max error).<br/>&gt;<br/>&gt; I&apos;m not clear what you are computing.</p><p>I assume you know what a generalized patent val is. (It&apos;s simply the function that rounds each prime to the nearest interval of a rank-1 temperament.)</p><p>I also assume that you know what a TOP tuning is. (It&apos;s the tuning that minimizes the maximum Tenney-weighted error of any interval.) The TOP error is guaranteed to exist for these GPVs because it&apos;s the maximum of a sequence that&apos;s bounded by a decreasing function (no convergence problems). Also, although for general temperaments there can be more than one TOP tuning, for these rank-1 temperaments I claim this can never happen; there is always one unique TOP tuning.</p><p>So we have a function from real numbers to integer sequences (GPV), and we have another function from integer sequences to real numbers (TOP tuning). We can compose them to get a function from reals to reals.</p><p>The fixed points of this R-&gt;R function are rank-1 temperaments that are the TOP tunings of their own GPVs. That&apos;s what these are.</p><p>Keenan</p></div><h3><a id=19992 href="#19992">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/6/2011 11:01:57 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Keenan wrote:<br/>&gt;First is the tuning, in steps per octave<br/>&gt;Next comes the beginning of the infinite val, which is the GPV of that tuning<br/>&gt;Next, the flattest and sharpest primes, which determing the TOP tuning<br/>&gt;Next, the TOP error in cents per octave<br/>&gt;<br/>&gt;0.815464876786<br/>&gt;Val: [1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, ...] (all of the following<br/>&gt;should have &quot;...&quot; because they&apos;re infinite vals)<br/>&gt;Worst primes: 3 2<br/>&gt;Error: 271.553262637</p><p>Ah, now I see what you&apos;re driving at.  Can you clarify how you<br/>know which are the flattest and sharpest primes?</p><p>-Carl</p></div><h3><a id=19996 href="#19996">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 7:37:39 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;carl@...&gt; wrote:<br/>&gt; Ah, now I see what you&apos;re driving at.  Can you clarify how you<br/>&gt; know which are the flattest and sharpest primes?</p><p>Since these are generalized patent vals, the worst unweighted error for any prime is 1/2 of a step of the temperament. Since the weights are decreasing, this means the worst weighted error is bounded by a decreasing function (it&apos;s no more than 1/2/log2(p)). So, given any error, I can find some limit such that if I check all errors in primes up to that limit, I&apos;m guaranteed not to find any larger weighted errors for any primes above that limit.</p><p>Specifically, if I already have some prime whose weighted error is x, verifying that that&apos;s the worst weighted error of all primes only requires explicitly checking other primes up to the point where</p><p>1/2/log2(p) = x</p><p>p = 2^(1/(2x))</p><p>Now, this does grow exponentially fast as x shrinks, so I have to be a little careful in my algorithm. But if I have a tuning where some pair of primes have the same weighted error in opposite directions, and no other primes up to this cutoff have larger weighted error, then that should convince you that I&apos;ve found the TOP tuning.</p><p>Keenan</p></div><h3><a id=19997 href="#19997">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>11/7/2011 10:04:43 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@...&gt; wrote:</p><p>&gt; The fixed points of this R-&gt;R function are rank-1 temperaments that are the TOP tunings of their own GPVs. That&apos;s what these are.</p><p>How are you computing them in practice?</p></div><h3><a id=19998 href="#19998">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>11/7/2011 10:18:27 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;genewardsmith&quot; &lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt;<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@&gt; wrote:<br/>&gt;<br/>&gt; &gt; The fixed points of this R-&gt;R function are rank-1 temperaments that are the TOP tunings of their own GPVs. That&apos;s what these are.<br/>&gt;<br/>&gt; How are you computing them in practice?</p><p>I see this has already been answered. What should these tunings be called?</p></div><h3><a id=19999 href="#19999">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2011 11:46:16 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Well that makes sense.  I&apos;ll have to run through some calculations<br/>when I get a chance.  It appears you&apos;ve rather directly solved a<br/>problem that eluded me for years!</p><p>-Carl</p><p>&gt;Since these are generalized patent vals, the worst unweighted error<br/>&gt;for any prime is 1/2 of a step of the temperament. Since the weights<br/>&gt;are decreasing, this means the worst weighted error is bounded by a<br/>&gt;decreasing function (it&apos;s no more than 1/2/log2(p)). So, given any<br/>&gt;error, I can find some limit such that if I check all errors in primes<br/>&gt;up to that limit, I&apos;m guaranteed not to find any larger weighted<br/>&gt;errors for any primes above that limit.<br/>&gt;<br/>&gt;Specifically, if I already have some prime whose weighted error is x,<br/>&gt;verifying that that&apos;s the worst weighted error of all primes only<br/>&gt;requires explicitly checking other primes up to the point where<br/>&gt;<br/>&gt;1/2/log2(p) = x<br/>&gt;<br/>&gt;p = 2^(1/(2x))<br/>&gt;<br/>&gt;Now, this does grow exponentially fast as x shrinks, so I have to be a<br/>&gt;little careful in my algorithm. But if I have a tuning where some pair<br/>&gt;of primes have the same weighted error in opposite directions, and no<br/>&gt;other primes up to this cutoff have larger weighted error, then that<br/>&gt;should convince you that I&apos;ve found the TOP tuning.<br/>&gt;<br/>&gt;Keenan</p></div><h3><a id=20000 href="#20000">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>11/7/2011 11:53:56 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;carl@...&gt; wrote:<br/>&gt;<br/>&gt; Well that makes sense.  I&apos;ll have to run through some calculations<br/>&gt; when I get a chance.  It appears you&apos;ve rather directly solved a<br/>&gt; problem that eluded me for years!</p><p>Now that we have these Pepper tunings, or whatever we should call them, what do we do with them? One thing to play with is the sequence of superparticular ratios mapped to unison by the Pepper map. Is there some use for that?</p></div><h3><a id=20001 href="#20001">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2011 12:21:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene wrote:</p><p>&gt;&gt; Well that makes sense.  I&apos;ll have to run through some calculations<br/>&gt;&gt; when I get a chance.  It appears you&apos;ve rather directly solved a<br/>&gt;&gt; problem that eluded me for years!<br/>&gt;<br/>&gt;Now that we have these Pepper tunings, or whatever we should call<br/>&gt;them, what do we do with them? One thing to play with is the sequence<br/>&gt;of superparticular ratios mapped to unison by the Pepper map. Is there<br/>&gt;some use for that?</p><p>I&apos;m wondering for which divisions, if any, the best val is<br/>not a self-consistent generalized patent val (and therefore<br/>doesn&apos;t appear on Keenan&apos;s list).</p><p>One trend that&apos;s evident is that there are fewer entries on<br/>Keenan&apos;s list for &apos;better&apos; divisions.</p><p>It would be interesting to compare zeta tunings with Keenan&apos;s<br/>results.</p><p>-Carl</p></div><h3><a id=20003 href="#20003">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 1:08:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;genewardsmith&quot; &lt;genewardsmith@...&gt; wrote:<br/>&gt; I see this has already been answered. What should these tunings be called?</p><p>I&apos;ve suggested &quot;TOP self-consistent&quot; as a name for these tunings and their associated vals. If anybody has a better name that more clearly indicates what they are, that&apos;s great. Maybe &quot;TOP fixed-point&quot; would be better.</p><p>Keenan</p></div><h3><a id=20004 href="#20004">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2011 1:19:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@...&gt; wrote:</p><p>&gt; I&apos;ve suggested &quot;TOP self-consistent&quot; as a name for these tunings<br/>&gt; and their associated vals. If anybody has a better name that<br/>&gt; more clearly indicates what they are, that&apos;s great. Maybe<br/>&gt; &quot;TOP fixed-point&quot; would be better.</p><p>Fixed point actually tells you what it is, so I think<br/>that&apos;s better.  My vote for TOP-FP.</p><p>-Carl</p></div><h3><a id=20005 href="#20005">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2011 3:55:16 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Nov 7, 2011 at 2:53 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;carl@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Well that makes sense. I&apos;ll have to run through some calculations<br/>&gt; &gt; when I get a chance. It appears you&apos;ve rather directly solved a<br/>&gt; &gt; problem that eluded me for years!<br/>&gt;<br/>&gt; Now that we have these Pepper tunings, or whatever we should call them, what do we do with them? One thing to play with is the sequence of superparticular ratios mapped to unison by the Pepper map. Is there some use for that?</p><p>I think there may very well be. Keenan might have some other ideas<br/>what to do with them, but one idea he expressed in XA chat was to<br/>wedge them together to start getting optimal w-limit linear<br/>temperaments (where w is pronounced &quot;omega&quot;).</p><p>I wonder though, what would the difference be in these two algorithms?</p><p>1) Finding 2 TOP-FP vals<br/>2) Getting the wedgie</p><p>and</p><p>1) Finding 2 TOP-FP vals<br/>2) Coming up with the fractional val representing the optimum tuning<br/>3) Wedging the two fractional vals<br/>4) Re-patenting by rounding the wedgie off directly</p><p>I&apos;m intrigued by the latter, but I&apos;m not sure what the implications might be.</p><p>-Mike</p></div><h3><a id=20006 href="#20006">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2011 4:36:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Carl wrote:<br/>&gt;<br/>&gt; I&apos;m wondering for which divisions, if any, the best val is<br/>&gt; not a self-consistent generalized patent val (and therefore<br/>&gt; doesn&apos;t appear on Keenan&apos;s list).</p><p>Likewise, and a few related questions:</p><p>1) Are there any generalized vals that are lower in TOP-max error than<br/>the ones listed, but which aren&apos;t patent with respect to anything and<br/>hence unobtainable by this method?<br/>2) One stated objective for this approach was to obtain a set of<br/>&quot;fixed point&quot; infinite vals that we know have desirable properties to<br/>hence wedge together and obtain really good linear temperaments. If L<br/>is the set of all linear temperaments you can obtain by using this<br/>method, which hence consists of pairs of FP vals, does there exist an<br/>infinite linear temperament under some threshold of badness deemed<br/>&quot;tolerable&quot; that&apos;s not going to be in L? (Let&apos;s assume we have some<br/>way to measure infinite-limit complexity for this.)</p><p>-Mike</p></div><h3><a id=20007 href="#20007">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 4:38:47 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; &gt; Now that we have these Pepper tunings, or whatever we should call them, what do we do with them? One thing to play with is the sequence of superparticular ratios mapped to unison by the Pepper map. Is there some use for that?<br/>&gt;<br/>&gt; I think there may very well be. Keenan might have some other ideas<br/>&gt; what to do with them, but one idea he expressed in XA chat was to<br/>&gt; wedge them together to start getting optimal w-limit linear<br/>&gt; temperaments (where w is pronounced &quot;omega&quot;).</p><p>Yes, this is something I had in mind. If you want to construct finite-rank omega-limit temperaments, you have to start with rank-1 omega-limit temperaments. There are way too many of those to deal with, but TOP fixed-point temperaments are an optimal, discrete subset.</p><p>&gt; I wonder though, what would the difference be in these two algorithms?<br/>&gt;<br/>&gt; 1) Finding 2 TOP-FP vals<br/>&gt; 2) Getting the wedgie<br/>&gt;<br/>&gt; and<br/>&gt;<br/>&gt; 1) Finding 2 TOP-FP vals<br/>&gt; 2) Coming up with the fractional val representing the optimum tuning<br/>&gt; 3) Wedging the two fractional vals<br/>&gt; 4) Re-patenting by rounding the wedgie off directly<br/>&gt;<br/>&gt; I&apos;m intrigued by the latter, but I&apos;m not sure what the implications might be.</p><p>One of the basic properties of wedge product is that if you wedge together any set of things that is linearly dependent, you get zero.</p><p>Fractional vals representing JI tuning are all exactly proportional to each other.</p><p>Therefore, if you wedge together any two of these fractional vals, you get 0. So your second algorithm doesn&apos;t actually work at all.</p><p>Keenan</p></div><h3><a id=20008 href="#20008">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2011 4:49:16 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Nov 7, 2011 at 7:38 PM, Keenan Pepper &lt;<a href="mailto:keenanpepper@gmail.com">keenanpepper@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt; 1) Finding 2 TOP-FP vals<br/>&gt; &gt; 2) Coming up with the fractional val representing the optimum tuning<br/>&gt; &gt; 3) Wedging the two fractional vals<br/>&gt; &gt; 4) Re-patenting by rounding the wedgie off directly<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m intrigued by the latter, but I&apos;m not sure what the implications might be.<br/>&gt;<br/>&gt; One of the basic properties of wedge product is that if you wedge together any set of things that is linearly dependent, you get zero.<br/>&gt;<br/>&gt; Fractional vals representing JI tuning are all exactly proportional to each other.<br/>&gt;<br/>&gt; Therefore, if you wedge together any two of these fractional vals, you get 0. So your second algorithm doesn&apos;t actually work at all.</p><p>How do these fractional vals represent JI? I thought that the<br/>coefficients of these vals were linearly scaled versions of some<br/>integer val so as to sync up with the TOP tuning for that val. For<br/>example, the TOP tuning for 12p is &lt;11.9844   18.9753   27.9636|, but<br/>11.9844*log2(&lt;2 3 5]) is &lt;11.9844   18.9753   27.9636].</p><p>-Mike</p></div><h3><a id=20009 href="#20009">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2011 4:50:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Mon, Nov 7, 2011 at 7:49 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;<br/>&gt; How do these fractional vals represent JI? I thought that the<br/>&gt; coefficients of these vals were linearly scaled versions of some<br/>&gt; integer val so as to sync up with the TOP tuning for that val. For<br/>&gt; example, the TOP tuning for 12p is &lt;11.9844 &nbsp; 18.9753 &nbsp; 27.9636|, but<br/>&gt; 11.9844*log2(&lt;2 3 5]) is &lt;11.9844 &nbsp; 18.9753 &nbsp; 27.9636].</p><p>Agh, clipboard fail. The second should have been &lt;11.9844   18.9948   27.8269].</p><p>-Mike</p></div><h3><a id=20010 href="#20010">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 4:55:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; Carl wrote:<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m wondering for which divisions, if any, the best val is<br/>&gt; &gt; not a self-consistent generalized patent val (and therefore<br/>&gt; &gt; doesn&apos;t appear on Keenan&apos;s list).<br/>&gt;<br/>&gt; Likewise, and a few related questions:<br/>&gt;<br/>&gt; 1) Are there any generalized vals that are lower in TOP-max error than<br/>&gt; the ones listed, but which aren&apos;t patent with respect to anything and<br/>&gt; hence unobtainable by this method?</p><p>I would be very surprised if there were. I&apos;m thinking up a proof.</p><p>Basically the only reason that some best vals aren&apos;t patent (in the restricted sence of &quot;patent&quot;) is because you&apos;re rounding all the primes based on octaves being pure. I think that every best val is a generalized patent val.</p><p>Note that for each TOP-FP val, there are an infinite number of non-patent vals that have exactly the same TOP error as the patent val (make a small enough change to whatever really large primes you want to). But none of them are strictly better.</p><p>&gt; 2) One stated objective for this approach was to obtain a set of<br/>&gt; &quot;fixed point&quot; infinite vals that we know have desirable properties to<br/>&gt; hence wedge together and obtain really good linear temperaments. If L<br/>&gt; is the set of all linear temperaments you can obtain by using this<br/>&gt; method, which hence consists of pairs of FP vals, does there exist an<br/>&gt; infinite linear temperament under some threshold of badness deemed<br/>&gt; &quot;tolerable&quot; that&apos;s not going to be in L? (Let&apos;s assume we have some<br/>&gt; way to measure infinite-limit complexity for this.)</p><p>Now this is a much more significant question. I honestly have no idea.</p><p>Keenan</p></div><h3><a id=20011 href="#20011">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 5:02:39 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; How do these fractional vals represent JI? I thought that the<br/>&gt; coefficients of these vals were linearly scaled versions of some<br/>&gt; integer val so as to sync up with the TOP tuning for that val. For<br/>&gt; example, the TOP tuning for 12p is &lt;11.9844   18.9753   27.9636|, but<br/>&gt; 11.9844*log2(&lt;2 3 5]) is &lt;11.9844   18.9753   27.9636].</p><p>Oh, you were talking about a different thing than I thought you were talking about. Hmmm.</p><p>Keenan</p></div><h3><a id=20012 href="#20012">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>11/7/2011 5:46:39 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Keenan Pepper&quot; &lt;keenanpepper@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@&gt; wrote:<br/>&gt; &gt; How do these fractional vals represent JI? I thought that the<br/>&gt; &gt; coefficients of these vals were linearly scaled versions of some<br/>&gt; &gt; integer val so as to sync up with the TOP tuning for that val. For<br/>&gt; &gt; example, the TOP tuning for 12p is &lt;11.9844   18.9753   27.9636|, but<br/>&gt; &gt; 11.9844*log2(&lt;2 3 5]) is &lt;11.9844   18.9753   27.9636].<br/>&gt;<br/>&gt; Oh, you were talking about a different thing than I thought you were talking about. Hmmm.</p><p>The thing that you were actually talking about does not seem useful because:</p><p>1. It&apos;s based on singling 2 out from all other primes. Why should the fractional val for 12p be &lt;11.984   18.975   27.964| (where everything is in units of 2^(1/12)) rather than &lt;12.043   19.069   28.101| (where everything is in units of 5^(1/28))? 28ed5 is an equally accurate characterization of this rank-1 temperament as 12ed2.</p><p>2. If you naively apply the algorithm you said, you get crazy wedgies that don&apos;t correspond to any reasonable temperament. For example, I applied it to &lt;2 3 4 4| and &lt;7 11 16 19| (yields meantone), and I got &lt;&lt;1 5 11 5 15 14||, which is a totally unreasonable temperament. It tempers out 243/160.</p><p>I think that the &quot;rounding wedgie entries&quot; operation just doesn&apos;t make sense.</p><p>Keenan</p></div><h3><a id=20013 href="#20013">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>11/7/2011 8:33:34 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;1) Are there any generalized vals that are lower in TOP-max error than<br/>&gt;the ones listed, but which aren&apos;t patent with respect to anything and<br/>&gt;hence unobtainable by this method?</p><p>I believe there must be (see my earlier message).</p><p>&gt;2) One stated objective for this approach was to obtain a set of<br/>&gt;&quot;fixed point&quot; infinite vals that we know have desirable properties to<br/>&gt;hence wedge together and obtain really good linear temperaments.</p><p>I don&apos;t know how to wedge infinite vals.  And even if I did,<br/>finding the optimal tuning of an infinite rank 2 temperament<br/>seems like a much harder problem...</p><p>-Carl</p></div><h3><a id=20014 href="#20014">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>11/7/2011 9:23:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;carl@...&gt; wrote:</p><p>&gt; I don&apos;t know how to wedge infinite vals.  And even if I did,<br/>&gt; finding the optimal tuning of an infinite rank 2 temperament<br/>&gt; seems like a much harder problem...</p><p>Wedging infinite vals is not a problem, but then what?</p></div><h3><a id=20015 href="#20015">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>11/7/2011 10:08:31 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Tue, Nov 8, 2011 at 12:23 AM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt;wrote:</p><p>&gt; **<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;carl@...&gt; wrote:<br/>&gt;<br/>&gt; &gt; I don&apos;t know how to wedge infinite vals. And even if I did,<br/>&gt; &gt; finding the optimal tuning of an infinite rank 2 temperament<br/>&gt; &gt; seems like a much harder problem...<br/>&gt;<br/>&gt; Wedging infinite vals is not a problem, but then what?<br/>&gt;</p><p>Then we work out a complexity measure for the resulting infinite<br/>temperament that actually converges. And then all of these subgroup<br/>temperaments, and even lower vs higher-limit extensions of the same<br/>temperament, become unnecessary; they serve no purpose except as a ways to<br/>optimize for a select number of primes at the exclusion of others. There&apos;d<br/>be no point describing them as independent mathematical objects. It&apos;d be<br/>like saying that quarter-comma meantone is a contorted 2.5-limit<br/>temperament or something.</p><p>-Mike</p></div><h3><a id=20016 href="#20016">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/9/2011 1:03:02 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 8 November 2011 00:55, Keenan Pepper &lt;<a href="mailto:keenanpepper@gmail.com">keenanpepper@gmail.com</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt;&gt; 1) Are there any generalized vals that are lower in TOP-max error than<br/>&gt;&gt; the ones listed, but which aren&apos;t patent with respect to anything and<br/>&gt;&gt; hence unobtainable by this method?<br/>&gt;<br/>&gt; I would be very surprised if there were. I&apos;m thinking up a proof.</p><p>The TOP tuning is the one for which the highest and lowest weighted<br/>prime deviations are equal and opposite.  The optimal tuning is the<br/>middle of that range.  If you&apos;re taking the nearest approximation to<br/>the optimal tuning, as in the optimal patent val, and it&apos;s possible to<br/>lie within the TOP range, it will lie within that range.  There may<br/>not be a unique TOP val but it&apos;s difficult to see how the generalized<br/>patent val for the TOP tuning wouldn&apos;t be TOP.  Still, that seems to<br/>be what you found, so I&apos;m not sure.</p><p>A similar argument should work for TE error and the tuning with zero<br/>mean deviation (which has come out before, and is easy to find, but I<br/>forget how).  The ZMD tuning is very close to the TE tuning so TE vals<br/>should also be self-consistent almost all the time.</p><p>                      Graham</p></div>