<a href="/tuning-math">back to list</a><h1>More prime errors and complexities</h1><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/1/2007 11:35:46 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve been working on my prime errors and complexities paper over the<br/>past week.  Because I&apos;ve had a really bad Internet connection I<br/>haven&apos;t been able to update you on it.  I&apos;ve discovered that Yahoo!<br/>Groups is available, so I put a copy of the PDF in the files section<br/>of this group.  It only needs to stay there until I can get at my new<br/>website -- at the moment I can connect by FTP but not transfer anything.</p><p>The new results are for octave-equivalent weighted standard-deviations<br/>of a rank 2 temperament given two equal temperaments.  I had a nice<br/>equation for the error*complexity badness given the canonical mapping<br/>(by period and generator) and I&apos;ve now proved that it works for an<br/>arbitrary pair of mappings.  Along the way I found a simple formula<br/>for the optimal error.</p><p>Because of this simplicity I now prefer the standard deviation error<br/>to the TOP-RMS error it was designed to approximate.  And<br/>error*complexity badness is clearly the easiest kind to work with as<br/>well.  I probably need some cute names for them.  Maybe TOPPO for TOP<br/>Pure Octaves.</p><p>I&apos;ve also implemented rank 2 temperament searches using the new<br/>formulae.  They&apos;re very efficient because you don&apos;t need to calculate<br/>the canonical mapping to get the error and complexity.  You still need<br/>the generator mapping for the invariant but it&apos;s much easier to<br/>calculate than the period mapping (I wonder why I didn&apos;t notice that<br/>before...)  I could previously calculate the error without the<br/>canonical mapping but I had to fudge the calculation because it lost<br/>precision.  The new formula involves standard deviations of weighted<br/>errors, so it holds the precision much better.  Also, the same<br/>standard deviations can be shared between the error and complexity<br/>calculations.  And some of them only depend on the equal temperaments,<br/>so they only need to be calculated once for each rank 2 temperament<br/>that involves a given equal temperament.  With these improvements, and<br/>some code profiling, I&apos;ve reduced the runtime of the big pure-Python<br/>searches by two-thirds.  The small searches can&apos;t be improved much<br/>because so little time&apos;s actually spent on the errors and<br/>complexities.  Anyway, I haven&apos;t uploaded the code yet, but this is to<br/>show that it is practical.</p><p>There&apos;s a formula for the rank 2 badness that looks similar to the<br/>geometric definition of a vector product.  Perhaps a wedge product<br/>would give similar results.  I remember Gene giving a formula for<br/>wedgie error before, and it not having much correlation with the other<br/>errors.  Perhaps this approach will do better.</p><p>Another thing is I&apos;ve got confused about the units for weighted<br/>complexity.</p><p>                     Graham</p></div><h3>Mohajeri Shahin &#x3C;shahinm@kayson-ir.com&#x3E;</h3><span>1/2/2007 3:38:52 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hi graham</p><p>do you permit me to put it in my site until your problem benig solved?</p><p>Shaahin Mohajeri</p><p>Tombak Player &amp; Researcher , Microtonal Composer</p><p>My web site?? ???? ????? ??????  &lt;<a href="http://240edo.googlepages.com/">http://240edo.googlepages.com/</a>&gt;</p><p>My farsi page in Harmonytalk   ???? ??????? ?? ??????? ???  &lt;<a href="http://www.harmonytalk.com/mohajeri">http://www.harmonytalk.com/mohajeri</a>&gt;</p><p>Shaahin Mohajeri in Wikipedia  ????? ?????? ??????? ??????? ???? ???? &lt;<a href="http://en.wikipedia.org/wiki/Shaahin_mohajeri">http://en.wikipedia.org/wiki/Shaahin_mohajeri</a>&gt;</p><p>________________________________</p><p>From: <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a> [<a href="mailto:tuning-math@yahoogroups.com">mailto:tuning-math@yahoogroups.com</a>] On Behalf Of Graham Breed<br/>Sent: Tuesday, January 02, 2007 11:06 AM<br/>To: <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a><br/>Subject: [tuning-math] More prime errors and complexities</p><p>I&apos;ve been working on my prime errors and complexities paper over the<br/>past week. Because I&apos;ve had a really bad Internet connection I<br/>haven&apos;t been able to update you on it. I&apos;ve discovered that Yahoo!<br/>Groups is available, so I put a copy of the PDF in the files section<br/>of this group. It only needs to stay there until I can get at my new<br/>website -- at the moment I can connect by FTP but not transfer anything.</p><p>The new results are for octave-equivalent weighted standard-deviations<br/>of a rank 2 temperament given two equal temperaments. I had a nice<br/>equation for the error*complexity badness given the canonical mapping<br/>(by period and generator) and I&apos;ve now proved that it works for an<br/>arbitrary pair of mappings. Along the way I found a simple formula<br/>for the optimal error.</p><p>Because of this simplicity I now prefer the standard deviation error<br/>to the TOP-RMS error it was designed to approximate. And<br/>error*complexity badness is clearly the easiest kind to work with as<br/>well. I probably need some cute names for them. Maybe TOPPO for TOP<br/>Pure Octaves.</p><p>I&apos;ve also implemented rank 2 temperament searches using the new<br/>formulae. They&apos;re very efficient because you don&apos;t need to calculate<br/>the canonical mapping to get the error and complexity. You still need<br/>the generator mapping for the invariant but it&apos;s much easier to<br/>calculate than the period mapping (I wonder why I didn&apos;t notice that<br/>before...) I could previously calculate the error without the<br/>canonical mapping but I had to fudge the calculation because it lost<br/>precision. The new formula involves standard deviations of weighted<br/>errors, so it holds the precision much better. Also, the same<br/>standard deviations can be shared between the error and complexity<br/>calculations. And some of them only depend on the equal temperaments,<br/>so they only need to be calculated once for each rank 2 temperament<br/>that involves a given equal temperament. With these improvements, and<br/>some code profiling, I&apos;ve reduced the runtime of the big pure-Python<br/>searches by two-thirds. The small searches can&apos;t be improved much<br/>because so little time&apos;s actually spent on the errors and<br/>complexities. Anyway, I haven&apos;t uploaded the code yet, but this is to<br/>show that it is practical.</p><p>There&apos;s a formula for the rank 2 badness that looks similar to the<br/>geometric definition of a vector product. Perhaps a wedge product<br/>would give similar results. I remember Gene giving a formula for<br/>wedgie error before, and it not having much correlation with the other<br/>errors. Perhaps this approach will do better.</p><p>Another thing is I&apos;ve got confused about the units for weighted<br/>complexity.</p><p>Graham</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/2/2007 4:41:01 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 02/01/07, Mohajeri Shahin &lt;<a href="mailto:shahinm@kayson-ir.com">shahinm@kayson-ir.com</a>&gt; wrote:<br/>&gt;<br/>&gt;<br/>&gt; Hi graham<br/>&gt;<br/>&gt; do you permit me to put it in my site until your problem  benig solved?</p><p>Certainly you may!</p><p>                     Graham</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/2/2007 11:30:47 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve updated it now with a new equation that means the octave-specific<br/>error might be almost as simple and stable as the octave-equivalent<br/>one.  That&apos;s Equation 32 on page 10.  It&apos;s got a beautiful symmetry to<br/>it, so it&apos;s probably the one to put on a T-shirt.</p><p>I&apos;ve also got a feeling I uploaded an old file before.  (I can&apos;t<br/>*download* from Yahoo Groups to check.)  This one should have today&apos;s<br/>date at the top (January 3, 2007).</p><p>                       Graham</p></div><h3>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>1/2/2007 11:44:36 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 11:30 PM 1/2/2007, you wrote:<br/>&gt;I&apos;ve updated it now with a new equation that means the octave-specific<br/>&gt;error might be almost as simple and stable as the octave-equivalent<br/>&gt;one.  That&apos;s Equation 32 on page 10.  It&apos;s got a beautiful symmetry to<br/>&gt;it, so it&apos;s probably the one to put on a T-shirt.<br/>&gt;<br/>&gt;I&apos;ve also got a feeling I uploaded an old file before.</p><p>You did.</p><p>-Carl</p></div><h3>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>1/2/2007 11:48:17 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 11:30 PM 1/2/2007, you wrote:<br/>&gt;I&apos;ve updated it now with a new equation that means the octave-specific<br/>&gt;error might be almost as simple and stable as the octave-equivalent<br/>&gt;one.  That&apos;s Equation 32 on page 10.  It&apos;s got a beautiful symmetry to<br/>&gt;it, so it&apos;s probably the one to put on a T-shirt.</p><p>Good work, dude!</p><p>-Carl</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/3/2007 12:26:55 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 03/01/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; At 11:30 PM 1/2/2007, you wrote:<br/>&gt; &gt;I&apos;ve updated it now with a new equation that means the octave-specific<br/>&gt; &gt;error might be almost as simple and stable as the octave-equivalent<br/>&gt; &gt;one.  That&apos;s Equation 32 on page 10.  It&apos;s got a beautiful symmetry to<br/>&gt; &gt;it, so it&apos;s probably the one to put on a T-shirt.<br/>&gt;<br/>&gt; Good work, dude!</p><p>Thanks!</p><p>One thing to note is that the numerator&apos;s an approximation to badness<br/>squared.  As the whole thing&apos;s error squared that means the<br/>denominator must be an approximation to complexity squared.  And it<br/>works!</p><p>It doesn&apos;t appear to calculate a generator mapping or wedge product by<br/>the back door.  But it is the determinant of a 2x2 matrix containing<br/>the means-of-errors-of-products so there is a similarity.</p><p>                       Graham</p></div><h3>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>1/3/2007 2:27:27 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Graham Breed&quot; &lt;gbreed@...&gt; wrote:<br/>&gt;<br/>&gt; I&apos;ve been working on my prime errors and complexities paper over the<br/>&gt; past week.</p><p>Wow--looks great! I look forward to reading it.</p><p>&gt; I&apos;ve also implemented rank 2 temperament searches using the new<br/>&gt; formulae.  They&apos;re very efficient because you don&apos;t need to<br/>calculate<br/>&gt; the canonical mapping to get the error and complexity.</p><p>Right.</p><p>  You still need<br/>&gt; the generator mapping for the invariant but it&apos;s much easier to<br/>&gt; calculate than the period mapping (I wonder why I didn&apos;t notice that<br/>&gt; before...)</p><p>&gt; There&apos;s a formula for the rank 2 badness that looks similar to the<br/>&gt; geometric definition of a vector product.  Perhaps a wedge product<br/>&gt; would give similar results.  I remember Gene giving a formula for<br/>&gt; wedgie error before, and it not having much correlation with the<br/>other<br/>&gt; errors.  Perhaps this approach will do better.</p><p>It seems worth comparing; some sort of wedgie computation, without<br/>ever calculating generators, would seem to be the way to go anyway.</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/4/2007 3:33:13 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 04/01/07, Gene Ward Smith &lt;<a href="mailto:genewardsmith@coolgoose.com">genewardsmith@coolgoose.com</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Graham Breed&quot; &lt;gbreed@...&gt; wrote:<br/>&gt; &gt; There&apos;s a formula for the rank 2 badness that looks similar to the<br/>&gt; &gt; geometric definition of a vector product.  Perhaps a wedge product<br/>&gt; &gt; would give similar results.  I remember Gene giving a formula for<br/>&gt; &gt; wedgie error before, and it not having much correlation with the<br/>&gt; other<br/>&gt; &gt; errors.  Perhaps this approach will do better.<br/>&gt;<br/>&gt; It seems worth comparing; some sort of wedgie computation, without<br/>&gt; ever calculating generators, would seem to be the way to go anyway.</p><p>I&apos;ve got a good correlation between the std error*compelxity badness and</p><p>m*n*std(err_m ^ err_n)/sqrt(n_primes)</p><p>where</p><p>m and n are the numbers of notes to the octave in each ET<br/>err_m and err_n are the weighted errors of the ETs<br/>n_primes is the number of prime intervals (including 2)<br/>std is the sample standard deviation<br/>sqrt is the square root<br/>^ is an exterior product, giving a result in vector form</p><p>For some perfectly reasonable temperaments the two badnesses agree to<br/>2 significant figures, which is probably not a coincidence.  As we<br/>have a reasonable wedgie complexity this is what we need for a<br/>reasonable wedgie error.</p><p>It&apos;s also a better agreement than that between what I call rin (the<br/>square root of 1 minus the square of the statistical correlation) and<br/>the sine of the angle between the error vectors, generalized as<br/>|a^b|/|a||b|</p><p>The trouble is, I don&apos;t know how to extract this weighted-error wedge<br/>product from the standard wedgie.</p><p>                    Graham</p></div><h3>Paul G Hjelmstad &#x3C;paul_hjelmstad@allianzlife.com&#x3E;</h3><span>1/5/2007 9:02:44 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; At 11:30 PM 1/2/2007, you wrote:<br/>&gt; &gt;I&apos;ve updated it now with a new equation that means the octave-<br/>specific<br/>&gt; &gt;error might be almost as simple and stable as the octave-equivalent<br/>&gt; &gt;one.  That&apos;s Equation 32 on page 10.  It&apos;s got a beautiful symmetry<br/>to<br/>&gt; &gt;it, so it&apos;s probably the one to put on a T-shirt.<br/>&gt;<br/>&gt; Good work, dude!<br/>&gt;<br/>&gt; -Carl</p><p>I agree. I hope to finish your paper this weekend. You make it<br/>meaningful too which really helps me get my mind around it :)</p><p> - Paul Hj</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>1/6/2007 9:40:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I wrote:</p><p>&gt; One thing to note is that the numerator&apos;s an approximation to badness<br/>&gt; squared.  As the whole thing&apos;s error squared that means the<br/>&gt; denominator must be an approximation to complexity squared.  And it<br/>&gt; works!<br/>&gt;<br/>&gt; It doesn&apos;t appear to calculate a generator mapping or wedge product by<br/>&gt; the back door.  But it is the determinant of a 2x2 matrix containing<br/>&gt; the means-of-errors-of-products so there is a similarity.</p><p>You can also generalize it to an nxn matrix containing<br/>means-of-errors-of-products.  That gives us a complexity measure that<br/>works for temperaments of any rank.  Previously we needed the wedgie<br/>for this.  You can also calculate it with wedge products, of course.<br/>It&apos;s close to the std-complexity, and so is also correlated with the<br/>Kees-max complexity</p><p>                         Graham.</p></div>