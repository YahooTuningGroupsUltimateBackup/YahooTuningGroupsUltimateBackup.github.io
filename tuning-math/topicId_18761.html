<a href="/tuning-math">back to list</a><h1>HE, seeded with the right half of the Stern-Brocot tree</h1><h3><a id=18761 href="#18761">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/1/2011 3:31:39 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This is 2400 cents of the Stern-Brocot tree down 20 levels, limited to<br/>only intervals less than 4/1. There are 1,835,011 intervals in this<br/>series, and this took like 30 minutes to compute. This is with Paul&apos;s<br/>code.</p><p><a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/HE%20Optimizations%20test%202/HEsbtree.png">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/HE%20Optimizations%20test%202/HEsbtree.png</a><br/><a href="http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/HE%20Optimizations%20test%202/HEsbtree.xls">http://tech.groups.yahoo.com/group/tuning-math/files/MikeBattaglia/HE%20Optimizations%20test%202/HEsbtree.xls</a></p><p>The Stern-Brocot tree generates intervals by a somewhat reasonable<br/>measure of complexity; it just keeps adding mediants between<br/>successive ratios. Generally speaking, simple ratios pop up before<br/>more complex ones. And, as you can see, the minima of entropy are<br/>ordered with respect to Tenney height, as you&apos;d expect.</p><p>Which is all fine and good, but the first local maxima occurs at about<br/>250 cents, because the way it calculates successive members in the<br/>series leaves -huge- widths around the simple ratios. This is with the<br/>curve seeded with about 2 million intervals, so don&apos;t expect it to<br/>change anytime soon.</p><p>So can we please stop saying that HE really only has one free parameter now?</p><p>-Mike</p></div><h3><a id=18762 href="#18762">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/1/2011 8:34:51 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;So can we please stop saying that HE really only has one free parameter now?<br/>&gt;</p><p>No way Jose.  First, I have no idea what I&apos;m looking at.<br/>The overlaid curves look quite similar; evidently they don&apos;t<br/>include the Stern-Brocot curves.  So what are they?</p><p>Second, you say you used an s-b tree to 20 levels.  Really,<br/>you should run up the number of levels until the curve<br/>stabilizes, if it can be made to do so.  Otherwise, the number<br/>of levels should be chosen to match what goes into the stable<br/>Tenney height curve -- somethin like the number of ratios<br/>admitted, or perhaps their average Tenney height.</p><p>Then we need plots of one curve against another (as I showed<br/>last) and/or plots of the minima vs. the Tenney height of their<br/>associated ratios.</p><p>Finally, on the overlay, the entropy at 1/1 is zero.  What&apos;s<br/>the deal there?  And it goes up to 1?  On the s-b curve, it<br/>goes from 0 to 9...?</p><p>Postfinally, when you say you truncate the s-b tree to above<br/>4/1... do you also truncate it below 1/1?  What are you seeding<br/>it with?  I would suggest seeding with 1/1 2/1, or if you insist<br/>on computing two octaves, 1/1 2/1 4/1.</p><p>-Carl</p></div><h3><a id=18763 href="#18763">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/1/2011 10:45:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Tue, Feb 1, 2011 at 11:34 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt; &gt;So can we please stop saying that HE really only has one free parameter now?<br/>&gt; &gt;<br/>&gt;<br/>&gt; No way Jose. First, I have no idea what I&apos;m looking at.<br/>&gt; The overlaid curves look quite similar; evidently they don&apos;t<br/>&gt; include the Stern-Brocot curves. So what are they?</p><p>The overlaid curves are various optimizations of HE/DC/whatever. The<br/>Stern-Brocot thing came later, as an afterthought. I didn&apos;t think<br/>you&apos;d really take it seriously. We&apos;re talking about a field of<br/>attraction for 1/1 that&apos;s 250 cents wide.</p><p>&gt; Second, you say you used an s-b tree to 20 levels. Really,<br/>&gt; you should run up the number of levels until the curve<br/>&gt; stabilizes, if it can be made to do so. Otherwise, the number<br/>&gt; of levels should be chosen to match what goes into the stable<br/>&gt; Tenney height curve -- somethin like the number of ratios<br/>&gt; admitted, or perhaps their average Tenney height.</p><p>There are 2 million ratios in this series. This is as stabilized as I<br/>think that it&apos;s going to get. Think about the way the Stern-Brocot<br/>tree adds ratios for an explanation of why 1/1&apos;s field of attraction<br/>ends up so wide. I don&apos;t understand what you mean by &quot;chosen to match<br/>what goes into the stable Tenney height curve.&quot;</p><p>&gt; Then we need plots of one curve against another (as I showed<br/>&gt; last) and/or plots of the minima vs. the Tenney height of their<br/>&gt; associated ratios.</p><p>So feel free to plot them against your data set. I don&apos;t think the<br/>results will yield a line, and you&apos;ve never told me why they should.<br/>If the minima don&apos;t line up exactly, but are still ordered by Tenney<br/>height, you won&apos;t get a line, but a bunch of lines intersecting at the<br/>top right.</p><p>&gt; Finally, on the overlay, the entropy at 1/1 is zero. What&apos;s<br/>&gt; the deal there? And it goes up to 1? On the s-b curve, it<br/>&gt; goes from 0 to 9...?</p><p>I&apos;ve normalized the entropy so that it goes from 0 to 1 in all cases.<br/>Looks like I screwed up the one for the SB curve.</p><p>&gt; Postfinally, when you say you truncate the s-b tree to above<br/>&gt; 4/1... do you also truncate it below 1/1? What are you seeding<br/>&gt; it with? I would suggest seeding with 1/1 2/1, or if you insist<br/>&gt; on computing two octaves, 1/1 2/1 4/1.</p><p>I&apos;m seeding it with 1/1 1/0, and then computing mediants forever,<br/>pruning if they end up out of range, which is the same thing you&apos;re<br/>doing.</p><p>-Mike</p></div><h3><a id=18764 href="#18764">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/1/2011 11:28:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Feb 2, 2011 at 1:45 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt; I&apos;m seeding it with 1/1 1/0, and then computing mediants forever,<br/>&gt; pruning if they end up out of range, which is the same thing you&apos;re<br/>&gt; doing.</p><p>Sorry, this is wrong; I meant 0/1 1/0. Ratios less than one are<br/>included, and range from 1/4 to 4/1. This took me about 20 minutes to<br/>compute.</p><p>-Mike</p></div><h3><a id=18765 href="#18765">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 1:46:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;The overlaid curves are various optimizations of HE/DC/whatever.</p><p>Yes, whatever are they?</p><p>&gt;&gt; Second, you say you used an s-b tree to 20 levels. Really,<br/>&gt;&gt; you should run up the number of levels until the curve<br/>&gt;&gt; stabilizes, if it can be made to do so. Otherwise, the number<br/>&gt;&gt; of levels should be chosen to match what goes into the stable<br/>&gt;&gt; Tenney height curve -- somethin like the number of ratios<br/>&gt;&gt; admitted, or perhaps their average Tenney height.<br/>&gt;<br/>&gt;There are 2 million ratios in this series. This is as stabilized as I<br/>&gt;think that it&apos;s going to get. Think about the way the Stern-Brocot<br/>&gt;tree adds ratios for an explanation of why 1/1&apos;s field of attraction<br/>&gt;ends up so wide. I don&apos;t understand what you mean by &quot;chosen to match<br/>&gt;what goes into the stable Tenney height curve.&quot;</p><p>There are 4218 ratios between 1 and 2 in a Tenney series of<br/>limit 10,000.  Their mean Tenney height is 5010.  Paul seemed<br/>to think the Tenney series version was stable at 10,000.  It<br/>would be interesting to see what the s-b version does as its<br/>limit is increased.  For that, stacked curves can work.</p><p>&gt;&gt; Then we need plots of one curve against another (as I showed<br/>&gt;&gt; last) and/or plots of the minima vs. the Tenney height of their<br/>&gt;&gt; associated ratios.<br/>&gt;<br/>&gt;So feel free to plot them against your data set.</p><p>How can I plot them if I don&apos;t even know what they are?  You<br/>dumped a bunch of xls files in a folder with cryptic names.</p><p>&gt;&gt; Postfinally, when you say you truncate the s-b tree to above<br/>&gt;&gt; 4/1... do you also truncate it below 1/1? What are you seeding<br/>&gt;&gt; it with? I would suggest seeding with 1/1 2/1, or if you insist<br/>&gt;&gt; on computing two octaves, 1/1 2/1 4/1.<br/>&gt;<br/>&gt;I&apos;m seeding it with 1/1 1/0, and then computing mediants forever,<br/>&gt;pruning if they end up out of range, which is the same thing you&apos;re<br/>&gt;doing.</p><p>Not quite.  For the 1/1 2/1 4/1 seed, the rightmost quadrant of<br/>the tree will be different.  However that looks pathological so<br/>forget I mentioned it.</p><p>-Carl</p></div><h3><a id=18766 href="#18766">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 1:47:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 11:28 PM 2/1/2011, you wrote:<br/>&gt;On Wed, Feb 2, 2011 at 1:45 AM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt; wrote:<br/>&gt;&gt; I&apos;m seeding it with 1/1 1/0, and then computing mediants forever,<br/>&gt;&gt; pruning if they end up out of range, which is the same thing you&apos;re<br/>&gt;&gt; doing.<br/>&gt;<br/>&gt;Sorry, this is wrong; I meant 0/1 1/0. Ratios less than one are<br/>&gt;included, and range from 1/4 to 4/1. This took me about 20 minutes to<br/>&gt;compute.</p><p>Ratios less than one, hmm, sounds wrong... can you try it without?</p><p>-Carl</p></div><h3><a id=18767 href="#18767">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/2/2011 8:32:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Feb 2, 2011 at 4:46 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; &gt;The overlaid curves are various optimizations of HE/DC/whatever.<br/>&gt;<br/>&gt; Yes, whatever are they?</p><p>I didn&apos;t want to get into the specifics at first, because the answer<br/>is very long. These are a bunch of are successive approximations and<br/>optimizations, each one also utilizing the optimization of the<br/>previous ones, that alter what Paul is doing, but always in a way that<br/>calculates -plogp. They are all approximations in the sense that the<br/>sqrt(n*d) widths version is an approximation of the mediant-mediant<br/>model. Sometimes it alters what Paul is doing in ways that don&apos;t<br/>correlate with r=1 to the original curve, but always in ways that<br/>preserve the locations of the minima and maxima and such.</p><p>They are a series of optimizations that prepares the curve for the<br/>final optimization, which is the convolution. I didn&apos;t throw that one<br/>in, which would be opt5, because as you know, I&apos;m having trouble<br/>figuring out which kernel corresponds to HE, if one even does at all.<br/>It will probably end up taking a form where each interval has a<br/>complexity sqrt(n*d + k) + l, where k and l are two constants that<br/>work out to various terms from simplifying out the entropy summation,<br/>which I have not yet managed to do.</p><p>So while the results are halfway between HE and DC, they all still<br/>involve taking the actual entropy of something. Some of the<br/>optimizations were determined mathematically, some empirically.</p><p>The &quot;ent&quot; one is Paul&apos;s HE with n*d&lt;10000 mediant-mediant widths. If<br/>you plot this against the sqrt(n*d) widths version, you won&apos;t get a<br/>line. At least I don&apos;t, with the tabular data that&apos;s on<br/>harmonic_entropy (called, I believe &quot;lumma.txt&quot;.). You get something<br/>kind of like a line, but at the top it diverges.</p><p>The &quot;opt&quot; one is my recoding of Paul&apos;s HE, but with two optimizations.<br/>The first is that I don&apos;t bother seeding the model with any intervals<br/>that lie two standard deviations away from the upper and lower bounds<br/>of the cents values I want to calculate. So if I&apos;m going from 0-1200<br/>cents, and s is 1%, then I only use intervals where n*d &lt; 10000 and<br/>where -34 &lt; 1200*log2(n/d) &lt; 1234. The second optimization is that I<br/>take advantage of the first identity in my proof, which is that G(i-d)<br/>= G(d-i), which means you can just work out the integral beforehand<br/>for each interval and get p_i(d) for each dyad a bit more quickly.<br/>This correlates with ent perfectly, which was a bit unexpected given<br/>the lossy nature of the first optimization.</p><p>&quot;opt2&quot; adds a second optimization, which is that rather than setting<br/>the p_i(d) for each dyad as Integral(i_low,i_high,G_s(d)), I set it as<br/>(i_high - i_low) * G_s(i-d). So, in other words, I did a<br/>quasi-sqrt(n*d) approximation - rather than assuming that the width<br/>was 1/sqrt(n*d) and multiplying by the value of the Gaussian at the<br/>point of the interval i, I figured out what the actual width was, but<br/>rather than integrating I just multiplied the result of that<br/>calculation by the value of the Gaussian at i. This was derived from<br/>observing how the integral changes as more intervals are added and<br/>i_high - i_low tends to zero. This decorrelates the curve a little<br/>bit, but r should approach 1 as N -&gt; Infinity. We&apos;re preparing<br/>ourselves for a convolution now, since the curve now consists of a<br/>bunch of Gaussians *log(Gaussians) added together.</p><p>&quot;opt3&quot; adds a third optimization, which is my version of Paul&apos;s<br/>sqrt(n*d) widths approximation. I replace (i_high-i_low) with<br/>k/sqrt(n*d) for each interval, where k is a normalizing constant.<br/>Different values of k will either make this work or not work. Paul&apos;s<br/>approach was to work out all of the probabilities, and then<br/>retrospectively pick k such that at each point, the probabilities sum<br/>to 1. I wanted to work out an algebraic expression beforehand such<br/>that it converges at infinity that all of the probabilities sum to 1<br/>for all i. I wasn&apos;t sure what the exact expression would be, but it<br/>seems to be asymptotic to the number of entries in the series for some<br/>N. So I set k to L = length(series), and it turned out to be the only<br/>value that worked, as well as 0.5L, 2L, etc, but never L^2 or sqrt(L).</p><p>This correlates to ent decently well, but less well than opt2. It will<br/>probably converge to the sqrt(n*d) widths at N-&gt; infinity.</p><p>&quot;opt4&quot; adds the final optimization, which is that I did all of the<br/>above, but I seeded the calculation with a Farey series rather than a<br/>Tenney series. This is because Farey series are much, much, much<br/>quicker to compute. This means that you get the quickness of the Farey<br/>series, but the slope of the Tenney series curve. This one was<br/>trickier to work out and I did it almost entirely empirically.<br/>Different values of N end up changing the slope, with N &lt; 80 giving a<br/>negative Farey-ish slope, with 80 giving a slope of about 0, and N &gt;<br/>80 giving a positive slope. I don&apos;t know why this is, but setting k as<br/>above to length(series) produced the above behavior. The algebra for<br/>the proper k is really over my head now, since I don&apos;t know what the<br/>invariant measure for the Tenney series is to compare with that of the<br/>Farey series, so I tweaked it a bit to try and give it a slope of 0<br/>across the range of N. This took too long and I gave up and just went<br/>with N=80. Correlates less well than the others, but the minima are as<br/>always in the same place. The maxima shift sometimes by a few cents<br/>here and there between all of these but are generally in the same<br/>place.</p><p>&gt; &gt;There are 2 million ratios in this series. This is as stabilized as I<br/>&gt; &gt;think that it&apos;s going to get. Think about the way the Stern-Brocot<br/>&gt; &gt;tree adds ratios for an explanation of why 1/1&apos;s field of attraction<br/>&gt; &gt;ends up so wide. I don&apos;t understand what you mean by &quot;chosen to match<br/>&gt; &gt;what goes into the stable Tenney height curve.&quot;<br/>&gt;<br/>&gt; There are 4218 ratios between 1 and 2 in a Tenney series of<br/>&gt; limit 10,000. Their mean Tenney height is 5010. Paul seemed<br/>&gt; to think the Tenney series version was stable at 10,000. It<br/>&gt; would be interesting to see what the s-b version does as its<br/>&gt; limit is increased. For that, stacked curves can work.</p><p>I don&apos;t know, but I don&apos;t think we&apos;ll ever be able to run that test.<br/>It took me a half an hour to run this one. Maybe Colby can hook this<br/>one up for us with his fancy supercomputer. I&apos;ve run it before with<br/>different values and the curve generally looks the same as the one I<br/>posted. Again, think about it - intervals right next to 1/1,<br/>cents-wise, will take the longest to appear, since every successive<br/>level just brings you one mediant closer to 1/1, and the mediants are<br/>always skewed -away- from 1/1. By the time any intervals around 50<br/>cents appear at all, you have a quarter of a million intervals closer<br/>to 9/8.</p><p>-Mike</p></div><h3><a id=18768 href="#18768">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 8:45:31 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Sounds interesting, but I&apos;m afraid it&apos;s 0a bit of tl;dr and a bit<br/>of WTF for me.  I will bow out, with only words of encouragement.</p><p>-Carl</p><p>At 08:32 PM 2/2/2011, you wrote:<br/>&gt;On Wed, Feb 2, 2011 at 4:46 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; &gt;The overlaid curves are various optimizations of HE/DC/whatever.<br/>&gt;&gt;<br/>&gt;&gt; Yes, whatever are they?<br/>&gt;<br/>&gt;I didn&apos;t want to get into the specifics at first, because the answer<br/>&gt;is very long. These are a bunch of are successive approximations and<br/>&gt;optimizations, each one also utilizing the optimization of the<br/>&gt;previous ones, that alter what Paul is doing, but always in a way that<br/>&gt;calculates -plogp. They are all approximations in the sense that the<br/>&gt;sqrt(n*d) widths version is an approximation of the mediant-mediant<br/>&gt;model. Sometimes it alters what Paul is doing in ways that don&apos;t<br/>&gt;correlate with r=1 to the original curve, but always in ways that<br/>&gt;preserve the locations of the minima and maxima and such.<br/>&gt;<br/>&gt;They are a series of optimizations that prepares the curve for the<br/>&gt;final optimization, which is the convolution. I didn&apos;t throw that one<br/>&gt;in, which would be opt5, because as you know, I&apos;m having trouble<br/>&gt;figuring out which kernel corresponds to HE, if one even does at all.<br/>&gt;It will probably end up taking a form where each interval has a<br/>&gt;complexity sqrt(n*d + k) + l, where k and l are two constants that<br/>&gt;work out to various terms from simplifying out the entropy summation,<br/>&gt;which I have not yet managed to do.<br/>&gt;<br/>&gt;So while the results are halfway between HE and DC, they all still<br/>&gt;involve taking the actual entropy of something. Some of the<br/>&gt;optimizations were determined mathematically, some empirically.<br/>&gt;<br/>&gt;The &quot;ent&quot; one is Paul&apos;s HE with n*d&lt;10000 mediant-mediant widths. If<br/>&gt;you plot this against the sqrt(n*d) widths version, you won&apos;t get a<br/>&gt;line. At least I don&apos;t, with the tabular data that&apos;s on<br/>&gt;harmonic_entropy (called, I believe &quot;lumma.txt&quot;.). You get something<br/>&gt;kind of like a line, but at the top it diverges.<br/>&gt;<br/>&gt;The &quot;opt&quot; one is my recoding of Paul&apos;s HE, but with two optimizations.<br/>&gt;The first is that I don&apos;t bother seeding the model with any intervals<br/>&gt;that lie two standard deviations away from the upper and lower bounds<br/>&gt;of the cents values I want to calculate. So if I&apos;m going from 0-1200<br/>&gt;cents, and s is 1%, then I only use intervals where n*d &lt; 10000 and<br/>&gt;where -34 &lt; 1200*log2(n/d) &lt; 1234. The second optimization is that I<br/>&gt;take advantage of the first identity in my proof, which is that G(i-d)<br/>&gt;= G(d-i), which means you can just work out the integral beforehand<br/>&gt;for each interval and get p_i(d) for each dyad a bit more quickly.<br/>&gt;This correlates with ent perfectly, which was a bit unexpected given<br/>&gt;the lossy nature of the first optimization.<br/>&gt;<br/>&gt;&quot;opt2&quot; adds a second optimization, which is that rather than setting<br/>&gt;the p_i(d) for each dyad as Integral(i_low,i_high,G_s(d)), I set it as<br/>&gt;(i_high - i_low) * G_s(i-d). So, in other words, I did a<br/>&gt;quasi-sqrt(n*d) approximation - rather than assuming that the width<br/>&gt;was 1/sqrt(n*d) and multiplying by the value of the Gaussian at the<br/>&gt;point of the interval i, I figured out what the actual width was, but<br/>&gt;rather than integrating I just multiplied the result of that<br/>&gt;calculation by the value of the Gaussian at i. This was derived from<br/>&gt;observing how the integral changes as more intervals are added and<br/>&gt;i_high - i_low tends to zero. This decorrelates the curve a little<br/>&gt;bit, but r should approach 1 as N -&gt; Infinity. We&apos;re preparing<br/>&gt;ourselves for a convolution now, since the curve now consists of a<br/>&gt;bunch of Gaussians *log(Gaussians) added together.<br/>&gt;<br/>&gt;&quot;opt3&quot; adds a third optimization, which is my version of Paul&apos;s<br/>&gt;sqrt(n*d) widths approximation. I replace (i_high-i_low) with<br/>&gt;k/sqrt(n*d) for each interval, where k is a normalizing constant.<br/>&gt;Different values of k will either make this work or not work. Paul&apos;s<br/>&gt;approach was to work out all of the probabilities, and then<br/>&gt;retrospectively pick k such that at each point, the probabilities sum<br/>&gt;to 1. I wanted to work out an algebraic expression beforehand such<br/>&gt;that it converges at infinity that all of the probabilities sum to 1<br/>&gt;for all i. I wasn&apos;t sure what the exact expression would be, but it<br/>&gt;seems to be asymptotic to the number of entries in the series for some<br/>&gt;N. So I set k to L = length(series), and it turned out to be the only<br/>&gt;value that worked, as well as 0.5L, 2L, etc, but never L^2 or sqrt(L).<br/>&gt;<br/>&gt;This correlates to ent decently well, but less well than opt2. It will<br/>&gt;probably converge to the sqrt(n*d) widths at N-&gt; infinity.<br/>&gt;<br/>&gt;&quot;opt4&quot; adds the final optimization, which is that I did all of the<br/>&gt;above, but I seeded the calculation with a Farey series rather than a<br/>&gt;Tenney series. This is because Farey series are much, much, much<br/>&gt;quicker to compute. This means that you get the quickness of the Farey<br/>&gt;series, but the slope of the Tenney series curve. This one was<br/>&gt;trickier to work out and I did it almost entirely empirically.<br/>&gt;Different values of N end up changing the slope, with N &lt; 80 giving a<br/>&gt;negative Farey-ish slope, with 80 giving a slope of about 0, and N &gt;<br/>&gt;80 giving a positive slope. I don&apos;t know why this is, but setting k as<br/>&gt;above to length(series) produced the above behavior. The algebra for<br/>&gt;the proper k is really over my head now, since I don&apos;t know what the<br/>&gt;invariant measure for the Tenney series is to compare with that of the<br/>&gt;Farey series, so I tweaked it a bit to try and give it a slope of 0<br/>&gt;across the range of N. This took too long and I gave up and just went<br/>&gt;with N=80. Correlates less well than the others, but the minima are as<br/>&gt;always in the same place. The maxima shift sometimes by a few cents<br/>&gt;here and there between all of these but are generally in the same<br/>&gt;place.</p></div><h3><a id=18769 href="#18769">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 8:51:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Mike wrote:<br/>&gt;&gt; There are 4218 ratios between 1 and 2 in a Tenney series of<br/>&gt;&gt; limit 10,000. Their mean Tenney height is 5010. Paul seemed<br/>&gt;&gt; to think the Tenney series version was stable at 10,000. It<br/>&gt;&gt; would be interesting to see what the s-b version does as its<br/>&gt;&gt; limit is increased. For that, stacked curves can work.<br/>&gt;<br/>&gt;I don&apos;t know, but I don&apos;t think we&apos;ll ever be able to run that test.<br/>&gt;It took me a half an hour to run this one.</p><p>I meant, start small and work up to what you did already.</p><p>&gt;Again, think about it - intervals right next to 1/1,<br/>&gt;cents-wise, will take the longest to appear, since every successive<br/>&gt;level just brings you one mediant closer to 1/1, and the mediants are<br/>&gt;always skewed -away- from 1/1. By the time any intervals around 50<br/>&gt;cents appear at all, you have a quarter of a million intervals closer<br/>&gt;to 9/8.</p><p>Yep, makes sense.  Also I showed this in a previous post.  And<br/>you know, minus critical band effects it might actually be ok.</p><p>-Carl</p></div><h3><a id=18770 href="#18770">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/2/2011 9:20:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Feb 2, 2011 at 11:45 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; Sounds interesting, but I&apos;m afraid it&apos;s 0a bit of tl;dr and a bit<br/>&gt; of WTF for me. I will bow out, with only words of encouragement.</p><p>Haha OK, I&apos;ll sum everything up in plain English as follows:</p><p>1) I object to using cross-correlations to HE to validate the<br/>convolution&apos;s usefulness as an approximation. This is because, while<br/>cross-correlations in general are a good tool to see how data syncs up<br/>with other data, I don&apos;t think anyone is using the HE curve like that<br/>- we&apos;re more concerned with the locations of the maxima and the minima<br/>and their ordering, not the locations in between. The maxima and<br/>minima are always in the same spot with both models and the minima are<br/>always ordered by Tenney height, but the spot at which a detuned 3/2<br/>becomes equivalent in entropy to a just 5/4 changes between the two<br/>models.</p><p>As you know, I am all about the theory whereby a sharpened 3/2 can<br/>become as minor sounding as a 6/5, but I don&apos;t think that HE predicts<br/>where that point happens. If it does, then I have been missing out on<br/>an entirely different world of interpretation, and will be the first<br/>to ditch this and try something new. If not, then the convolution<br/>model serves as an ultra-fast and useful approximation to all of the<br/>features of HE that are themselves psychoacoustically justified.</p><p>2) I am going to change the term &quot;Distributed Complexity&quot; to be a<br/>simple descriptor referring to any psychoacoustic model that ends up<br/>distributing the consonance of an interval out in pitch space by some<br/>measure of complexity. It is not necessary do this with the same<br/>spreading function for each interval - if so, then you have a<br/>convolution-based model, and if not, then you don&apos;t. HE is a type of<br/>model that ends up doing this, and is currently the most theoretically<br/>justfied (maybe). What I&apos;ve been calling DC I will just call a<br/>convolution-based DC model, and it will be a type of &quot;DC&quot; model.</p><p>3) For the convolution model I used to call &quot;DC,&quot; I am going to leave<br/>the question of whether or not the -plogp actually works out to the<br/>linear superposition of scaled Gaussians in the infinite case as an<br/>open hypothesis. If this is false, then by approximating -plogp as a<br/>piecewise linear function, we can asymptotically approach HE by<br/>performing one convolution per &quot;piece&quot; and adding them together, with<br/>more pieces yielding a curve that converges to HE. If it&apos;s true, then<br/>there is some kernel that yields HE when you convolve it with a<br/>Gaussian.</p><p>Right now, as it stands, if I seed the model with sqrt(n*d) heights<br/>instead of n*d heights, the maxima get screwed up vs HE, so there are<br/>bounds on how well the approximation will work. If the hypothesis is<br/>true then some term can be added to the sqrt(n*d) complexity to<br/>correct this, and if not then it can&apos;t.</p><p>-Mike</p></div><h3><a id=18771 href="#18771">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>2/2/2011 9:21:43 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Wed, Feb 2, 2011 at 11:51 PM, Carl Lumma &lt;<a href="mailto:carl@lumma.org">carl@lumma.org</a>&gt; wrote:<br/>&gt;<br/>&gt; I meant, start small and work up to what you did already.</p><p>I&apos;ll work out a graph for this at some point, but they all look the same.</p><p>&gt; &gt;Again, think about it - intervals right next to 1/1,<br/>&gt; &gt;cents-wise, will take the longest to appear, since every successive<br/>&gt; &gt;level just brings you one mediant closer to 1/1, and the mediants are<br/>&gt; &gt;always skewed -away- from 1/1. By the time any intervals around 50<br/>&gt; &gt;cents appear at all, you have a quarter of a million intervals closer<br/>&gt; &gt;to 9/8.<br/>&gt;<br/>&gt; Yep, makes sense. Also I showed this in a previous post. And<br/>&gt; you know, minus critical band effects it might actually be ok.</p><p>I don&apos;t think so. The same characteristics apply at 2/1 as well. And<br/>9/8 is a lot more consonant than 16/15 for reasons besides critical<br/>band effects.</p><p>-Mike</p></div><h3><a id=18772 href="#18772">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 11:13:19 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Yep, makes sense. Also I showed this in a previous post. And<br/>&gt;&gt; you know, minus critical band effects it might actually be ok.<br/>&gt;<br/>&gt;I don&apos;t think so. The same characteristics apply at 2/1 as well. And<br/>&gt;9/8 is a lot more consonant than 16/15 for reasons besides critical<br/>&gt;band effects.<br/>&gt;</p><p>Yeah, OK. -C.</p></div><h3><a id=18773 href="#18773">ðŸ”—</a>Carl Lumma &#x3C;carl@lumma.org&#x3E;</h3><span>2/2/2011 11:15:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;1) I object to using cross-correlations to HE to validate the<br/>&gt;convolution&apos;s usefulness as an approximation.</p><p>Mike, I never suggested this.   -Carl</p></div>