<a href="/tuning-math">back to list</a><h1>7-limit JI-approx. efficiency of N-tets</h1><h3><a id=14314 href="#14314">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/8/2006 10:49:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hey all,</p><p>I wrote a Python script to rank the octave-ET&apos;s according to 7-limit<br/>JI-approximation efficiency. By this, I mean the following equation:</p><p>Eff = TE*W</p><p>where Eff is &quot;efficiency&quot; (the lower the number, the more efficient<br/>the n-tet is at approximating n-limit JI with fewer notes), TE is the<br/>total error in cents from JI of the intervals one is interested in<br/>looking at (for me, I chose the 7 limit intervals plus their<br/>inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>ratio of dissonant intervals to consonant intervals in the ET, which<br/>obviously grows as the ET size grows. Thus, it is a &apos;penalty&apos; for an<br/>ET to get to large in this ranking, unless of course, it really<br/>significantly lowers the JI-approximation error.</p><p>One motivation for the ranking might be the question--What n-tet gives<br/>me the most consonance for the buck (the buck being the size of the<br/>n-tet).</p><p>Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins! (no surprise there)<br/>And, perhaps we get another insight into why 12 will always remain an<br/>important system: as we all know, but don&apos;t often admit, it does a<br/>fair job of &apos;expressing&apos; a fair amount of important intervals with<br/>only a handful of tones.....we also see that the equivalent of<br/>Republican wasteful spending, i.e. the George Bush of n-tets, is 97-equal.</p><p>The left column is &apos;total error in cents&apos; and the right column is our<br/>ranking of interest, &apos;efficiency&apos;:</p><p>total_error     efficiency</p><p>99:  11.9495    31:  66.8946<br/>72:  22.2008    12:  76.6857<br/>94:  29.0217    99:  79.0507<br/>68:  31.7373    41:  96.4323<br/>87:  32.0118    72: 100.7573<br/>84:  32.4549     7: 102.3707<br/>95:  37.2044    19: 104.3473<br/>53:  37.2751    15: 112.3440<br/>77:  37.9581    53: 114.6928<br/>89:  38.7576    14: 124.1117<br/>90:  40.4889     6: 125.4311<br/>93:  40.4928    27: 126.0480<br/>80:  44.4171    22: 133.4705<br/>41:  44.7721    68: 134.2733<br/>82:  44.7721    10: 137.0975<br/>91:  44.9235    18: 144.5536<br/>62:  46.9444    37: 149.9255<br/>98:  47.2205    21: 153.0049<br/>88:  47.3411    25: 153.4885<br/>96:  47.5408    46: 155.2680<br/>81:  47.7354    23: 159.2434<br/>58:  47.9142    11: 161.2244<br/>31:  48.3128    13: 165.7086<br/>76:  48.6515    58: 165.8568<br/>78:  49.3335    26: 167.4354<br/>83:  49.3755    35: 169.1267<br/>97:  50.4854     5: 171.8133<br/>92:  52.8566    62: 176.9442<br/>85:  55.1878    84: 177.2534<br/>86:  55.9476    17: 178.0986<br/>65:  58.9982    94: 180.8277<br/>75:  59.1556    36: 182.0417<br/>63:  59.8286    87: 182.2212<br/>46:  61.1662     8: 182.5136<br/>74:  62.6937    77: 186.8706<br/>79:  63.0488    16: 188.6049<br/>64:  63.4544    50: 205.3443<br/>73:  63.6051    43: 208.8975<br/>66:  63.7535     9: 214.2032<br/>57:  64.4410    45: 215.9116<br/>60:  65.5829    49: 216.1089<br/>70:  65.8457    57: 218.1080<br/>71:  66.4328    34: 222.3343<br/>67:  66.6762    47: 225.0262<br/>69:  69.3722    89: 226.5829<br/>56:  69.9429    80: 228.9191<br/>50:  72.1480    63: 230.1098<br/>59:  74.6132    56: 231.3495<br/>54:  76.1355    38: 232.3184<br/>61:  77.8686    30: 232.4634<br/>49:  78.0393    29: 232.9472<br/>37:  81.2097    95: 234.6738<br/>52:  82.0285    42: 235.3100<br/>47:  86.0394    76: 235.7728<br/>45:  87.7141    65: 235.9929<br/>55:  89.2080    60: 237.1073<br/>43:  90.5223    82: 237.6367<br/>51:  90.6945    40: 238.8968<br/>48:  92.7956    90: 239.8191<br/>35:  99.9385    54: 240.1196<br/>27: 100.8384    39: 240.9800<br/>36: 102.8932    24: 241.9821<br/>42: 105.4838    33: 243.2895<br/>40: 115.0244    32: 245.7927<br/>44: 116.7938    52: 246.0854<br/>39: 120.4900    78: 246.6677<br/>38: 120.8056    64: 248.9364<br/>34: 121.2733    93: 249.1862<br/>22: 133.4705    81: 249.6930<br/>33: 139.0226    48: 249.8344<br/>29: 142.3567    20: 250.2792<br/>19: 143.4776    28: 252.7545<br/>26: 143.5161    66: 259.9180<br/>32: 147.4756    59: 264.0161<br/>30: 154.9756    51: 265.1071<br/>28: 163.5470    83: 265.8680<br/>25: 166.2792    91: 269.5412<br/>24: 172.8444    88: 273.1216<br/>23: 173.7200    67: 276.9627<br/>21: 204.0065    44: 278.5083<br/>15: 224.6880    75: 282.1268<br/>18: 227.1556    61: 287.5149<br/>12: 230.0571    55: 288.2105<br/>16: 242.4920    70: 288.7081<br/>20: 250.2792    73: 293.5622<br/>17: 254.4266    74: 294.1783<br/>14: 310.2792    71: 296.3927<br/>10: 319.8941    69: 298.8341<br/>13: 372.8444    96: 303.5295<br/> 9: 428.4065    85: 305.6556<br/>11: 429.9317    98: 308.7495<br/> 8: 547.5408    86: 314.1673<br/> 7: 614.2242    79: 320.0938<br/> 6: 627.1556    92: 321.2052<br/> 5: 687.2532    97: 326.2135</p><p>-Aaron.</p></div><h3><a id=14315 href="#14315">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 12:57:48 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hi Aaron!</p><p>This is what we might call a type of &quot;badness&quot; calculation, or<br/>rather, the inverse of one.  Actually, since you&apos;ve defined<br/>efficiency as going up when Eff goes down, it&apos;s not the inverse.<br/>The general approach is</p><p>badness = complexity * error</p><p>Complexity can be number of notes, but this gets tricky in linear<br/>and planar temperaments where the number of notes is not fixed.<br/>Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>forgetting have proposed various complexity measures.  Usually<br/>they wind up being fairly similar to one another.</p><p>Error also has variants.  Weighted error is popular; the deviation<br/>from JI is weighted depending on the harmonic limit of the interval<br/>in question.  There&apos;s weighted complexity, too, for that matter.</p><p>Probably thousands of messages have been written on this topic,<br/>with little consensus to this day.  :(</p><p>-Carl</p><p>At 10:49 PM 2/8/2006, you wrote:<br/>&gt;Hey all,<br/>&gt;<br/>&gt;I wrote a Python script to rank the octave-ET&apos;s according to 7-limit<br/>&gt;JI-approximation efficiency. By this, I mean the following equation:<br/>&gt;<br/>&gt;Eff = TE*W<br/>&gt;<br/>&gt;where Eff is &quot;efficiency&quot; (the lower the number, the more efficient<br/>&gt;the n-tet is at approximating n-limit JI with fewer notes), TE is the<br/>&gt;total error in cents from JI of the intervals one is interested in<br/>&gt;looking at (for me, I chose the 7 limit intervals plus their<br/>&gt;inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>&gt;ratio of dissonant intervals to consonant intervals in the ET, which<br/>&gt;obviously grows as the ET size grows. Thus, it is a &apos;penalty&apos; for an<br/>&gt;ET to get to large in this ranking, unless of course, it really<br/>&gt;significantly lowers the JI-approximation error.<br/>&gt;<br/>&gt;One motivation for the ranking might be the question--What n-tet gives<br/>&gt;me the most consonance for the buck (the buck being the size of the<br/>&gt;n-tet).<br/>&gt;<br/>&gt;Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins! (no surprise there)<br/>&gt;And, perhaps we get another insight into why 12 will always remain an<br/>&gt;important system: as we all know, but don&apos;t often admit, it does a<br/>&gt;fair job of &apos;expressing&apos; a fair amount of important intervals with<br/>&gt;only a handful of tones.....we also see that the equivalent of<br/>&gt;Republican wasteful spending, i.e. the George Bush of n-tets, is 97-equal.<br/>&gt;<br/>&gt;The left column is &apos;total error in cents&apos; and the right column is our<br/>&gt;ranking of interest, &apos;efficiency&apos;:<br/>&gt;<br/>&gt;total_error     efficiency<br/>&gt;<br/>&gt;99:  11.9495    31:  66.8946<br/>&gt;72:  22.2008    12:  76.6857<br/>&gt;94:  29.0217    99:  79.0507<br/>&gt;68:  31.7373    41:  96.4323<br/>&gt;87:  32.0118    72: 100.7573<br/>&gt;84:  32.4549     7: 102.3707<br/>&gt;95:  37.2044    19: 104.3473<br/>&gt;53:  37.2751    15: 112.3440<br/>&gt;77:  37.9581    53: 114.6928<br/>&gt;89:  38.7576    14: 124.1117<br/>&gt;90:  40.4889     6: 125.4311<br/>&gt;93:  40.4928    27: 126.0480<br/>&gt;80:  44.4171    22: 133.4705<br/>&gt;41:  44.7721    68: 134.2733<br/>&gt;82:  44.7721    10: 137.0975<br/>&gt;91:  44.9235    18: 144.5536<br/>&gt;62:  46.9444    37: 149.9255<br/>&gt;98:  47.2205    21: 153.0049<br/>&gt;88:  47.3411    25: 153.4885<br/>&gt;96:  47.5408    46: 155.2680<br/>&gt;81:  47.7354    23: 159.2434<br/>&gt;58:  47.9142    11: 161.2244<br/>&gt;31:  48.3128    13: 165.7086<br/>&gt;76:  48.6515    58: 165.8568<br/>&gt;78:  49.3335    26: 167.4354<br/>&gt;83:  49.3755    35: 169.1267<br/>&gt;97:  50.4854     5: 171.8133<br/>&gt;92:  52.8566    62: 176.9442<br/>&gt;85:  55.1878    84: 177.2534<br/>&gt;86:  55.9476    17: 178.0986<br/>&gt;65:  58.9982    94: 180.8277<br/>&gt;75:  59.1556    36: 182.0417<br/>&gt;63:  59.8286    87: 182.2212<br/>&gt;46:  61.1662     8: 182.5136<br/>&gt;74:  62.6937    77: 186.8706<br/>&gt;79:  63.0488    16: 188.6049<br/>&gt;64:  63.4544    50: 205.3443<br/>&gt;73:  63.6051    43: 208.8975<br/>&gt;66:  63.7535     9: 214.2032<br/>&gt;57:  64.4410    45: 215.9116<br/>&gt;60:  65.5829    49: 216.1089<br/>&gt;70:  65.8457    57: 218.1080<br/>&gt;71:  66.4328    34: 222.3343<br/>&gt;67:  66.6762    47: 225.0262<br/>&gt;69:  69.3722    89: 226.5829<br/>&gt;56:  69.9429    80: 228.9191<br/>&gt;50:  72.1480    63: 230.1098<br/>&gt;59:  74.6132    56: 231.3495<br/>&gt;54:  76.1355    38: 232.3184<br/>&gt;61:  77.8686    30: 232.4634<br/>&gt;49:  78.0393    29: 232.9472<br/>&gt;37:  81.2097    95: 234.6738<br/>&gt;52:  82.0285    42: 235.3100<br/>&gt;47:  86.0394    76: 235.7728<br/>&gt;45:  87.7141    65: 235.9929<br/>&gt;55:  89.2080    60: 237.1073<br/>&gt;43:  90.5223    82: 237.6367<br/>&gt;51:  90.6945    40: 238.8968<br/>&gt;48:  92.7956    90: 239.8191<br/>&gt;35:  99.9385    54: 240.1196<br/>&gt;27: 100.8384    39: 240.9800<br/>&gt;36: 102.8932    24: 241.9821<br/>&gt;42: 105.4838    33: 243.2895<br/>&gt;40: 115.0244    32: 245.7927<br/>&gt;44: 116.7938    52: 246.0854<br/>&gt;39: 120.4900    78: 246.6677<br/>&gt;38: 120.8056    64: 248.9364<br/>&gt;34: 121.2733    93: 249.1862<br/>&gt;22: 133.4705    81: 249.6930<br/>&gt;33: 139.0226    48: 249.8344<br/>&gt;29: 142.3567    20: 250.2792<br/>&gt;19: 143.4776    28: 252.7545<br/>&gt;26: 143.5161    66: 259.9180<br/>&gt;32: 147.4756    59: 264.0161<br/>&gt;30: 154.9756    51: 265.1071<br/>&gt;28: 163.5470    83: 265.8680<br/>&gt;25: 166.2792    91: 269.5412<br/>&gt;24: 172.8444    88: 273.1216<br/>&gt;23: 173.7200    67: 276.9627<br/>&gt;21: 204.0065    44: 278.5083<br/>&gt;15: 224.6880    75: 282.1268<br/>&gt;18: 227.1556    61: 287.5149<br/>&gt;12: 230.0571    55: 288.2105<br/>&gt;16: 242.4920    70: 288.7081<br/>&gt;20: 250.2792    73: 293.5622<br/>&gt;17: 254.4266    74: 294.1783<br/>&gt;14: 310.2792    71: 296.3927<br/>&gt;10: 319.8941    69: 298.8341<br/>&gt;13: 372.8444    96: 303.5295<br/>&gt; 9: 428.4065    85: 305.6556<br/>&gt;11: 429.9317    98: 308.7495<br/>&gt; 8: 547.5408    86: 314.1673<br/>&gt; 7: 614.2242    79: 320.0938<br/>&gt; 6: 627.1556    92: 321.2052<br/>&gt; 5: 687.2532    97: 326.2135<br/>&gt;<br/>&gt;-Aaron.</p></div><h3><a id=14316 href="#14316">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 2:12:15 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:</p><p>&gt; Eff = TE*W<br/>&gt;<br/>&gt; where Eff is &quot;efficiency&quot; (the lower the number, the more efficient<br/>&gt; the n-tet is at approximating n-limit JI with fewer notes), TE is the<br/>&gt; total error in cents from JI of the intervals one is interested in<br/>&gt; looking at (for me, I chose the 7 limit intervals plus their<br/>&gt; inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>&gt; ratio of dissonant intervals to consonant intervals in the ET, which<br/>&gt; obviously grows as the ET size grows.</p><p>I&apos;m afraid these statements need to be clarified if you intend for<br/>them to be precise definitions. What, *precisely*, is TE? Sum of<br/>absolute values of errors of elements of the 7-limit tonality diamond,<br/>perhaps? What is W? &quot;Ratio of dissonant intervals to consonant<br/>intervals&quot; is meaningless unless you defined what a consonant interval<br/>is. Tonality diamond interval, approximated by the rounded val, or<br/>something else?</p><p>&gt; Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins!</p><p>Here&apos;s the sorted ranking using another system, which you might want<br/>to consider for comparison; it ranks 5-99 according to the unweighted<br/>maximum error of the 7-limit tonality diamond, adjusted to be logflat,<br/>so that there are an infinity of ets scoring less than 1, and the size<br/>of such ets grows roughly exponentially.</p><p>31 .483982<br/>99 .597311<br/>5 .730389<br/>41 .743399<br/>72 .743933<br/>12 .758467<br/>9 .767221<br/>10 .796400<br/>15 .828367<br/>6 .890798<br/>22 .898361<br/>19 .906572<br/>68 .908372<br/>27 .923824<br/>53 1.023130<br/>16 1.113068<br/>26 1.122497<br/>7 1.151608<br/>46 1.181095<br/>37 1.187562<br/>62 1.219558<br/>18 1.233517<br/>58 1.270308<br/>57 1.346596<br/>50 1.354788<br/>29 1.380638<br/>84 1.397227<br/>21 1.444911<br/>43 1.531426<br/>87 1.542628<br/>35 1.549463<br/>36 1.569623<br/>90 1.583616<br/>77 1.607562<br/>49 1.608663<br/>25 1.636896<br/>42 1.665086<br/>94 1.681776<br/>63 1.706349<br/>81 1.712305<br/>60 1.727613<br/>45 1.731369<br/>56 1.736848<br/>8 1.766829<br/>80 1.773558<br/>40 1.783082<br/>34 1.818531<br/>89 1.830823<br/>32 1.850589<br/>38 1.861797<br/>82 1.873248<br/>24 1.875628<br/>59 1.896716<br/>14 1.918414<br/>17 1.922660<br/>95 1.955489<br/>47 1.976035<br/>76 1.976161<br/>11 1.986856<br/>70 1.999464<br/>83 2.000319<br/>20 2.006801<br/>78 2.065046<br/>73 2.086531<br/>30 2.087354<br/>93 2.094068<br/>13 2.095278<br/>91 2.110875<br/>88 2.120343<br/>51 2.170341<br/>33 2.181558<br/>65 2.222431<br/>44 2.263729<br/>74 2.279532<br/>28 2.347320<br/>61 2.405343<br/>75 2.500727<br/>23 2.516749<br/>52 2.529855<br/>67 2.531584<br/>48 2.542196<br/>97 2.628467<br/>55 2.656269<br/>69 2.686210<br/>66 2.739556<br/>79 2.794541<br/>92 2.976172<br/>96 2.977757<br/>54 3.009229<br/>39 3.159409<br/>85 3.248025<br/>71 3.270674<br/>86 3.295969<br/>64 3.336808<br/>98 3.584023</p></div><h3><a id=14317 href="#14317">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 6:29:29 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt;<br/>&gt; &gt; Eff = TE*W<br/>&gt; &gt;<br/>&gt; &gt; where Eff is &quot;efficiency&quot; (the lower the number, the more efficient<br/>&gt; &gt; the n-tet is at approximating n-limit JI with fewer notes), TE is the<br/>&gt; &gt; total error in cents from JI of the intervals one is interested in<br/>&gt; &gt; looking at (for me, I chose the 7 limit intervals plus their<br/>&gt; &gt; inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>&gt; &gt; ratio of dissonant intervals to consonant intervals in the ET, which<br/>&gt; &gt; obviously grows as the ET size grows.<br/>&gt;<br/>&gt; I&apos;m afraid these statements need to be clarified if you intend for<br/>&gt; them to be precise definitions.</p><p>You are right, of course; forgive my 1-in-the-morning lack-of-rigor...;)</p><p>&gt;What, *precisely*, is TE? Sum of<br/>&gt; absolute values of errors of elements of the 7-limit tonality diamond,<br/>&gt; perhaps?</p><p>Exactly!</p><p>&gt; What is W? &quot;Ratio of dissonant intervals to consonant<br/>&gt; intervals&quot; is meaningless unless you defined what a consonant interval<br/>&gt; is. Tonality diamond interval, approximated by the rounded val, or<br/>&gt; something else?</p><p>A consonance would be anything in said diamond. So, in a given n-tet,<br/>it would be that n-tet&apos;s approximation of that interval.</p><p>I should have noted that I set up an array to &apos;catch&apos; a count of such<br/>intervals in a given n-tet, so that if an n-tet has the same index for<br/>say, 7/6 and 6/5, it won&apos;t count twice---otherwise, that would<br/>artificially give an &apos;advantage&apos; to those tunings that do a lot of<br/>comma tempering of that sort.</p><p>&gt; &gt; Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins!<br/>&gt;<br/>&gt; Here&apos;s the sorted ranking using another system, which you might want<br/>&gt; to consider for comparison; it ranks 5-99 according to the unweighted<br/>&gt; maximum error of the 7-limit tonality diamond, adjusted to be logflat,</p><p>It sound like we had the same idea, except I didn&apos;t make mine<br/>&apos;logflat&apos;. How would I do that? Would I have to know the error of<br/>1-tet beforehand, perhaps?</p><p>&gt; so that there are an infinity of ets scoring less than 1, and the size<br/>&gt; of such ets grows roughly exponentially.</p><p>Gene, this is interesting--however, how did you define the &apos;penalty&apos;<br/>for the size of the n-tet? I&apos;m not clear on this...can you walk me<br/>through it?</p><p>Better yet, could you post a snippet of Python code which could<br/>explain it as well? ;)</p><p>Best,<br/>Aaron.</p><p>P.S. Shall I upload my code to the &apos;files&apos; section?</p></div><h3><a id=14318 href="#14318">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 7:16:03 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; Hi Aaron!<br/>&gt;<br/>&gt; This is what we might call a type of &quot;badness&quot; calculation, or<br/>&gt; rather, the inverse of one.  Actually, since you&apos;ve defined<br/>&gt; efficiency as going up when Eff goes down, it&apos;s not the inverse.<br/>&gt; The general approach is<br/>&gt;<br/>&gt; badness = complexity * error</p><p>Yup....I hit on the same idea....I knew I was a genius! ;)</p><p>&gt; Complexity can be number of notes, but this gets tricky in linear<br/>&gt; and planar temperaments where the number of notes is not fixed.</p><p>That&apos;s not an issue if you are just interested in n-tets that<br/>*real men* would use---the EDO&apos;s. We could call my measure the &quot;real<br/>man&quot; measure, or the &quot;stud measure&quot;, take your pick.....</p><p>(No doubt, someone will chime in about why linear or planar<br/>temperaments are more for real men)</p><p>&gt; Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>&gt; forgetting have proposed various complexity measures.  Usually<br/>&gt; they wind up being fairly similar to one another.</p><p>Mine is as good as any, I suppose. They probably all show different<br/>facets of the same phenomenon. I just thought this: we might do a<br/>recursive function, and take the last best &quot;efficiency&quot;, and have the<br/>next one usurp the best ranked ET iff it can prove that its error<br/>decrease over the last best ET has a greater factor than it&apos;s ratio of<br/>note increase over the last. But maybe this is equivalent to some<br/>easier measure that wouldn&apos;t involve recursion....</p><p>In fact, my first script used the size of n-tet directly in its<br/>equation, but I found it didn&apos;t capture as precisely my instincts--it<br/>didn&apos;t penalize the growth of the n-tet as much as I intuitively<br/>wanted, I guess.</p><p>&gt; Error also has variants.  Weighted error is popular; the deviation<br/>&gt; from JI is weighted depending on the harmonic limit of the interval<br/>&gt; in question.  There&apos;s weighted complexity, too, for that matter.</p><p>I used simple linear error in cents, for each interval in the 7 limit<br/>diamond. I thought about this, and concluded simply that although the<br/>higher harmonics are more sensitive to error, they also tend to be<br/>lower in amplitude. This justifies a simple unweighted error factor if<br/>you assume they cancel each other out, as I do. Furthermore, cents<br/>measure is more in line with our perception of pitch distance than<br/>say, error in Hertz.</p><p>&gt; Probably thousands of messages have been written on this topic,<br/>&gt; with little consensus to this day.  :(</p><p>I don&apos;t see why anyone would conclude anything other than what I said,<br/>but that&apos;s probably only because I&apos;ve thought so long about this, and<br/>come to my own conclusions that I&apos;m comfortable with, perhaps....</p><p>-Aaron.</p></div><h3><a id=14319 href="#14319">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 7:55:48 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene,</p><p>Below you chose unweigted maximum error. I respectfully submit that<br/>total sum error is a better measure for my (our) purposes. (unless I&apos;m<br/>wrong, see below)...I reason that if a particular interval error is<br/>bad in a given EDO, it may have superlative representations of other<br/>intervals; however the maximum error alone wouldn&apos;t reflect this, and<br/>the EDO by your accounting would suffer in its ranking. Furthermore,<br/>the maximum error may be close to the other errors, and may make a<br/>poor EDO appear better than one that has an passable maximum error in<br/>a given interval, but several smaller very good ones (witness<br/>31-equal, with its passable 3/2, but excellent 5/4 and 7/4)</p><p>The sum, by contrast, I think would give us an overall sense of the<br/>average sound of the EDO in relation to JI.</p><p>Unless, of course, somehow, I&apos;ve misunderstood you, and maximum error<br/>means &quot;sum of all errors&quot;. Unless you can clarify, my instincts (which<br/>of course may be wrong) tell me that maximum wouldn&apos;t capture the<br/>overall utility of a given EDO at JI approximation?</p><p>Best,<br/>Aaron.</p><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:</p><p>&gt; Here&apos;s the sorted ranking using another system, which you might want<br/>&gt; to consider for comparison; it ranks 5-99 according to the<br/>&gt; unweighted<br/>&gt; maximum error of the 7-limit tonality diamond, adjusted to be<br/>&gt; logflat,<br/>&gt; so that there are an infinity of ets scoring less than 1, and the<br/>&gt; size<br/>&gt; of such ets grows roughly exponentially.<br/>&gt; 31 .483982<br/>&gt; 99 .597311<br/>&gt; 5 .730389<br/>&gt; 41 .743399<br/>&gt; 72 .743933<br/>&gt; 12 .758467<br/>&gt; 9 .767221<br/>&gt; 10 .796400<br/>&gt; 15 .828367</p><p>etc, etc...</p></div><h3><a id=14320 href="#14320">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 8:07:35 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Correction:</p><p>I hadn&apos;t used *all* the 14 intervals of the 7-limit diamond in my<br/>script as of my post below. (there are 14, aren&apos;t there? 8/7,9/7,10/7,<br/> 12/7,6/5,7/5,8/5,9/5,4/3,5/3,7/6,3/2,5/4,7/4)</p><p>With the correction, and all else being equal, my script now ranks the<br/>EDO&apos;s 5-99 thusly (31 is still king, but 99 and 15!!!--who&apos;da thunk it<br/>--maybe that&apos;s why we want *some* weighting?--are &apos;better&apos; than 12,<br/>and the EDO equivalent of George Bush&apos;s wasteful economic policies is<br/>now 96-EDO):</p><p>((Incidentally, I&apos;m sure you guys have talked about 171-equal, which<br/>fares best when you look at 5-1024 EDO, it looks like the most<br/>practical way to do &apos;virtual JI&apos;, as I&apos;m sure Gene would agree))</p><p>total_error     efficiency</p><p>99:  11.2555    31:  69.4797<br/>72:  21.3796    99:  74.4596<br/>68:  31.5781    15:  79.9373<br/>94:  33.7436    12:  91.6392<br/>84:  34.4581    72:  97.0306<br/>87:  35.4273    27: 103.4966<br/>93:  40.7937     7: 108.7439<br/>53:  43.1694    19: 114.5954<br/>77:  43.8469    41: 115.3822<br/>95:  44.7669    21: 118.7218<br/>90:  45.3493    22: 125.7986<br/>89:  45.4157    14: 125.8967<br/>98:  45.7160     9: 128.6322<br/>80:  46.9049    53: 132.8288<br/>91:  47.2159    68: 133.5995<br/>62:  47.2453     6: 134.4032<br/>78:  48.0401    26: 147.9777<br/>31:  50.1798    10: 150.0970<br/>97:  51.5543    46: 152.8003<br/>92:  51.8844    16: 155.9879<br/>58:  52.3187    25: 159.4998<br/>88:  52.4807    37: 162.8902<br/>41:  53.5703    23: 167.1498<br/>82:  53.5703    18: 173.1011<br/>83:  53.5744    11: 174.1613<br/>81:  54.7269    62: 178.0784<br/>96:  54.9011    13: 178.4491<br/>85:  55.3423     5: 179.3963<br/>86:  57.5989    58: 181.1032<br/>73:  59.5743    84: 188.1942<br/>76:  60.0364     8: 198.2868<br/>79:  60.0584    35: 199.5827<br/>46:  60.1940    87: 201.6631<br/>74:  63.9653    17: 206.7396<br/>67:  64.7421    36: 206.7867<br/>63:  64.8899    94: 210.2485<br/>57:  65.2995    33: 212.8132<br/>66:  65.6186    77: 215.8617<br/>75:  68.6434    57: 221.0137<br/>65:  69.2027    50: 223.8791<br/>64:  72.3848    45: 224.9345<br/>60:  72.9025    32: 237.2267<br/>71:  74.2971    43: 237.2873<br/>56:  74.5608    47: 237.3757<br/>69:  78.1644    20: 237.5989<br/>70:  78.5261    38: 237.6167<br/>50:  78.6602    78: 240.2004<br/>52:  80.8627    28: 241.3798<br/>61:  80.9260    80: 241.7408<br/>59:  82.1855    52: 242.5880<br/>54:  85.8033    56: 246.6241<br/>37:  88.2322    34: 247.9334<br/>47:  90.7613    40: 249.5714<br/>45:  91.3796    63: 249.5765<br/>49:  91.7888    93: 251.0380<br/>51:  92.7914    39: 252.4136<br/>27:  96.1040    49: 254.1843<br/>55:  98.3458    42: 256.8767<br/>48: 102.4634    60: 263.5705<br/>43: 102.8245    89: 265.5073<br/>42: 115.1516    30: 266.2317<br/>36: 116.8794    66: 267.5218<br/>35: 117.9352    90: 268.6075<br/>40: 120.1640    67: 268.9287<br/>38: 123.5607    54: 270.6104<br/>44: 125.2905    51: 271.2365<br/>39: 126.2068    73: 274.9584<br/>26: 126.8380    48: 275.8631<br/>34: 135.2364    65: 276.8109<br/>33: 138.3286    95: 282.3760<br/>32: 142.3360    91: 283.2956<br/>22: 150.9583    29: 283.6773<br/>19: 157.5687    64: 283.9710<br/>25: 172.7914    82: 284.3346<br/>29: 173.3583    81: 286.2637<br/>30: 177.4878    24: 287.4992<br/>28: 181.0349    83: 288.4776<br/>21: 192.9229    59: 290.8102<br/>24: 205.3566    76: 290.9454<br/>23: 217.2948    44: 298.7697<br/>15: 219.8276    61: 298.8035<br/>20: 237.5989    98: 298.9121<br/>16: 259.9798    74: 300.1450<br/>18: 272.0160    88: 302.7732<br/>12: 274.9175    79: 304.9117<br/>17: 295.3423    85: 306.5113<br/>14: 314.7417    92: 315.2975<br/>10: 350.2263    55: 317.7327<br/>13: 401.5104    86: 323.4397<br/> 9: 450.2128    75: 327.3763<br/>11: 464.4302    71: 331.4796<br/> 8: 594.8604    97: 333.1202<br/> 7: 652.4634    69: 336.7080<br/> 6: 672.0160    70: 344.3067<br/> 5: 717.5853    96: 350.5227</p><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt;<br/>&gt; Hey all,<br/>&gt;<br/>&gt; I wrote a Python script to rank the octave-ET&apos;s according to 7-limit<br/>&gt; JI-approximation efficiency. By this, I mean the following equation:<br/>&gt;<br/>&gt; Eff = TE*W<br/>&gt;<br/>&gt; where Eff is &quot;efficiency&quot; (the lower the number, the more efficient<br/>&gt; the n-tet is at approximating n-limit JI with fewer notes), TE is the<br/>&gt; total error in cents from JI of the intervals one is interested in<br/>&gt; looking at (for me, I chose the 7 limit intervals plus their<br/>&gt; inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>&gt; ratio of dissonant intervals to consonant intervals in the ET, which<br/>&gt; obviously grows as the ET size grows. Thus, it is a &apos;penalty&apos; for an<br/>&gt; ET to get to large in this ranking, unless of course, it really<br/>&gt; significantly lowers the JI-approximation error.<br/>&gt;<br/>&gt; One motivation for the ranking might be the question--What n-tet gives<br/>&gt; me the most consonance for the buck (the buck being the size of the<br/>&gt; n-tet).<br/>&gt;<br/>&gt; Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins! (no surprise there)<br/>&gt; And, perhaps we get another insight into why 12 will always remain an<br/>&gt; important system: as we all know, but don&apos;t often admit, it does a<br/>&gt; fair job of &apos;expressing&apos; a fair amount of important intervals with<br/>&gt; only a handful of tones.....we also see that the equivalent of<br/>&gt; Republican wasteful spending, i.e. the George Bush of n-tets, is<br/>97-equal.<br/>&gt;<br/>&gt; The left column is &apos;total error in cents&apos; and the right column is our<br/>&gt; ranking of interest, &apos;efficiency&apos;:<br/>&gt;<br/>&gt; total_error     efficiency<br/>&gt;<br/>&gt; 99:  11.9495    31:  66.8946<br/>&gt; 72:  22.2008    12:  76.6857<br/>&gt; 94:  29.0217    99:  79.0507<br/>&gt; 68:  31.7373    41:  96.4323<br/>&gt; 87:  32.0118    72: 100.7573<br/>&gt; 84:  32.4549     7: 102.3707<br/>&gt; 95:  37.2044    19: 104.3473<br/>&gt; 53:  37.2751    15: 112.3440<br/>&gt; 77:  37.9581    53: 114.6928<br/>&gt; 89:  38.7576    14: 124.1117<br/>&gt; 90:  40.4889     6: 125.4311<br/>&gt; 93:  40.4928    27: 126.0480<br/>&gt; 80:  44.4171    22: 133.4705<br/>&gt; 41:  44.7721    68: 134.2733<br/>&gt; 82:  44.7721    10: 137.0975<br/>&gt; 91:  44.9235    18: 144.5536<br/>&gt; 62:  46.9444    37: 149.9255<br/>&gt; 98:  47.2205    21: 153.0049<br/>&gt; 88:  47.3411    25: 153.4885<br/>&gt; 96:  47.5408    46: 155.2680<br/>&gt; 81:  47.7354    23: 159.2434<br/>&gt; 58:  47.9142    11: 161.2244<br/>&gt; 31:  48.3128    13: 165.7086<br/>&gt; 76:  48.6515    58: 165.8568<br/>&gt; 78:  49.3335    26: 167.4354<br/>&gt; 83:  49.3755    35: 169.1267<br/>&gt; 97:  50.4854     5: 171.8133<br/>&gt; 92:  52.8566    62: 176.9442<br/>&gt; 85:  55.1878    84: 177.2534<br/>&gt; 86:  55.9476    17: 178.0986<br/>&gt; 65:  58.9982    94: 180.8277<br/>&gt; 75:  59.1556    36: 182.0417<br/>&gt; 63:  59.8286    87: 182.2212<br/>&gt; 46:  61.1662     8: 182.5136<br/>&gt; 74:  62.6937    77: 186.8706<br/>&gt; 79:  63.0488    16: 188.6049<br/>&gt; 64:  63.4544    50: 205.3443<br/>&gt; 73:  63.6051    43: 208.8975<br/>&gt; 66:  63.7535     9: 214.2032<br/>&gt; 57:  64.4410    45: 215.9116<br/>&gt; 60:  65.5829    49: 216.1089<br/>&gt; 70:  65.8457    57: 218.1080<br/>&gt; 71:  66.4328    34: 222.3343<br/>&gt; 67:  66.6762    47: 225.0262<br/>&gt; 69:  69.3722    89: 226.5829<br/>&gt; 56:  69.9429    80: 228.9191<br/>&gt; 50:  72.1480    63: 230.1098<br/>&gt; 59:  74.6132    56: 231.3495<br/>&gt; 54:  76.1355    38: 232.3184<br/>&gt; 61:  77.8686    30: 232.4634<br/>&gt; 49:  78.0393    29: 232.9472<br/>&gt; 37:  81.2097    95: 234.6738<br/>&gt; 52:  82.0285    42: 235.3100<br/>&gt; 47:  86.0394    76: 235.7728<br/>&gt; 45:  87.7141    65: 235.9929<br/>&gt; 55:  89.2080    60: 237.1073<br/>&gt; 43:  90.5223    82: 237.6367<br/>&gt; 51:  90.6945    40: 238.8968<br/>&gt; 48:  92.7956    90: 239.8191<br/>&gt; 35:  99.9385    54: 240.1196<br/>&gt; 27: 100.8384    39: 240.9800<br/>&gt; 36: 102.8932    24: 241.9821<br/>&gt; 42: 105.4838    33: 243.2895<br/>&gt; 40: 115.0244    32: 245.7927<br/>&gt; 44: 116.7938    52: 246.0854<br/>&gt; 39: 120.4900    78: 246.6677<br/>&gt; 38: 120.8056    64: 248.9364<br/>&gt; 34: 121.2733    93: 249.1862<br/>&gt; 22: 133.4705    81: 249.6930<br/>&gt; 33: 139.0226    48: 249.8344<br/>&gt; 29: 142.3567    20: 250.2792<br/>&gt; 19: 143.4776    28: 252.7545<br/>&gt; 26: 143.5161    66: 259.9180<br/>&gt; 32: 147.4756    59: 264.0161<br/>&gt; 30: 154.9756    51: 265.1071<br/>&gt; 28: 163.5470    83: 265.8680<br/>&gt; 25: 166.2792    91: 269.5412<br/>&gt; 24: 172.8444    88: 273.1216<br/>&gt; 23: 173.7200    67: 276.9627<br/>&gt; 21: 204.0065    44: 278.5083<br/>&gt; 15: 224.6880    75: 282.1268<br/>&gt; 18: 227.1556    61: 287.5149<br/>&gt; 12: 230.0571    55: 288.2105<br/>&gt; 16: 242.4920    70: 288.7081<br/>&gt; 20: 250.2792    73: 293.5622<br/>&gt; 17: 254.4266    74: 294.1783<br/>&gt; 14: 310.2792    71: 296.3927<br/>&gt; 10: 319.8941    69: 298.8341<br/>&gt; 13: 372.8444    96: 303.5295<br/>&gt;  9: 428.4065    85: 305.6556<br/>&gt; 11: 429.9317    98: 308.7495<br/>&gt;  8: 547.5408    86: 314.1673<br/>&gt;  7: 614.2242    79: 320.0938<br/>&gt;  6: 627.1556    92: 321.2052<br/>&gt;  5: 687.2532    97: 326.2135<br/>&gt;<br/>&gt; -Aaron.<br/>&gt;</p></div><h3><a id=14322 href="#14322">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 9:31:24 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:</p><p>&gt; A consonance would be anything in said diamond. So, in a given n-tet,<br/>&gt; it would be that n-tet&apos;s approximation of that interval.</p><p>This doesn&apos;t seem to enforce consistency, and therefore would tend to<br/>give too high a score to inconsistent systems.</p><p>&gt; It sound like we had the same idea, except I didn&apos;t make mine<br/>&gt; &apos;logflat&apos;. How would I do that? Would I have to know the error of<br/>&gt; 1-tet beforehand, perhaps?</p><p>I used a little multiple Diophantine approximation theory. Take any<br/>badness measure of of the type you are using, which is<br/>error*complexity. Since we are looking at n-ets, complexity is just n,<br/>and so we have n*error, or a relative error measure. On average, such<br/>a badness measure applied to various n will give a fixed percentage of<br/>the n less than the cutoff score, with actual error O(1/n). Now we<br/>penalize the larger n a smidgen more, in order to make the percentage<br/>passing a badness score continually drop, but not so fast we only get<br/>a finite list passing the requirement. If we are in a m-odd limit, and<br/>the number of odd primes less than or equal to m is d, it turns out<br/>that multiplying relative error, or n*error, by n^(1/d), and using<br/>this as a badness figure will give us an infinite list so long as the<br/>cutoff is not set too low; this is by Diophantine approximation theory.</p><p>&gt; P.S. Shall I upload my code to the &apos;files&apos; section?</p><p>If you like, but first I&apos;d address the question of inconsistency. What<br/>you really want to measure the badness of is a val, that is, a map<br/>which tells you, for each prime number under the limit, how many et<br/>steps that prime is mapped to. From the val, you can compute how many<br/>et steps any interval under the limit should get; hence you can figure<br/>out what the error is, computed in a consistent way.</p></div><h3><a id=14323 href="#14323">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 9:34:22 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt;<br/>&gt;<br/>&gt; Gene,<br/>&gt;<br/>&gt; Below you chose unweigted maximum error. I respectfully submit that<br/>&gt; total sum error is a better measure for my (our) purposes.</p><p>You can certainly argue the point, for the reasons you give. A<br/>compromise is to take rms error.</p></div><h3><a id=14324 href="#14324">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 9:41:48 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt;<br/>&gt; &gt; A consonance would be anything in said diamond. So, in a given n-tet,<br/>&gt; &gt; it would be that n-tet&apos;s approximation of that interval.<br/>&gt;<br/>&gt; This doesn&apos;t seem to enforce consistency, and therefore would tend to<br/>&gt; give too high a score to inconsistent systems.</p><p>I don&apos;t understand, for musical purposes, why we just wouldn&apos;t want to<br/>take some measure of the audible error for each interval we care to<br/>look at?</p><p>&gt; &gt; It sound like we had the same idea, except I didn&apos;t make mine<br/>&gt; &gt; &apos;logflat&apos;. How would I do that? Would I have to know the error of<br/>&gt; &gt; 1-tet beforehand, perhaps?<br/>&gt;<br/>&gt; I used a little multiple Diophantine approximation theory. Take any<br/>&gt; badness measure of of the type you are using, which is<br/>&gt; error*complexity. Since we are looking at n-ets, complexity is just n,<br/>&gt; and so we have n*error, or a relative error measure. On average, such<br/>&gt; a badness measure applied to various n will give a fixed percentage of<br/>&gt; the n less than the cutoff score, with actual error O(1/n). Now we<br/>&gt; penalize the larger n a smidgen more, in order to make the percentage<br/>&gt; passing a badness score continually drop, but not so fast we only get<br/>&gt; a finite list passing the requirement. If we are in a m-odd limit, and<br/>&gt; the number of odd primes less than or equal to m is d, it turns out<br/>&gt; that multiplying relative error, or n*error, by n^(1/d), and using<br/>&gt; this as a badness figure will give us an infinite list so long as the<br/>&gt; cutoff is not set too low; this is by Diophantine approximation theory.</p><p>Hmm...I&apos;ll have to grok this further when I have time-I only half<br/>understand.</p><p>&gt; &gt; P.S. Shall I upload my code to the &apos;files&apos; section?<br/>&gt;<br/>&gt; If you like, but first I&apos;d address the question of inconsistency. What<br/>&gt; you really want to measure the badness of is a val, that is, a map<br/>&gt; which tells you, for each prime number under the limit, how many et<br/>&gt; steps that prime is mapped to. From the val, you can compute how many<br/>&gt; et steps any interval under the limit should get; hence you can figure<br/>&gt; out what the error is, computed in a consistent way.</p><p>So let me understand....do we only have to measure the error of 3/1,<br/>5/1, and 7/1? (as opposed to any octave reduced diamond interval?)</p><p>-Aaron.</p></div><h3><a id=14325 href="#14325">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 9:41:17 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt;<br/>&gt; Correction:<br/>&gt;<br/>&gt; I hadn&apos;t used *all* the 14 intervals of the 7-limit diamond in my<br/>&gt; script as of my post below. (there are 14, aren&apos;t there? 8/7,9/7,10/7,<br/>&gt;  12/7,6/5,7/5,8/5,9/5,4/3,5/3,7/6,3/2,5/4,7/4)</p><p>Actually, 9/7 and 9/5 are 9-limit intervals. Take those two out and<br/>you have the 12 intervals of the 7-limit diamond; add 10/9, 9/8, 14/9<br/>and 16/9 and you have the 18 elements of the 9-limit diamond. Of<br/>course, you can simply use the half of these between 1 and sqrt(2) to<br/>the same effect as using the whole diamond.</p></div><h3><a id=14326 href="#14326">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 9:45:21 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Correction:<br/>&gt; &gt;<br/>&gt; &gt; I hadn&apos;t used *all* the 14 intervals of the 7-limit diamond in my<br/>&gt; &gt; script as of my post below. (there are 14, aren&apos;t there? 8/7,9/7,10/7,<br/>&gt; &gt;  12/7,6/5,7/5,8/5,9/5,4/3,5/3,7/6,3/2,5/4,7/4)<br/>&gt;<br/>&gt; Actually, 9/7 and 9/5 are 9-limit intervals. Take those two out and<br/>&gt; you have the 12 intervals of the 7-limit diamond; add 10/9, 9/8, 14/9<br/>&gt; and 16/9 and you have the 18 elements of the 9-limit diamond. Of<br/>&gt; course, you can simply use the half of these between 1 and sqrt(2) to<br/>&gt; the same effect as using the whole diamond.</p><p>Cool....of course! And that sqrt(2) trick is neat...the same kind of<br/>shortcut used in prime number finding algorithms.</p><p>-Aaron.</p></div><h3><a id=14328 href="#14328">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 9:47:18 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; Gene,<br/>&gt; &gt;<br/>&gt; &gt; Below you chose unweigted maximum error. I respectfully submit that<br/>&gt; &gt; total sum error is a better measure for my (our) purposes.<br/>&gt;<br/>&gt; You can certainly argue the point, for the reasons you give. A<br/>&gt; compromise is to take rms error.</p><p>Yes....but it turns out that I have used rms error rankings before,<br/>and the ranking, if I remember correctly, remains *exactly* the same<br/>as linear error sum.</p><p>-Aaron.</p></div><h3><a id=14330 href="#14330">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 10:01:20 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&gt; &lt;genewardsmith@&gt; wrote:</p><p>&gt; &gt; This doesn&apos;t seem to enforce consistency, and therefore would tend to<br/>&gt; &gt; give too high a score to inconsistent systems.<br/>&gt;<br/>&gt; I don&apos;t understand, for musical purposes, why we just wouldn&apos;t want to<br/>&gt; take some measure of the audible error for each interval we care to<br/>&gt; look at?</p><p>One reason is that we are interested in chords, not just intervals.<br/>Conisder evaluating 11-edo. If we take 4 steps for a major third and 6<br/>steps for a fifth, then the minor third between the major third and<br/>the fifth is 2 steps, which is 218 cents. You are scoring it as<br/>327 cents, from 3 steps, however. Hence, you end up giving 11 too high<br/>a score. Of course you can use other mappings, but you get the same<br/>problem.</p><p>&gt; So let me understand....do we only have to measure the error of 3/1,<br/>&gt; 5/1, and 7/1? (as opposed to any octave reduced diamond interval?)</p><p>What I&apos;m suggesting is that you figure out what 3, 5 and 7 are in<br/>terms of number of steps; then for instance 5/3 will be &quot;number of 5<br/>steps&quot; minus &quot;number of 3 steps&quot;, and you use *that* as the number of<br/>steps in computing the error for 5/3.</p></div><h3><a id=14332 href="#14332">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 10:34:25 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&gt; &gt; &lt;genewardsmith@&gt; wrote:<br/>&gt;<br/>&gt; &gt; &gt; This doesn&apos;t seem to enforce consistency, and therefore would<br/>tend to<br/>&gt; &gt; &gt; give too high a score to inconsistent systems.<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t understand, for musical purposes, why we just wouldn&apos;t want to<br/>&gt; &gt; take some measure of the audible error for each interval we care to<br/>&gt; &gt; look at?<br/>&gt;<br/>&gt; One reason is that we are interested in chords, not just intervals.<br/>&gt; Conisder evaluating 11-edo. If we take 4 steps for a major third and 6<br/>&gt; steps for a fifth, then the minor third between the major third and<br/>&gt; the fifth is 2 steps, which is 218 cents. You are scoring it as<br/>&gt; 327 cents, from 3 steps, however. Hence, you end up giving 11 too high<br/>&gt; a score. Of course you can use other mappings, but you get the same<br/>&gt; problem.<br/>&gt;<br/>&gt; &gt; So let me understand....do we only have to measure the error of 3/1,<br/>&gt; &gt; 5/1, and 7/1? (as opposed to any octave reduced diamond interval?)<br/>&gt;<br/>&gt; What I&apos;m suggesting is that you figure out what 3, 5 and 7 are in<br/>&gt; terms of number of steps; then for instance 5/3 will be &quot;number of 5<br/>&gt; steps&quot; minus &quot;number of 3 steps&quot;, and you use *that* as the number of<br/>&gt; steps in computing the error for 5/3.</p><p>Good...got it.</p><p>-Aaron.</p></div><h3><a id=14333 href="#14333">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 11:43:09 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;I should have noted that I set up an array to &apos;catch&apos; a count of such<br/>&gt;intervals in a given n-tet, so that if an n-tet has the same index for<br/>&gt;say, 7/6 and 6/5, it won&apos;t count twice---otherwise, that would<br/>&gt;artificially give an &apos;advantage&apos; to those tunings that do a lot of<br/>&gt;comma tempering of that sort.</p><p>Some people wouldn&apos;t like this... the very point of temperament is<br/>to use an interval in more than one sense.</p><p>-Carl</p></div><h3><a id=14334 href="#14334">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 12:06:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Complexity can be number of notes, but this gets tricky in linear<br/>&gt;&gt; and planar temperaments where the number of notes is not fixed.<br/>&gt;<br/>&gt;That&apos;s not an issue if you are just interested in n-tets that<br/>&gt;*real men* would use---the EDO&apos;s. We could call my measure the &quot;real<br/>&gt;man&quot; measure, or the &quot;stud measure&quot;, take your pick.....<br/>&gt;<br/>&gt;(No doubt, someone will chime in about why linear or planar<br/>&gt;temperaments are more for real men)</p><p>That would be me... of course, you can always tune them in an<br/>n-tet, but your calculation wouldn&apos;t be the relevant one.  Actually,<br/>to get &apos;notes&apos; for linear temperaments, someone did suggest &apos;size<br/>of the smallest MOS&apos;, but I&apos;m not sure that&apos;s so relevant either.</p><p>Too bad Graham took off just as you showed up.  He&apos;s been writing<br/>python code left and right, and none of us can read it.</p><p>&gt;&gt; Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>&gt;&gt; forgetting have proposed various complexity measures.  Usually<br/>&gt;&gt; they wind up being fairly similar to one another.<br/>&gt;<br/>&gt;Mine is as good as any, I suppose. They probably all show different<br/>&gt;facets of the same phenomenon. I just thought this: we might do a<br/>&gt;recursive function, and take the last best &quot;efficiency&quot;, and have the<br/>&gt;next one usurp the best ranked ET iff it can prove that its error<br/>&gt;decrease over the last best ET has a greater factor than it&apos;s ratio<br/>&gt;of note increase over the last. But maybe this is equivalent to some<br/>&gt;easier measure that wouldn&apos;t involve recursion....</p><p>&quot;Consistency&quot; is a form of badness, and Paul Hahn has charts that<br/>show an ET only if it beats all lower ETs...</p><p><a href="http://library.wustl.edu/~manynote/music.html">http://library.wustl.edu/~manynote/music.html</a></p><p>&gt;In fact, my first script used the size of n-tet directly in its<br/>&gt;equation, but I found it didn&apos;t capture as precisely my instincts--it<br/>&gt;didn&apos;t penalize the growth of the n-tet as much as I intuitively<br/>&gt;wanted, I guess.</p><p>Did you try raising it to a power?  With your harmonic waste, all<br/>you&apos;re doing is subtracting a fixed number, right?  So this will<br/>become the same as notes for big ets.</p><p>&gt;&gt; Error also has variants.  Weighted error is popular; the deviation<br/>&gt;&gt; from JI is weighted depending on the harmonic limit of the interval<br/>&gt;&gt; in question.  There&apos;s weighted complexity, too, for that matter.<br/>&gt;<br/>&gt;I used simple linear error in cents, for each interval in the 7 limit<br/>&gt;diamond. I thought about this, and concluded simply that although the<br/>&gt;higher harmonics are more sensitive to error, they also tend to be<br/>&gt;lower in amplitude. This justifies a simple unweighted error factor if<br/>&gt;you assume they cancel each other out, as I do.</p><p>Higher harmonics might require more accuracy to distinguish because<br/>they&apos;re closer together and/or because the brain has to work harder<br/>to pick them out.  But, mistuning them a given amount seems to be less<br/>*painful* than mistuning lower harmonics the same amount.  For example,<br/>a 2:1 - 5cents is worse, to my ear, than 3:2 - 5cents.  And in 12-tET,<br/>the octave, 5th, major 3rd, and dom. 7th get progressively worse, and<br/>it seems to work.  This is one way of thinking about the weighting of<br/>Paul&apos;s TOP temperaments.</p><p>&gt;Furthermore, cents measure is more in line with our perception of<br/>&gt;pitch distance than say, error in Hertz.</p><p>That may not be true, actually, but it&apos;s a necessary abstraction<br/>to get results that don&apos;t change depending on the concert pitch.</p><p>-Carl</p></div><h3><a id=14335 href="#14335">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 12:14:53 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Of course, you can simply use the half of these between 1 and sqrt(2)<br/>&gt;to the same effect as using the whole diamond.</p><p>Assuming an octave-equivalent tuning...</p><p>-Carl</p></div><h3><a id=14336 href="#14336">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 1:00:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;Of course, you can simply use the half of these between 1 and sqrt(2)<br/>&gt; &gt;to the same effect as using the whole diamond.<br/>&gt;<br/>&gt; Assuming an octave-equivalent tuning...</p><p>Which, given we are talking about the tonality diamond, we have<br/>already assumed.</p></div><h3><a id=14337 href="#14337">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 2:37:48 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:</p><p>&gt; Below you chose unweigted maximum error. I respectfully submit that<br/>&gt; total sum error is a better measure for my (our) purposes.</p><p>Here are 5 to 99 (rounded vals) sorted according to an unweighted<br/>average error loglat badness. One can vary this proceedure in all<br/>sorts of ways; one of the most significant would be to look at the<br/>9-limit instead, where 31 will not do as well. We can also use systems<br/>such as TOP error, where there is no difference between the 7 and 9<br/>limits, and could look at more vals (for instance, all of what I&apos;ve<br/>called &quot;semistandard&quot; vals, etc.)</p><p>31: .256652<br/>99: .311647<br/>72: .380463<br/>5: .386623<br/>10: .424609<br/>41: .430062<br/>12: .431456<br/>19: .454330<br/>68: .460325<br/>27: .464210<br/>6: .471880<br/>9: .475364<br/>15: .484503<br/>22: .510277<br/>53: .513451<br/>7: .606008<br/>37: .615556<br/>16: .619887<br/>46: .645323<br/>62: .646723<br/>26: .664455<br/>58: .700468<br/>84: .729241<br/>18: .731336<br/>21: .743399<br/>50: .770551<br/>87: .777033<br/>35: .780217<br/>36: .817088<br/>77: .833641<br/>25: .841939<br/>94: .851137<br/>43: .857439<br/>29: .862325<br/>45: .870225<br/>49: .872220<br/>57: .880881<br/>56: .886261<br/>80: .917481<br/>34: .938662<br/>63: .943791<br/>24: .956611<br/>17: .985173<br/>8: .990181<br/>60: 1.006005<br/>32: 1.012412<br/>89: 1.023616<br/>90: 1.024532<br/>42: 1.029797<br/>14: 1.035317<br/>11: 1.037572<br/>78: 1.051263<br/>76: 1.057390<br/>38: 1.058976<br/>40: 1.059231<br/>95: 1.065617<br/>47: 1.066582<br/>20: 1.069948<br/>81: 1.076663<br/>82: 1.083687<br/>91: 1.103317<br/>93: 1.110469<br/>13: 1.120164<br/>65: 1.126335<br/>28: 1.180739<br/>59: 1.181819<br/>51: 1.193396<br/>83: 1.203700<br/>73: 1.206710<br/>30: 1.220871<br/>74: 1.263842<br/>88: 1.269551<br/>52: 1.275842<br/>33: 1.281781<br/>70: 1.283007<br/>44: 1.285817<br/>48: 1.318464<br/>67: 1.328597<br/>75: 1.340198<br/>61: 1.410839<br/>55: 1.437747<br/>23: 1.450026<br/>97: 1.488086<br/>69: 1.488737<br/>66: 1.536314<br/>79: 1.548907<br/>96: 1.561303<br/>92: 1.626112<br/>39: 1.685094<br/>71: 1.734099<br/>54: 1.758384<br/>85: 1.827869<br/>86: 1.873669<br/>98: 1.884821</p></div><h3><a id=14338 href="#14338">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>2/9/2006 3:06:34 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>How about this:</p><p>Any EDO will have two different intervals that approximate a just<br/>interval. If it&apos;s a good EDO one will be a lot better than the other.<br/>Define the ratio between the errors on either side to be the<br/>&quot;ambiguity&quot;, and sort the temperaments by the maximum ambiguity of any<br/>ratio in the odd limit.</p><p>The least ambiguous 5-limit EDOs &lt;100:</p><p>53 0.0663129681303<br/>65 0.107709678321<br/>34 0.125207429082<br/>87 0.131230283612<br/>19 0.132033355179<br/>99 0.148261987296<br/>84 0.158547263143<br/>31 0.182126131158<br/>12 0.185414006976<br/>72 0.21776400859<br/>46 0.23656416643<br/>22 0.271040602217<br/>41 0.274842474139<br/>68 0.286256269759<br/>15 0.291259767292<br/>38 0.304236011756<br/>3 0.324700696639<br/>80 0.325824585915<br/>50 0.330008376335<br/>96 0.335653737938<br/>7 0.339578742731</p><p>The least ambiguous 7-limit EDOs &lt;100:</p><p>99 0.148261987296<br/>31 0.182126131158<br/>72 0.21776400859<br/>41 0.274842474139<br/>68 0.286256269759<br/>53 0.374340275394<br/>27 0.444964422042<br/>62 0.44536483704<br/>84 0.468507722095<br/>22 0.471908195495<br/>58 0.488472886111<br/>46 0.49172254311<br/>12 0.495418659238<br/>15 0.505766302044<br/>19 0.514562944734</p><p>The least ambiguous 9-limit EDOs &lt;100:</p><p>99 0.215694406482<br/>41 0.302298401211<br/>72 0.306506579143<br/>53 0.374340275394<br/>31 0.404305241048<br/>58 0.488472886111<br/>46 0.49172254311<br/>19 0.514562944734<br/>22 0.524548406018<br/>57 0.538228221377<br/>12 0.540454537344<br/>90 0.584015021081<br/>94 0.58699488416<br/>24 0.603895874325</p><p>The least ambiguous 11-limit EDOs &lt;100:</p><p>72 0.306506579143<br/>31 0.404305241048<br/>46 0.49172254311<br/>58 0.545514121318<br/>41 0.568272040567<br/>22 0.585151764949<br/>94 0.58699488416<br/>90 0.600812284154<br/>24 0.603895874325</p></div><h3><a id=14339 href="#14339">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 3:09:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:</p><p>&gt; Here are 5 to 99 (rounded vals) sorted according to an unweighted<br/>&gt; average error loglat badness.</p><p>Here&apos;s something else to ponder, again using logflat average error<br/>badness. If look at ets which beat out previous ets in terms of<br/>7-limit badness, you get this:</p><p>1: .400590<br/>4: .285161<br/>31: .256652<br/>171: .179643<br/>103169: .090844</p><p>This list is probably infinite, but proving it is another matter. One<br/>can prove its rate of growth for n is hyperexponential. The 103169 et<br/>is the Paul Erlich division, and beating it will probably not be easy.</p></div><h3><a id=14340 href="#14340">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 3:49:30 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Keenan Pepper &lt;keenanpepper@...&gt;<br/>wrote:<br/>&gt;<br/>&gt; How about this:<br/>&gt;<br/>&gt; Any EDO will have two different intervals that approximate a just<br/>&gt; interval. If it&apos;s a good EDO one will be a lot better than the other.<br/>&gt; Define the ratio between the errors on either side to be the<br/>&gt; &quot;ambiguity&quot;, and sort the temperaments by the maximum ambiguity of any<br/>&gt; ratio in the odd limit.</p><p>I like it; it&apos;s a nice variation on the consistency level business.</p></div><h3><a id=14341 href="#14341">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/9/2006 3:52:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:</p><p>&gt; The 103169 et<br/>&gt; is the Paul Erlich division, and beating it will probably not be easy.</p><p>In case anyone wants some 7-limit nanocommas to play with, here is its<br/>TM basis:</p><p>[&lt;9 -28 37 -18|, &lt;-92 -17 21 25|, &lt;110 -71 -11 10|]</p></div><h3><a id=14342 href="#14342">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 4:03:46 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; How about this:<br/>&gt;&gt;<br/>&gt;&gt; Any EDO will have two different intervals that approximate a just<br/>&gt;&gt; interval. If it&apos;s a good EDO one will be a lot better than the other.<br/>&gt;&gt; Define the ratio between the errors on either side to be the<br/>&gt;&gt; &quot;ambiguity&quot;, and sort the temperaments by the maximum ambiguity of any<br/>&gt;&gt; ratio in the odd limit.<br/>&gt;<br/>&gt;I like it; it&apos;s a nice variation on the consistency level business.</p><p>The only difference is Keenan isn&apos;t discarding the decimal part of<br/>the result.</p><p>-Carl</p></div><h3><a id=14344 href="#14344">ðŸ”—</a>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>2/9/2006 8:28:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 2/9/06, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; &gt;&gt; How about this:<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Any EDO will have two different intervals that approximate a just<br/>&gt; &gt;&gt; interval. If it&apos;s a good EDO one will be a lot better than the other.<br/>&gt; &gt;&gt; Define the ratio between the errors on either side to be the<br/>&gt; &gt;&gt; &quot;ambiguity&quot;, and sort the temperaments by the maximum ambiguity of any<br/>&gt; &gt;&gt; ratio in the odd limit.<br/>&gt; &gt;<br/>&gt; &gt;I like it; it&apos;s a nice variation on the consistency level business.<br/>&gt;<br/>&gt; The only difference is Keenan isn&apos;t discarding the decimal part of<br/>&gt; the result.<br/>&gt;<br/>&gt; -Carl</p><p>Ah yes, that&apos;s true. I guess I haven&apos;t really discovered anything new then.</p><p>Keenan</p></div><h3><a id=14345 href="#14345">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/9/2006 9:41:30 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Keenan,</p><p>This is interesting. But I fail to see how it has anything to do with<br/>my original concept, where the larger the EDO, the more it gets<br/>penalized....</p><p>Best,<br/>Aaron.</p><p>-- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Keenan Pepper &lt;keenanpepper@...&gt; wrote:<br/>&gt;<br/>&gt; How about this:<br/>&gt;<br/>&gt; Any EDO will have two different intervals that approximate a just<br/>&gt; interval. If it&apos;s a good EDO one will be a lot better than the other.<br/>&gt; Define the ratio between the errors on either side to be the<br/>&gt; &quot;ambiguity&quot;, and sort the temperaments by the maximum ambiguity of any<br/>&gt; ratio in the odd limit.<br/>&gt;<br/>&gt; The least ambiguous 5-limit EDOs &lt;100:<br/>&gt;<br/>&gt; 53 0.0663129681303<br/>&gt; 65 0.107709678321<br/>&gt; 34 0.125207429082<br/>&gt; 87 0.131230283612<br/>&gt; 19 0.132033355179<br/>&gt; 99 0.148261987296<br/>&gt; 84 0.158547263143<br/>&gt; 31 0.182126131158<br/>&gt; 12 0.185414006976<br/>&gt; 72 0.21776400859<br/>&gt; 46 0.23656416643<br/>&gt; 22 0.271040602217<br/>&gt; 41 0.274842474139<br/>&gt; 68 0.286256269759<br/>&gt; 15 0.291259767292<br/>&gt; 38 0.304236011756<br/>&gt; 3 0.324700696639<br/>&gt; 80 0.325824585915<br/>&gt; 50 0.330008376335<br/>&gt; 96 0.335653737938<br/>&gt; 7 0.339578742731<br/>&gt;<br/>&gt; The least ambiguous 7-limit EDOs &lt;100:<br/>&gt;<br/>&gt; 99 0.148261987296<br/>&gt; 31 0.182126131158<br/>&gt; 72 0.21776400859<br/>&gt; 41 0.274842474139<br/>&gt; 68 0.286256269759<br/>&gt; 53 0.374340275394<br/>&gt; 27 0.444964422042<br/>&gt; 62 0.44536483704<br/>&gt; 84 0.468507722095<br/>&gt; 22 0.471908195495<br/>&gt; 58 0.488472886111<br/>&gt; 46 0.49172254311<br/>&gt; 12 0.495418659238<br/>&gt; 15 0.505766302044<br/>&gt; 19 0.514562944734<br/>&gt;<br/>&gt; The least ambiguous 9-limit EDOs &lt;100:<br/>&gt;<br/>&gt; 99 0.215694406482<br/>&gt; 41 0.302298401211<br/>&gt; 72 0.306506579143<br/>&gt; 53 0.374340275394<br/>&gt; 31 0.404305241048<br/>&gt; 58 0.488472886111<br/>&gt; 46 0.49172254311<br/>&gt; 19 0.514562944734<br/>&gt; 22 0.524548406018<br/>&gt; 57 0.538228221377<br/>&gt; 12 0.540454537344<br/>&gt; 90 0.584015021081<br/>&gt; 94 0.58699488416<br/>&gt; 24 0.603895874325<br/>&gt;<br/>&gt; The least ambiguous 11-limit EDOs &lt;100:<br/>&gt;<br/>&gt; 72 0.306506579143<br/>&gt; 31 0.404305241048<br/>&gt; 46 0.49172254311<br/>&gt; 58 0.545514121318<br/>&gt; 41 0.568272040567<br/>&gt; 22 0.585151764949<br/>&gt; 94 0.58699488416<br/>&gt; 90 0.600812284154<br/>&gt; 24 0.603895874325<br/>&gt;</p></div><h3><a id=14346 href="#14346">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2006 10:51:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Keenan,<br/>&gt;<br/>&gt;This is interesting. But I fail to see how it has anything to do with<br/>&gt;my original concept, where the larger the EDO, the more it gets<br/>&gt;penalized....<br/>&gt;<br/>&gt;Best,<br/>&gt;Aaron.</p><p>As I said, what Keenan&apos;s doing is consistency, and consistency is a<br/>type of badness.  It penalizes error in terms of the size of the scale<br/>step, which goes down as notes go up, so is the same.</p><p>-Carl</p></div><h3><a id=14356 href="#14356">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/10/2006 2:39:52 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;Keenan,<br/>&gt; &gt;<br/>&gt; &gt;This is interesting. But I fail to see how it has anything to do with<br/>&gt; &gt;my original concept, where the larger the EDO, the more it gets<br/>&gt; &gt;penalized....<br/>&gt; &gt;<br/>&gt; &gt;Best,<br/>&gt; &gt;Aaron.<br/>&gt;<br/>&gt; As I said, what Keenan&apos;s doing is consistency, and consistency is a<br/>&gt; type of badness.  It penalizes error in terms of the size of the scale<br/>&gt; step, which goes down as notes go up, so is the same.</p><p>Of course I know what you mean, but you are missing my point.</p><p>Suppose you have an error of .2 and .1, the ratio being 2:1<br/>Then you have .6 and .3, also 2:1</p><p>Clearly, the first one is a larger EDO. But the ratio wouldn&apos;t tell us<br/>that, we&apos;d already have to know that.</p></div><h3><a id=14378 href="#14378">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 5:30:16 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; Hi Aaron!<br/>&gt;<br/>&gt; This is what we might call a type of &quot;badness&quot; calculation, or<br/>&gt; rather, the inverse of one.  Actually, since you&apos;ve defined<br/>&gt; efficiency as going up when Eff goes down, it&apos;s not the inverse.<br/>&gt; The general approach is<br/>&gt;<br/>&gt; badness = complexity * error</p><p>No, that&apos;s not the general approach. I don&apos;t know why you say it is!<br/>We&apos;ve seen huge number of results from Gene which use logflat badness<br/>instead (which rarely agrees with your formula above), Dave Keenan<br/>has also proposed different badness functions, and my paper<br/>implicitly uses</p><p>badness = a*complexity + b*error</p><p>The general approach, instead, is to define a badness function that<br/>is an increasing function of both complexity and error. Anything more<br/>specific is less general. :)</p><p>&gt; Complexity can be number of notes, but this gets tricky in linear<br/>&gt; and planar temperaments where the number of notes is not fixed.<br/>&gt; Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>&gt; forgetting have proposed various complexity measures.  Usually<br/>&gt; they wind up being fairly similar to one another.<br/>&gt;<br/>&gt; Error also has variants.  Weighted error is popular; the deviation<br/>&gt; from JI is weighted depending on the harmonic limit of the interval<br/>&gt; in question.  There&apos;s weighted complexity, too, for that matter.<br/>&gt;<br/>&gt; Probably thousands of messages have been written on this topic,<br/>&gt; with little consensus to this day.  :(</p><p>Actually, Graham and I have made relatively huge strides toward<br/>consensus in the last few weeks. At least, we&apos;ll have a number of<br/>different but clearly defined and clearly motivated measures.</p></div><h3><a id=14379 href="#14379">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 5:39:17 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:</p><p>&gt; &gt; Complexity can be number of notes, but this gets tricky in linear<br/>&gt; &gt; and planar temperaments where the number of notes is not fixed.<br/>&gt;<br/>&gt; That&apos;s not an issue if you are just interested in n-tets that<br/>&gt; *real men* would use---the EDO&apos;s. We could call my measure the &quot;real<br/>&gt; man&quot; measure, or the &quot;stud measure&quot;, take your pick.....<br/>&gt;<br/>&gt; (No doubt, someone will chime in about why linear or planar<br/>&gt; temperaments are more for real men)</p><p>I assume you&apos;re joking, because I know you&apos;ve used both meantone and<br/>Superpythagorean temperaments in the past.</p><p>&gt; Furthermore, cents<br/>&gt; measure is more in line with our perception of pitch distance than<br/>&gt; say, error in Hertz.</p><p>Fortunately, we don&apos;t need to worry about this, since it&apos;s impossible<br/>to specify the error of a given ratio in a given ET (or regular<br/>temperament) in Hertz.</p><p>&gt; &gt; Probably thousands of messages have been written on this topic,<br/>&gt; &gt; with little consensus to this day.  :(<br/>&gt;<br/>&gt; I don&apos;t see why anyone would conclude anything other than what I<br/>said,<br/>&gt; but that&apos;s probably only because I&apos;ve thought so long about this,<br/>and<br/>&gt; come to my own conclusions that I&apos;m comfortable with, perhaps....<br/>&gt;<br/>&gt; -Aaron.</p><p>I see that Gene already brought up the very important issue of chords<br/>(and thus the very important issue of consistency) that you missed. I<br/>could go on and on about other issues, but for now I&apos;ll leave you<br/>with this: ratios of higher numbers are *less* sensitive to mistuning<br/>than ratios of lower numbers, since mistuning a ratio of lower<br/>numbers by a small amount will cause a greater increase in dissonance<br/>than mistuning a ratio of larger numbers by that same small amount.<br/>What do you think? You seem to have said something that contradicts<br/>this.</p></div><h3><a id=14381 href="#14381">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 5:49:15 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt;<br/>&gt;<br/>&gt; Gene,<br/>&gt;<br/>&gt; Below you chose unweigted maximum error. I respectfully submit that<br/>&gt; total sum error</p><p>You mean sum absolute error?</p><p>&gt; is a better measure for my (our) purposes.</p><p>They&apos;re actually identical (proportional) for the 5-odd-limit case.<br/>And in the 7-odd-limit case, they don&apos;t diverge nearly as much as<br/>your illustration below suggests, because of all the other intervals<br/>(7/5, 6/5, 7/6) you&apos;re considering.</p><p>&gt; (unless I&apos;m<br/>&gt; wrong, see below)...I reason that if a particular interval error is<br/>&gt; bad in a given EDO, it may have superlative representations of other<br/>&gt; intervals; however the maximum error alone wouldn&apos;t reflect this,<br/>and<br/>&gt; the EDO by your accounting would suffer in its ranking. Furthermore,<br/>&gt; the maximum error may be close to the other errors, and may make a<br/>&gt; poor EDO appear better than one that has an passable maximum error<br/>in<br/>&gt; a given interval, but several smaller very good ones (witness<br/>&gt; 31-equal, with its passable 3/2, but excellent 5/4 and 7/4)</p><p>There are a lot of ways of thinking about this, but typically larger<br/>errors are considered more important than smaller ones, and in<br/>particular if someone has a particular threshold for acceptable<br/>error, knowing the maximum error will be of quite a bit of interest.<br/>Nevertheless, your point of view is certainly valid too (so long as<br/>you take consistency into account), and corresponds to what we&apos;d call<br/>p=1, since we&apos;re raising the absolute errors to the 1st power before<br/>adding. Max-error would be p=infinity, and rms-error is p=2. Gene<br/>previously defined &quot;poptimal&quot; as optimal for any p from 2 to<br/>infinity. If he hasn&apos;t lowered that lower bound to 1 yet, I hope he<br/>will after reading your message.</p><p>&gt;<br/>&gt; The sum, by contrast, I think would give us an overall sense of the<br/>&gt; average sound of the EDO in relation to JI.<br/>&gt;<br/>&gt; Unless, of course, somehow, I&apos;ve misunderstood you, and maximum<br/>error<br/>&gt; means &quot;sum of all errors&quot;. Unless you can clarify, my instincts<br/>(which<br/>&gt; of course may be wrong) tell me that maximum wouldn&apos;t capture the<br/>&gt; overall utility of a given EDO at JI approximation?<br/>&gt;<br/>&gt; Best,<br/>&gt; Aaron.</p><p>George Secor has used maximum error to come up with things like his<br/>optimal miracle tuning, and I&apos;m sure he has very good reasons for<br/>doing so (which I know he&apos;s explained before) . . .</p></div><h3><a id=14382 href="#14382">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 5:56:13 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>It looks like you&apos;re messing up your diamonds. What you have below is<br/>more than the 7-limit diamond but less than the 9-limit diamond.<br/>You&apos;re missing 10/9, 9/8, and 16/9 from the 9-limit diamond, but<br/>should omit 9/7 and 9/5 if you want the 7-limit diamond.</p><p>If you are scoring 21 as better than 22 in the 7-limit or 9-limit,<br/>it&apos;s clear that consistency, or at least something else, has bitten<br/>you in the @$$ :)</p><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@...&gt; wrote:<br/>&gt;<br/>&gt; Correction:<br/>&gt;<br/>&gt; I hadn&apos;t used *all* the 14 intervals of the 7-limit diamond in my<br/>&gt; script as of my post below. (there are 14, aren&apos;t there?<br/>8/7,9/7,10/7,<br/>&gt;  12/7,6/5,7/5,8/5,9/5,4/3,5/3,7/6,3/2,5/4,7/4)<br/>&gt;<br/>&gt; With the correction, and all else being equal, my script now ranks<br/>the<br/>&gt; EDO&apos;s 5-99 thusly (31 is still king, but 99 and 15!!!--who&apos;da thunk<br/>it<br/>&gt; --maybe that&apos;s why we want *some* weighting?--are &apos;better&apos; than 12,<br/>&gt; and the EDO equivalent of George Bush&apos;s wasteful economic policies<br/>is<br/>&gt; now 96-EDO):<br/>&gt;<br/>&gt; ((Incidentally, I&apos;m sure you guys have talked about 171-equal, which<br/>&gt; fares best when you look at 5-1024 EDO, it looks like the most<br/>&gt; practical way to do &apos;virtual JI&apos;, as I&apos;m sure Gene would agree))<br/>&gt;<br/>&gt; total_error     efficiency<br/>&gt;<br/>&gt; 99:  11.2555    31:  69.4797<br/>&gt; 72:  21.3796    99:  74.4596<br/>&gt; 68:  31.5781    15:  79.9373<br/>&gt; 94:  33.7436    12:  91.6392<br/>&gt; 84:  34.4581    72:  97.0306<br/>&gt; 87:  35.4273    27: 103.4966<br/>&gt; 93:  40.7937     7: 108.7439<br/>&gt; 53:  43.1694    19: 114.5954<br/>&gt; 77:  43.8469    41: 115.3822<br/>&gt; 95:  44.7669    21: 118.7218<br/>&gt; 90:  45.3493    22: 125.7986<br/>&gt; 89:  45.4157    14: 125.8967<br/>&gt; 98:  45.7160     9: 128.6322<br/>&gt; 80:  46.9049    53: 132.8288<br/>&gt; 91:  47.2159    68: 133.5995<br/>&gt; 62:  47.2453     6: 134.4032<br/>&gt; 78:  48.0401    26: 147.9777<br/>&gt; 31:  50.1798    10: 150.0970<br/>&gt; 97:  51.5543    46: 152.8003<br/>&gt; 92:  51.8844    16: 155.9879<br/>&gt; 58:  52.3187    25: 159.4998<br/>&gt; 88:  52.4807    37: 162.8902<br/>&gt; 41:  53.5703    23: 167.1498<br/>&gt; 82:  53.5703    18: 173.1011<br/>&gt; 83:  53.5744    11: 174.1613<br/>&gt; 81:  54.7269    62: 178.0784<br/>&gt; 96:  54.9011    13: 178.4491<br/>&gt; 85:  55.3423     5: 179.3963<br/>&gt; 86:  57.5989    58: 181.1032<br/>&gt; 73:  59.5743    84: 188.1942<br/>&gt; 76:  60.0364     8: 198.2868<br/>&gt; 79:  60.0584    35: 199.5827<br/>&gt; 46:  60.1940    87: 201.6631<br/>&gt; 74:  63.9653    17: 206.7396<br/>&gt; 67:  64.7421    36: 206.7867<br/>&gt; 63:  64.8899    94: 210.2485<br/>&gt; 57:  65.2995    33: 212.8132<br/>&gt; 66:  65.6186    77: 215.8617<br/>&gt; 75:  68.6434    57: 221.0137<br/>&gt; 65:  69.2027    50: 223.8791<br/>&gt; 64:  72.3848    45: 224.9345<br/>&gt; 60:  72.9025    32: 237.2267<br/>&gt; 71:  74.2971    43: 237.2873<br/>&gt; 56:  74.5608    47: 237.3757<br/>&gt; 69:  78.1644    20: 237.5989<br/>&gt; 70:  78.5261    38: 237.6167<br/>&gt; 50:  78.6602    78: 240.2004<br/>&gt; 52:  80.8627    28: 241.3798<br/>&gt; 61:  80.9260    80: 241.7408<br/>&gt; 59:  82.1855    52: 242.5880<br/>&gt; 54:  85.8033    56: 246.6241<br/>&gt; 37:  88.2322    34: 247.9334<br/>&gt; 47:  90.7613    40: 249.5714<br/>&gt; 45:  91.3796    63: 249.5765<br/>&gt; 49:  91.7888    93: 251.0380<br/>&gt; 51:  92.7914    39: 252.4136<br/>&gt; 27:  96.1040    49: 254.1843<br/>&gt; 55:  98.3458    42: 256.8767<br/>&gt; 48: 102.4634    60: 263.5705<br/>&gt; 43: 102.8245    89: 265.5073<br/>&gt; 42: 115.1516    30: 266.2317<br/>&gt; 36: 116.8794    66: 267.5218<br/>&gt; 35: 117.9352    90: 268.6075<br/>&gt; 40: 120.1640    67: 268.9287<br/>&gt; 38: 123.5607    54: 270.6104<br/>&gt; 44: 125.2905    51: 271.2365<br/>&gt; 39: 126.2068    73: 274.9584<br/>&gt; 26: 126.8380    48: 275.8631<br/>&gt; 34: 135.2364    65: 276.8109<br/>&gt; 33: 138.3286    95: 282.3760<br/>&gt; 32: 142.3360    91: 283.2956<br/>&gt; 22: 150.9583    29: 283.6773<br/>&gt; 19: 157.5687    64: 283.9710<br/>&gt; 25: 172.7914    82: 284.3346<br/>&gt; 29: 173.3583    81: 286.2637<br/>&gt; 30: 177.4878    24: 287.4992<br/>&gt; 28: 181.0349    83: 288.4776<br/>&gt; 21: 192.9229    59: 290.8102<br/>&gt; 24: 205.3566    76: 290.9454<br/>&gt; 23: 217.2948    44: 298.7697<br/>&gt; 15: 219.8276    61: 298.8035<br/>&gt; 20: 237.5989    98: 298.9121<br/>&gt; 16: 259.9798    74: 300.1450<br/>&gt; 18: 272.0160    88: 302.7732<br/>&gt; 12: 274.9175    79: 304.9117<br/>&gt; 17: 295.3423    85: 306.5113<br/>&gt; 14: 314.7417    92: 315.2975<br/>&gt; 10: 350.2263    55: 317.7327<br/>&gt; 13: 401.5104    86: 323.4397<br/>&gt;  9: 450.2128    75: 327.3763<br/>&gt; 11: 464.4302    71: 331.4796<br/>&gt;  8: 594.8604    97: 333.1202<br/>&gt;  7: 652.4634    69: 336.7080<br/>&gt;  6: 672.0160    70: 344.3067<br/>&gt;  5: 717.5853    96: 350.5227<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Hey all,<br/>&gt; &gt;<br/>&gt; &gt; I wrote a Python script to rank the octave-ET&apos;s according to 7-<br/>limit<br/>&gt; &gt; JI-approximation efficiency. By this, I mean the following<br/>equation:<br/>&gt; &gt;<br/>&gt; &gt; Eff = TE*W<br/>&gt; &gt;<br/>&gt; &gt; where Eff is &quot;efficiency&quot; (the lower the number, the more<br/>efficient<br/>&gt; &gt; the n-tet is at approximating n-limit JI with fewer notes), TE is<br/>the<br/>&gt; &gt; total error in cents from JI of the intervals one is interested in<br/>&gt; &gt; looking at (for me, I chose the 7 limit intervals plus their<br/>&gt; &gt; inversions), and W stands for &quot;waste&quot;, which can be said to be the<br/>&gt; &gt; ratio of dissonant intervals to consonant intervals in the ET,<br/>which<br/>&gt; &gt; obviously grows as the ET size grows. Thus, it is a &apos;penalty&apos; for<br/>an<br/>&gt; &gt; ET to get to large in this ranking, unless of course, it really<br/>&gt; &gt; significantly lowers the JI-approximation error.<br/>&gt; &gt;<br/>&gt; &gt; One motivation for the ranking might be the question--What n-tet<br/>gives<br/>&gt; &gt; me the most consonance for the buck (the buck being the size of<br/>the<br/>&gt; &gt; n-tet).<br/>&gt; &gt;<br/>&gt; &gt; Here&apos;s the ranking for all ET&apos;s, 5 to 99--31 wins! (no surprise<br/>there)<br/>&gt; &gt; And, perhaps we get another insight into why 12 will always<br/>remain an<br/>&gt; &gt; important system: as we all know, but don&apos;t often admit, it does a<br/>&gt; &gt; fair job of &apos;expressing&apos; a fair amount of important intervals with<br/>&gt; &gt; only a handful of tones.....we also see that the equivalent of<br/>&gt; &gt; Republican wasteful spending, i.e. the George Bush of n-tets, is<br/>&gt; 97-equal.<br/>&gt; &gt;<br/>&gt; &gt; The left column is &apos;total error in cents&apos; and the right column is<br/>our<br/>&gt; &gt; ranking of interest, &apos;efficiency&apos;:<br/>&gt; &gt;<br/>&gt; &gt; total_error     efficiency<br/>&gt; &gt;<br/>&gt; &gt; 99:  11.9495    31:  66.8946<br/>&gt; &gt; 72:  22.2008    12:  76.6857<br/>&gt; &gt; 94:  29.0217    99:  79.0507<br/>&gt; &gt; 68:  31.7373    41:  96.4323<br/>&gt; &gt; 87:  32.0118    72: 100.7573<br/>&gt; &gt; 84:  32.4549     7: 102.3707<br/>&gt; &gt; 95:  37.2044    19: 104.3473<br/>&gt; &gt; 53:  37.2751    15: 112.3440<br/>&gt; &gt; 77:  37.9581    53: 114.6928<br/>&gt; &gt; 89:  38.7576    14: 124.1117<br/>&gt; &gt; 90:  40.4889     6: 125.4311<br/>&gt; &gt; 93:  40.4928    27: 126.0480<br/>&gt; &gt; 80:  44.4171    22: 133.4705<br/>&gt; &gt; 41:  44.7721    68: 134.2733<br/>&gt; &gt; 82:  44.7721    10: 137.0975<br/>&gt; &gt; 91:  44.9235    18: 144.5536<br/>&gt; &gt; 62:  46.9444    37: 149.9255<br/>&gt; &gt; 98:  47.2205    21: 153.0049<br/>&gt; &gt; 88:  47.3411    25: 153.4885<br/>&gt; &gt; 96:  47.5408    46: 155.2680<br/>&gt; &gt; 81:  47.7354    23: 159.2434<br/>&gt; &gt; 58:  47.9142    11: 161.2244<br/>&gt; &gt; 31:  48.3128    13: 165.7086<br/>&gt; &gt; 76:  48.6515    58: 165.8568<br/>&gt; &gt; 78:  49.3335    26: 167.4354<br/>&gt; &gt; 83:  49.3755    35: 169.1267<br/>&gt; &gt; 97:  50.4854     5: 171.8133<br/>&gt; &gt; 92:  52.8566    62: 176.9442<br/>&gt; &gt; 85:  55.1878    84: 177.2534<br/>&gt; &gt; 86:  55.9476    17: 178.0986<br/>&gt; &gt; 65:  58.9982    94: 180.8277<br/>&gt; &gt; 75:  59.1556    36: 182.0417<br/>&gt; &gt; 63:  59.8286    87: 182.2212<br/>&gt; &gt; 46:  61.1662     8: 182.5136<br/>&gt; &gt; 74:  62.6937    77: 186.8706<br/>&gt; &gt; 79:  63.0488    16: 188.6049<br/>&gt; &gt; 64:  63.4544    50: 205.3443<br/>&gt; &gt; 73:  63.6051    43: 208.8975<br/>&gt; &gt; 66:  63.7535     9: 214.2032<br/>&gt; &gt; 57:  64.4410    45: 215.9116<br/>&gt; &gt; 60:  65.5829    49: 216.1089<br/>&gt; &gt; 70:  65.8457    57: 218.1080<br/>&gt; &gt; 71:  66.4328    34: 222.3343<br/>&gt; &gt; 67:  66.6762    47: 225.0262<br/>&gt; &gt; 69:  69.3722    89: 226.5829<br/>&gt; &gt; 56:  69.9429    80: 228.9191<br/>&gt; &gt; 50:  72.1480    63: 230.1098<br/>&gt; &gt; 59:  74.6132    56: 231.3495<br/>&gt; &gt; 54:  76.1355    38: 232.3184<br/>&gt; &gt; 61:  77.8686    30: 232.4634<br/>&gt; &gt; 49:  78.0393    29: 232.9472<br/>&gt; &gt; 37:  81.2097    95: 234.6738<br/>&gt; &gt; 52:  82.0285    42: 235.3100<br/>&gt; &gt; 47:  86.0394    76: 235.7728<br/>&gt; &gt; 45:  87.7141    65: 235.9929<br/>&gt; &gt; 55:  89.2080    60: 237.1073<br/>&gt; &gt; 43:  90.5223    82: 237.6367<br/>&gt; &gt; 51:  90.6945    40: 238.8968<br/>&gt; &gt; 48:  92.7956    90: 239.8191<br/>&gt; &gt; 35:  99.9385    54: 240.1196<br/>&gt; &gt; 27: 100.8384    39: 240.9800<br/>&gt; &gt; 36: 102.8932    24: 241.9821<br/>&gt; &gt; 42: 105.4838    33: 243.2895<br/>&gt; &gt; 40: 115.0244    32: 245.7927<br/>&gt; &gt; 44: 116.7938    52: 246.0854<br/>&gt; &gt; 39: 120.4900    78: 246.6677<br/>&gt; &gt; 38: 120.8056    64: 248.9364<br/>&gt; &gt; 34: 121.2733    93: 249.1862<br/>&gt; &gt; 22: 133.4705    81: 249.6930<br/>&gt; &gt; 33: 139.0226    48: 249.8344<br/>&gt; &gt; 29: 142.3567    20: 250.2792<br/>&gt; &gt; 19: 143.4776    28: 252.7545<br/>&gt; &gt; 26: 143.5161    66: 259.9180<br/>&gt; &gt; 32: 147.4756    59: 264.0161<br/>&gt; &gt; 30: 154.9756    51: 265.1071<br/>&gt; &gt; 28: 163.5470    83: 265.8680<br/>&gt; &gt; 25: 166.2792    91: 269.5412<br/>&gt; &gt; 24: 172.8444    88: 273.1216<br/>&gt; &gt; 23: 173.7200    67: 276.9627<br/>&gt; &gt; 21: 204.0065    44: 278.5083<br/>&gt; &gt; 15: 224.6880    75: 282.1268<br/>&gt; &gt; 18: 227.1556    61: 287.5149<br/>&gt; &gt; 12: 230.0571    55: 288.2105<br/>&gt; &gt; 16: 242.4920    70: 288.7081<br/>&gt; &gt; 20: 250.2792    73: 293.5622<br/>&gt; &gt; 17: 254.4266    74: 294.1783<br/>&gt; &gt; 14: 310.2792    71: 296.3927<br/>&gt; &gt; 10: 319.8941    69: 298.8341<br/>&gt; &gt; 13: 372.8444    96: 303.5295<br/>&gt; &gt;  9: 428.4065    85: 305.6556<br/>&gt; &gt; 11: 429.9317    98: 308.7495<br/>&gt; &gt;  8: 547.5408    86: 314.1673<br/>&gt; &gt;  7: 614.2242    79: 320.0938<br/>&gt; &gt;  6: 627.1556    92: 321.2052<br/>&gt; &gt;  5: 687.2532    97: 326.2135<br/>&gt; &gt;<br/>&gt; &gt; -Aaron.<br/>&gt; &gt;<br/>&gt;</p></div><h3><a id=14384 href="#14384">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 6:17:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&gt; &lt;genewardsmith@&gt; wrote:<br/>&gt;<br/>&gt; The 103169 et<br/>&gt; is the Paul Erlich division,</p><p>It was already known to Marc Jones as &quot;Halloween &apos;69&quot; before I found it.</p></div><h3><a id=14385 href="#14385">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 6:20:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;Keenan,<br/>&gt; &gt;<br/>&gt; &gt;This is interesting. But I fail to see how it has anything to do with<br/>&gt; &gt;my original concept, where the larger the EDO, the more it gets<br/>&gt; &gt;penalized....<br/>&gt; &gt;<br/>&gt; &gt;Best,<br/>&gt; &gt;Aaron.<br/>&gt;<br/>&gt; As I said, what Keenan&apos;s doing is consistency,</p><p>Not really -- it&apos;s more closely related to &quot;unique articulation&quot; than<br/>to consistency.</p></div><h3><a id=14390 href="#14390">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 6:36:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Hi Aaron!<br/>&gt;&gt;<br/>&gt;&gt; This is what we might call a type of &quot;badness&quot; calculation, or<br/>&gt;&gt; rather, the inverse of one.  Actually, since you&apos;ve defined<br/>&gt;&gt; efficiency as going up when Eff goes down, it&apos;s not the inverse.<br/>&gt;&gt; The general approach is<br/>&gt;&gt;<br/>&gt;&gt; badness = complexity * error<br/>&gt;<br/>&gt;No, that&apos;s not the general approach.</p><p>Gene said the same thing.</p><p>&gt;I don&apos;t know why you say it is!  We&apos;ve seen huge number of results<br/>&gt;from Gene which use logflat badness instead (which rarely agrees<br/>&gt;with your formula above),</p><p>Clearly the formula above includes logflat badness.</p><p>&gt;Dave Keenan has also proposed different badness functions, and my<br/>&gt;paper implicitly uses<br/>&gt;<br/>&gt;badness = a*complexity + b*error<br/>&gt;<br/>&gt;The general approach, instead, is to define a badness function that<br/>&gt;is an increasing function of both complexity and error. Anything more<br/>&gt;specific is less general. :)</p><p>In what way does my formula above preclude such definitions?  Since<br/>I gave none, and mentioned such definitions in the accompanying text,<br/>I can only call your response here a misguided one.</p><p>&gt;&gt; Complexity can be number of notes, but this gets tricky in linear<br/>&gt;&gt; and planar temperaments where the number of notes is not fixed.<br/>&gt;&gt; Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>&gt;&gt; forgetting have proposed various complexity measures.  Usually<br/>&gt;&gt; they wind up being fairly similar to one another.<br/>&gt;&gt;<br/>&gt;&gt; Error also has variants.  Weighted error is popular; the deviation<br/>&gt;&gt; from JI is weighted depending on the harmonic limit of the interval<br/>&gt;&gt; in question.  There&apos;s weighted complexity, too, for that matter.<br/>&gt;&gt;<br/>&gt;&gt; Probably thousands of messages have been written on this topic,<br/>&gt;&gt; with little consensus to this day.  :(<br/>&gt;<br/>&gt;Actually, Graham and I have made relatively huge strides toward<br/>&gt;consensus in the last few weeks.</p><p>That&apos;s good.</p><p>-Carl</p></div><h3><a id=14391 href="#14391">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 6:38:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;Keenan,<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;This is interesting. But I fail to see how it has anything to do with<br/>&gt;&gt; &gt;my original concept, where the larger the EDO, the more it gets<br/>&gt;&gt; &gt;penalized....<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Best,<br/>&gt;&gt; &gt;Aaron.<br/>&gt;&gt;<br/>&gt;&gt; As I said, what Keenan&apos;s doing is consistency,<br/>&gt;<br/>&gt;Not really -- it&apos;s more closely related to &quot;unique articulation&quot; than<br/>&gt;to consistency.</p><p>What&apos;s &quot;unique articulation&quot;?  It&apos;d be pretty hard to get any closer<br/>to consistency (as Paul Hahn defines it) than Keenan has.</p><p>-Carl</p></div><h3><a id=14395 href="#14395">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 6:52:36 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; Hi Aaron!<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; This is what we might call a type of &quot;badness&quot; calculation, or<br/>&gt; &gt;&gt; rather, the inverse of one.  Actually, since you&apos;ve defined<br/>&gt; &gt;&gt; efficiency as going up when Eff goes down, it&apos;s not the inverse.<br/>&gt; &gt;&gt; The general approach is<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; badness = complexity * error<br/>&gt; &gt;<br/>&gt; &gt;No, that&apos;s not the general approach.<br/>&gt;<br/>&gt; Gene said the same thing.<br/>&gt;<br/>&gt; &gt;I don&apos;t know why you say it is!  We&apos;ve seen huge number of results<br/>&gt; &gt;from Gene which use logflat badness instead (which rarely agrees<br/>&gt; &gt;with your formula above),<br/>&gt;<br/>&gt; Clearly the formula above includes logflat badness.</p><p>Rarely.</p><p>&gt; &gt;Dave Keenan has also proposed different badness functions, and my<br/>&gt; &gt;paper implicitly uses<br/>&gt; &gt;<br/>&gt; &gt;badness = a*complexity + b*error<br/>&gt; &gt;<br/>&gt; &gt;The general approach, instead, is to define a badness function<br/>that<br/>&gt; &gt;is an increasing function of both complexity and error. Anything<br/>more<br/>&gt; &gt;specific is less general. :)<br/>&gt;<br/>&gt; In what way does my formula above preclude such definitions?</p><p>Because it&apos;s not equivalent to them! You&apos;ll never get a weighted sum<br/>of complexity and error to be equivalent to a product of them.</p><p>&gt; Since<br/>&gt; I gave none,</p><p>Huh? You wrote,</p><p>&quot;<br/>&gt; &gt;&gt; The general approach is<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; badness = complexity * error<br/>&quot;</p><p>That reads &quot;badness equals complexity times error&quot;.</p><p>&gt; and mentioned such definitions in the accompanying text,<br/>&gt; I can only call your response here a misguided one.</p><p>Since Aaron was very pleased to see your post since he&apos;s also<br/>multiplying complexity by error, it seems he was also misled by what<br/>you wrote. So I don&apos;t think my response was misguided at all! It<br/>seemed very &quot;guided&quot; considering the context of this list. Now, what<br/>is it that you *really* meant?</p></div><h3><a id=14396 href="#14396">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 6:54:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;Keenan,<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;This is interesting. But I fail to see how it has anything to<br/>do with<br/>&gt; &gt;&gt; &gt;my original concept, where the larger the EDO, the more it gets<br/>&gt; &gt;&gt; &gt;penalized....<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Best,<br/>&gt; &gt;&gt; &gt;Aaron.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; As I said, what Keenan&apos;s doing is consistency,<br/>&gt; &gt;<br/>&gt; &gt;Not really -- it&apos;s more closely related to &quot;unique articulation&quot;<br/>than<br/>&gt; &gt;to consistency.<br/>&gt;<br/>&gt; What&apos;s &quot;unique articulation&quot;?</p><p>It means all the consonances map to different numbers of steps in the<br/>ET.</p><p>&gt; It&apos;d be pretty hard to get any closer<br/>&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt;<br/>&gt; -Carl</p><p>Why? I could see inconsistent tunings getting a higher score than<br/>consistent tunings by Keenan&apos;s measure.</p></div><h3><a id=14398 href="#14398">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 7:07:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;&gt; This is interesting. But I fail to see how it has anything to do with<br/>&gt;&gt;&gt; my original concept, where the larger the EDO, the more it gets<br/>&gt;&gt;&gt; penalized....<br/>&gt;&gt;<br/>&gt;&gt; As I said, what Keenan&apos;s doing is consistency, and consistency is a<br/>&gt;&gt; type of badness.  It penalizes error in terms of the size of the scale<br/>&gt;&gt; step, which goes down as notes go up, so is the same.<br/>&gt;<br/>&gt;Of course I know what you mean, but you are missing my point.<br/>&gt;<br/>&gt;Suppose you have an error of .2 and .1, the ratio being 2:1<br/>&gt;Then you have .6 and .3, also 2:1<br/>&gt;<br/>&gt;Clearly, the first one is a larger EDO.</p><p>Why&apos;s that?  Are these errors fractions of the ET step?</p><p>&gt;But the ratio wouldn&apos;t tell us that, we&apos;d already have to know that.</p><p>If they are fractions of the ET step and the actual errors are<br/>the same size, then the first would be a smaller ET and it would<br/>have an advantage in complexity as you&apos;d hope.  If the errors<br/>aren&apos;t the same size, then...</p><p>-Carl</p></div><h3><a id=14399 href="#14399">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 7:11:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&quot;<br/>&gt;&gt; &gt;&gt; The general approach is<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; badness = complexity * error<br/>&gt;&quot;<br/>&gt;<br/>&gt;That reads &quot;badness equals complexity times error&quot;.</p><p>And it doesn&apos;t say what complexity and error are.</p><p>&gt;&gt; and mentioned such definitions in the accompanying text,<br/>&gt;&gt; I can only call your response here a misguided one.<br/>&gt;<br/>&gt;Since Aaron was very pleased to see your post since he&apos;s also<br/>&gt;multiplying complexity by error, it seems he was also misled by what<br/>&gt;you wrote. So I don&apos;t think my response was misguided at all! It<br/>&gt;seemed very &quot;guided&quot; considering the context of this list.</p><p>In another post I asked him about exponents.  Gene also&apos;s<br/>been talking with him about that.</p><p>-Carl</p></div><h3><a id=14400 href="#14400">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 7:11:51 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt;&gt;<br/>&gt;&gt; -Carl<br/>&gt;<br/>&gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt;consistent tunings by Keenan&apos;s measure.</p><p>Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>part lopped off.</p><p>-C.</p></div><h3><a id=14401 href="#14401">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/10/2006 7:20:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>wallyesterpaulrus wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:</p><p>Carl:<br/>&gt;&gt;Complexity can be number of notes, but this gets tricky in linear<br/>&gt;&gt;and planar temperaments where the number of notes is not fixed.<br/>&gt;&gt;Gene, Graham, Dave, Paul, myself, and probably someone I&apos;m<br/>&gt;&gt;forgetting have proposed various complexity measures.  Usually<br/>&gt;&gt;they wind up being fairly similar to one another.<br/>&gt;&gt;<br/>&gt;&gt;Error also has variants.  Weighted error is popular; the deviation<br/>&gt;&gt;from JI is weighted depending on the harmonic limit of the interval<br/>&gt;&gt;in question.  There&apos;s weighted complexity, too, for that matter.<br/>&gt;&gt;<br/>&gt;&gt;Probably thousands of messages have been written on this topic,<br/>&gt;&gt;with little consensus to this day.  :(</p><p>Paul E:<br/>&gt; Actually, Graham and I have made relatively huge strides toward &gt; consensus in the last few weeks. At least, we&apos;ll have a number of &gt; different but clearly defined and clearly motivated measures.</p><p>Oh, that&apos;s good!  There are a few choices you have to make:</p><p>weighted/unweighted</p><p>odd/prime limit</p><p>max/mean/rms/whatever</p><p>A lot of variations are covered by my code at:</p><p><a href="http://www.microtonal.co.uk/temper/regular.zip">http://www.microtonal.co.uk/temper/regular.zip</a></p><p>except that I moved the weighted-primes code to new files because it&apos;s a lot simpler without the baggage for odd-limits.</p><p>Complexity is a bit less obvious.  For rank 2, most people use max-unweighted, which they call &quot;Graham complexity&quot;.  The point is that the number of notes in a scale minus the complexity gives you the number of otonal chords you can play.  That can be adapted to weighted prime-limits.  But for higher rank temperaments it doesn&apos;t obviously generalize so you can use wedgies instead.</p><p>                   Graham</p></div><h3><a id=14402 href="#14402">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/10/2006 7:25:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt; &gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; -Carl<br/>&gt; &gt;<br/>&gt; &gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt;<br/>&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt; part lopped off.</p><p>Show me.</p></div><h3><a id=14416 href="#14416">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/10/2006 8:33:46 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt; &gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; -Carl<br/>&gt; &gt;<br/>&gt; &gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt;<br/>&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt; part lopped off.</p><p>floor(1 / Pepper ambiguity) you mean? That doesn&apos;t strike me as much<br/>of a consistency measure. I thought it had something to do with the<br/>whether the rounded value was chosen for each element of the<br/>corresponding diamond.</p></div><h3><a id=14417 href="#14417">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 9:02:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt;&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt;&gt;<br/>&gt;&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt;&gt; part lopped off.<br/>&gt;<br/>&gt;Show me.</p><p>For one, you can read the integer part of Keenan&apos;s result off the<br/>tables on Paul&apos;s website.  For two, Keenan&apos;s already agreed.  For<br/>three, here is Paul&apos;s algorithm for consistency</p><p>;; Takes an equal temperament and a list of natural number<br/>;; identities and returns the exact consistency level at which<br/>;; the et approximates the identities.</p><p>(define consist<br/>   (lambda (et ls)<br/>      (let ((y (map (lambda (r) (- r (round r)))<br/>                  (map (lambda (s) (* et (log2 s))) ls))))<br/>         (/ 1 (* 2 (abs (- (apply max y) (apply min y))))))))</p><p>;; Returns the Hahn consistency level for an equal temperament<br/>;; over a list of natural number identities.  Requires consist.</p><p>(define consist-int<br/>   (lambda (et ls)<br/>      (truncate (consist et ls))))</p><p>-C.</p></div><h3><a id=14418 href="#14418">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 9:14:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;For one, you can read the integer part of Keenan&apos;s result off the<br/>&gt;tables on Paul&apos;s website.</p><p>Uh, I mean the ranking.</p><p>-Carl</p></div><h3><a id=14422 href="#14422">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/10/2006 11:07:06 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt;&gt; &gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; -Carl<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt;&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt;&gt;<br/>&gt;&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt;&gt; part lopped off.<br/>&gt;<br/>&gt;floor(1 / Pepper ambiguity) you mean?</p><p>That&apos;s what I meant, but it&apos;s off by a factor of 2 and still only<br/>close.  Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>size S and reporting</p><p>E/(S-E)</p><p>Paul H.&apos;s consistency is the integer part of how many times the worst<br/>interval&apos;s error goes into half a step; floor(S/2E).</p><p>So ignoring floor, the factor of 2, and the inverse, we still<br/>have</p><p>E/S != E/(S-E)</p><p>So sorry about that, but it looks like the ranking should still be<br/>the same.</p><p>-Carl</p></div><h3><a id=14428 href="#14428">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/11/2006 3:17:14 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:</p><p>&gt; So sorry about that, but it looks like the ranking should still be<br/>&gt; the same.</p><p>If P is Pepper ambiguity, then the Hahn rank, according to this, will be</p><p>floor((P+1)/(2P))</p><p>This makes more sense; if this has a value greater than 1, then<br/>P&lt;=1/3, which insures that the rounded val/patent breed or whatever<br/>the hell you call it always gives the rounded value for every element<br/>of the diamond, which is a reasonable notion of consistency. This is a<br/>pretty strong requirement, however, in the higher limits.</p></div><h3><a id=14430 href="#14430">ðŸ”—</a>akjmicro &#x3C;aaron@akjmusic.com&#x3E;</h3><span>2/11/2006 6:22:03 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot; &lt;perlich@...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;akjmicro&quot; &lt;aaron@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; Gene,<br/>&gt; &gt;<br/>&gt; &gt; Below you chose unweigted maximum error. I respectfully submit that<br/>&gt; &gt; total sum error<br/>&gt;<br/>&gt; You mean sum absolute error?</p><p>Yes. What else would I mean?</p><p>&gt; &gt; is a better measure for my (our) purposes.<br/>&gt;<br/>&gt; They&apos;re actually identical (proportional) for the 5-odd-limit case.<br/>&gt; And in the 7-odd-limit case, they don&apos;t diverge nearly as much as<br/>&gt; your illustration below suggests, because of all the other intervals<br/>&gt; (7/5, 6/5, 7/6) you&apos;re considering.</p><p>but they *do* diverge, which might be of interest, agreed? ;)</p><p>&gt; &gt; (unless I&apos;m<br/>&gt; &gt; wrong, see below)...I reason that if a particular interval error is<br/>&gt; &gt; bad in a given EDO, it may have superlative representations of other<br/>&gt; &gt; intervals; however the maximum error alone wouldn&apos;t reflect this,<br/>&gt; and<br/>&gt; &gt; the EDO by your accounting would suffer in its ranking. Furthermore,<br/>&gt; &gt; the maximum error may be close to the other errors, and may make a<br/>&gt; &gt; poor EDO appear better than one that has an passable maximum error<br/>&gt; in<br/>&gt; &gt; a given interval, but several smaller very good ones (witness<br/>&gt; &gt; 31-equal, with its passable 3/2, but excellent 5/4 and 7/4)<br/>&gt;<br/>&gt; There are a lot of ways of thinking about this, but typically larger<br/>&gt; errors are considered more important than smaller ones, and in<br/>&gt; particular if someone has a particular threshold for acceptable<br/>&gt; error, knowing the maximum error will be of quite a bit of interest.<br/>&gt; Nevertheless, your point of view is certainly valid too (so long as<br/>&gt; you take consistency into account), and corresponds to what we&apos;d call<br/>&gt; p=1, since we&apos;re raising the absolute errors to the 1st power before<br/>&gt; adding. Max-error would be p=infinity, and rms-error is p=2. Gene<br/>&gt; previously defined &quot;poptimal&quot; as optimal for any p from 2 to<br/>&gt; infinity. If he hasn&apos;t lowered that lower bound to 1 yet, I hope he<br/>&gt; will after reading your message.</p><p>Interesting, I see your points...</p><p>&gt; &gt;<br/>&gt; &gt; The sum, by contrast, I think would give us an overall sense of the<br/>&gt; &gt; average sound of the EDO in relation to JI.<br/>&gt; &gt;<br/>&gt; &gt; Unless, of course, somehow, I&apos;ve misunderstood you, and maximum<br/>&gt; error<br/>&gt; &gt; means &quot;sum of all errors&quot;. Unless you can clarify, my instincts<br/>&gt; (which<br/>&gt; &gt; of course may be wrong) tell me that maximum wouldn&apos;t capture the<br/>&gt; &gt; overall utility of a given EDO at JI approximation?<br/>&gt; &gt;<br/>&gt; &gt; Best,<br/>&gt; &gt; Aaron.<br/>&gt;<br/>&gt; George Secor has used maximum error to come up with things like his<br/>&gt; optimal miracle tuning, and I&apos;m sure he has very good reasons for<br/>&gt; doing so (which I know he&apos;s explained before) . . .</p><p>I&apos;m going to try something novel....metaranking (or should it be<br/>called &apos;AKJ superranking&apos;....just kidding)....ranking an EDO in the<br/>n-limit not by using it&apos;s sum of absolute error(s), but by it&apos;s sum of<br/>rankings in a list of errors for each interval. It might give me the<br/>same thing, or it might illumine something else entirely.</p><p>Maybe there&apos;s a way to take this result and scale it consistently from<br/>0 to 1, like the &apos;logflat&apos; idea....</p><p>(As you can see, I&apos;m not a rigorous mathematician, but more of a<br/>playful explorer of sorts)</p><p>-Aaron.</p></div><h3><a id=14432 href="#14432">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/11/2006 1:29:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;I&apos;m going to try something novel....metaranking (or should it be<br/>&gt;called &apos;AKJ superranking&apos;....just kidding)....ranking an EDO in the<br/>&gt;n-limit not by using it&apos;s sum of absolute error(s), but by it&apos;s sum of<br/>&gt;rankings in a list of errors for each interval. It might give me the<br/>&gt;same thing, or it might illumine something else entirely.</p><p>Interesting...</p><p>-C.</p></div><h3><a id=14476 href="#14476">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/22/2006 11:53:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;For one, you can read the integer part of Keenan&apos;s result off the<br/>&gt; &gt;tables on Paul&apos;s website.<br/>&gt;<br/>&gt; Uh, I mean the ranking.<br/>&gt;<br/>&gt; -Carl<br/>&gt;<br/>Carl, you&apos;ve lost me. Can you pull this all together for me, with URLsn formulae, and no scheme code please?</p></div><h3><a id=14489 href="#14489">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/23/2006 3:52:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt; &gt;&gt; &gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; -Carl<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Why? I could see inconsistent tunings getting a higher score than<br/>&gt; &gt;&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt; &gt;&gt; part lopped off.<br/>&gt; &gt;<br/>&gt; &gt;floor(1 / Pepper ambiguity) you mean?<br/>&gt;<br/>&gt; That&apos;s what I meant, but it&apos;s off by a factor of 2 and still only<br/>&gt; close.  Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt; size S and reporting<br/>&gt;<br/>&gt; E/(S-E)<br/>&gt;<br/>&gt; Paul H.&apos;s consistency is the integer part of how many times the worst<br/>&gt; interval&apos;s error</p><p>I don&apos;t think this uses the same set of intervals that Keenan is using, Carl. If you do it with the full odd limit like Keenan, the measure won&apos;t help you to distinguish consistent from inconsistent tunings in every case.</p><p>&gt;goes into half a &gt;step; floor(S/2E).<br/>&gt;<br/>&gt; So ignoring floor, the factor of 2, and the inverse, we still<br/>&gt; have<br/>&gt;<br/>&gt; E/S != E/(S-E)<br/>&gt;<br/>&gt; So sorry about that, but it looks like the ranking should still be<br/>&gt; the same.<br/>&gt;<br/>&gt; -Carl</p><p>I&apos;m still skeptical, but not in number-crunch mode right now . . .</p></div><h3><a id=14491 href="#14491">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/23/2006 4:09:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@&gt; wrote:<br/>&gt;<br/>&gt; &gt; So sorry about that, but it looks like the ranking should still be<br/>&gt; &gt; the same.<br/>&gt;<br/>&gt; If P is Pepper ambiguity, then the Hahn rank, according to this, will be<br/>&gt;<br/>&gt; floor((P+1)/(2P))</p><p>I really don&apos;t think you meant to say &quot;rank&quot; here. Plus, I don&apos;t think Carl got the differences between the Hahn and Pepper measures right in the first place.</p><p>&gt; This makes more sense; if this has a value greater than 1, then<br/>&gt; P&lt;=1/3, which insures that the rounded val/patent breed or whatever<br/>&gt; the hell you call it always gives the rounded value for every element<br/>&gt; of the diamond, which is a reasonable notion of consistency. This is a<br/>&gt; pretty strong requirement, however, in the higher limits.</p><p>Is this stronger than our traditional consistency requirement?</p></div><h3><a id=14493 href="#14493">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/23/2006 4:15:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot; &lt;perlich@...&gt;<br/>wrote:</p><p>&gt; &gt; This makes more sense; if this has a value greater than 1, then<br/>&gt; &gt; P&lt;=1/3, which insures that the rounded val/patent breed or whatever<br/>&gt; &gt; the hell you call it always gives the rounded value for every element<br/>&gt; &gt; of the diamond, which is a reasonable notion of consistency. This is a<br/>&gt; &gt; pretty strong requirement, however, in the higher limits.<br/>&gt;<br/>&gt; Is this stronger than our traditional consistency requirement?</p><p>Much stronger, especially in higher limits.</p></div><h3><a id=14494 href="#14494">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/23/2006 4:20:57 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;<br/>&gt; &gt; &gt; is a better measure for my (our) purposes.<br/>&gt; &gt;<br/>&gt; &gt; They&apos;re actually identical (proportional) for the 5-odd-limit case.<br/>&gt; &gt; And in the 7-odd-limit case, they don&apos;t diverge nearly as much as<br/>&gt; &gt; your illustration below suggests, because of all the other intervals<br/>&gt; &gt; (7/5, 6/5, 7/6) you&apos;re considering.<br/>&gt;<br/>&gt; but they *do* &gt;diverge, which &gt;might be of &gt;interest, agreed? &gt;;)</p><p>Yeah, and that&apos;s no joke. Using the squared errors gives a thirdn compromise option to keep an eye on.</p><p>&gt; &gt; &gt; (unless I&apos;m<br/>&gt; &gt; &gt; wrong, see below)...I reason that if a particular interval error is<br/>&gt; &gt; &gt; bad in a given EDO, it may have superlative representations of other<br/>&gt; &gt; &gt; intervals; however the maximum error alone wouldn&apos;t reflect this,<br/>&gt; &gt; and<br/>&gt; &gt; &gt; the EDO by your accounting would suffer in its ranking. Furthermore,<br/>&gt; &gt; &gt; the maximum error may be close to the other errors, and may make a<br/>&gt; &gt; &gt; poor EDO appear better than one that has an passable maximum error<br/>&gt; &gt; in<br/>&gt; &gt; &gt; a given interval, but several smaller very good ones (witness<br/>&gt; &gt; &gt; 31-equal, with its passable 3/2, but excellent 5/4 and 7/4)<br/>&gt; &gt;<br/>&gt; &gt; There are a lot of ways of thinking about this, but typically larger<br/>&gt; &gt; errors are considered more important than smaller ones, and in<br/>&gt; &gt; particular if someone has a particular threshold for acceptable<br/>&gt; &gt; error, knowing the maximum error will be of quite a bit of interest.<br/>&gt; &gt; Nevertheless, your point of view is certainly valid too (so long as<br/>&gt; &gt; you take consistency into account), and corresponds to what we&apos;d call<br/>&gt; &gt; p=1, since we&apos;re raising the absolute errors to the 1st power before<br/>&gt; &gt; adding. Max-error would be p=infinity, and rms-error is p=2. Gene<br/>&gt; &gt; previously defined &quot;poptimal&quot; as optimal for any p from 2 to<br/>&gt; &gt; infinity. If he hasn&apos;t lowered that lower bound to 1 yet, I hope he<br/>&gt; &gt; will after reading your message.<br/>&gt;<br/>&gt; Interesting, I see your points...<br/>&gt;<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; The sum, by contrast, I think would give us an overall sense of the<br/>&gt; &gt; &gt; average sound of the EDO in relation to JI.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Unless, of course, somehow, I&apos;ve misunderstood you, and maximum<br/>&gt; &gt; error<br/>&gt; &gt; &gt; means &quot;sum of all errors&quot;. Unless you can clarify, my instincts<br/>&gt; &gt; (which<br/>&gt; &gt; &gt; of course may be wrong) tell me that maximum wouldn&apos;t capture the<br/>&gt; &gt; &gt; overall utility of a given EDO at JI approximation?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Best,<br/>&gt; &gt; &gt; Aaron.<br/>&gt; &gt;<br/>&gt; &gt; George Secor has used maximum error to come up with things like his<br/>&gt; &gt; optimal miracle tuning, and I&apos;m sure he has very good reasons for<br/>&gt; &gt; doing so (which I know he&apos;s explained before) . . .<br/>&gt;<br/>&gt; I&apos;m going to try something novel....metaranking (or should it be<br/>&gt; called &apos;AKJ superranking&apos;....just kidding)....ranking an EDO in the<br/>&gt; n-limit not by using it&apos;s sum of absolute error(s), but by it&apos;s sum of<br/>&gt; rankings in a list of errors for each interval.</p><p>It seems that this would make consistency very hard to deal with.</p><p> It might give me the<br/>&gt; same thing, or it might illumine something else entirely.<br/>&gt;<br/>&gt; Maybe there&apos;s a way to take this result and scale it consistently from<br/>&gt; 0 to 1, like the &apos;logflat&apos; idea....<br/>&gt;<br/>&gt; (As you can see, I&apos;m not a rigorous mathematician, but more of a<br/>&gt; playful explorer of sorts)<br/>&gt;<br/>&gt; -Aaron.</p><p>Did you read the longish e-mail I sent you a couple weeks ago?</p></div><h3><a id=14515 href="#14515">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/23/2006 11:24:27 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;genewardsmith@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot; &lt;perlich@&gt;<br/>&gt; wrote:<br/>&gt;<br/>&gt; &gt; &gt; This makes more sense; if this has a value greater than 1, then<br/>&gt; &gt; &gt; P&lt;=1/3, which insures that the rounded val/patent breed or whatever<br/>&gt; &gt; &gt; the hell you call it always gives the rounded value for every element<br/>&gt; &gt; &gt; of the diamond, which is a reasonable notion of consistency. This is a<br/>&gt; &gt; &gt; pretty strong requirement, however, in the higher limits.<br/>&gt; &gt;<br/>&gt; &gt; Is this stronger than our traditional consistency requirement?<br/>&gt;<br/>&gt; Much stronger, especially in higher limits.</p><p>I don&apos;t think Carl is aware of this difference right now. It doesn&apos;t look like Keenan&apos;s measure relates in any clean way to our traditional consistency condition.</p></div><h3><a id=14516 href="#14516">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/23/2006 11:44:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot; &lt;perlich@...&gt;<br/>wrote:</p><p>&gt; I don&apos;t think Carl is aware of this difference right now. It doesn&apos;t<br/>look like Keenan&apos;s measure relates in any clean way to our traditional<br/>consistency condition.</p><p>He didn&apos;t call it consistency, but ambiguity. I think it&apos;s an<br/>interesting measure.</p></div><h3><a id=14517 href="#14517">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 12:07:22 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt; So sorry about that, but it looks like the ranking should still be<br/>&gt;&gt; &gt; the same.<br/>&gt;&gt;<br/>&gt;&gt; If P is Pepper ambiguity, then the Hahn rank, according to this, will be<br/>&gt;&gt;<br/>&gt;&gt; floor((P+1)/(2P))<br/>&gt;<br/>&gt;I really don&apos;t think you meant to say &quot;rank&quot; here. Plus, I don&apos;t think<br/>&gt;Carl got the differences between the Hahn and Pepper measures right in<br/>&gt;the first place.</p><p>I did finally get it right.</p><p>-Carl</p></div><h3><a id=14526 href="#14526">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 1:54:40 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt; So sorry about that, but it looks like the ranking should<br/>still be<br/>&gt; &gt;&gt; &gt; the same.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; If P is Pepper ambiguity, then the Hahn rank, according to this,<br/>will be<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; floor((P+1)/(2P))<br/>&gt; &gt;<br/>&gt; &gt;I really don&apos;t think you meant to say &quot;rank&quot; here. Plus, I don&apos;t<br/>think<br/>&gt; &gt;Carl got the differences between the Hahn and Pepper measures<br/>right in<br/>&gt; &gt;the first place.<br/>&gt;<br/>&gt; I did finally get it right.<br/>&gt;<br/>&gt; -Carl</p><p>Where?</p></div><h3><a id=14527 href="#14527">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 2:03:34 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; I did finally get it right.<br/>&gt;&gt;<br/>&gt;&gt; -Carl<br/>&gt;<br/>&gt;Where?</p><p><a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a></p><p>-C.</p></div><h3><a id=14529 href="#14529">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 2:12:14 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; I did finally get it right.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; -Carl<br/>&gt; &gt;<br/>&gt; &gt;Where?<br/>&gt;<br/>&gt; <a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a><br/>&gt;<br/>&gt; -C.</p><p>That&apos;s incorrect, as I remarked in my reply. Are you caught up?</p></div><h3><a id=14531 href="#14531">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 2:25:48 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; I did finally get it right.<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; -Carl<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Where?<br/>&gt;&gt;<br/>&gt;&gt; <a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a><br/>&gt;&gt;<br/>&gt;&gt; -C.<br/>&gt;<br/>&gt;That&apos;s incorrect, as I remarked in my reply. Are you caught up?</p><p>What&apos;s incorrect about it?</p><p>-Carl</p></div><h3><a id=14534 href="#14534">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 2:47:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; I did finally get it right.<br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; -Carl<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Where?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; <a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a><br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; -C.<br/>&gt; &gt;<br/>&gt; &gt;That&apos;s incorrect, as I remarked in my reply. Are you caught up?<br/>&gt;<br/>&gt; What&apos;s incorrect about it?<br/>&gt;<br/>&gt; -Carl</p><p>I guess you haven&apos;t followed the thread. Keenan and Hahn use a<br/>different set of intervals as input into their formulae. If they used<br/>the same set of intervals, the consistency condition Gene derived<br/>from Keenan ambiguity would be the same as our traditional<br/>consistency condition, but as he told us, it isn&apos;t.</p></div><h3><a id=14536 href="#14536">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 2:59:11 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; &gt;&gt; I did finally get it right.<br/>&gt;&gt; &gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; &gt;&gt; -Carl<br/>&gt;&gt; &gt;&gt; &gt;<br/>&gt;&gt; &gt;&gt; &gt;Where?<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; <a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a><br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; -C.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;That&apos;s incorrect, as I remarked in my reply. Are you caught up?<br/>&gt;&gt;<br/>&gt;&gt; What&apos;s incorrect about it?<br/>&gt;&gt;<br/>&gt;&gt; -Carl<br/>&gt;<br/>&gt;I guess you haven&apos;t followed the thread.  Keenan and Hahn use a<br/>&gt;different set of intervals as input into their formulae.</p><p>What I saw were brief, vague assertions from you that this was<br/>the case.</p><p>-Carl</p></div><h3><a id=14538 href="#14538">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 3:18:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; It&apos;d be pretty hard to get any closer<br/>&gt; &gt;&gt; &gt;&gt; to consistency (as Paul Hahn defines it) than Keenan has.<br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; -Carl<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Why? I could see inconsistent tunings getting a higher score<br/>than<br/>&gt; &gt;&gt; &gt;consistent tunings by Keenan&apos;s measure.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Once again, Keenan&apos;s measure is just Paul&apos;s without the decimal<br/>&gt; &gt;&gt; part lopped off.<br/>&gt; &gt;<br/>&gt; &gt;floor(1 / Pepper ambiguity) you mean?<br/>&gt;<br/>&gt; That&apos;s what I meant, but it&apos;s off by a factor of 2 and still only<br/>&gt; close.  Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt; size S and reporting<br/>&gt;<br/>&gt; E/(S-E)<br/>&gt;<br/>&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>worst<br/>&gt; interval&apos;s error goes into half a step; floor(S/2E).</p><p>Paul H.&apos;s definition:</p><p>&quot;N-TET is level-P consistent at the M-limit IFF:<br/>For any triad whose three intervals can each be expressed as a<br/>product of P or fewer primary (or &quot;consonant&quot;) M-limit intervals, the<br/>sum of the N-TET approximations of the two smaller intervals equals<br/>the (N-TET) approximation of their sum (the larger interval).&quot;</p><p>The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>consistency level if you just use all the best approximations to the<br/>consonant intervals (which is what Keenan ambiguity uses). This is<br/>made clear when you consider that some ETs are inconsistent, so must<br/>have consistency level below 1, but your formula will never give<br/>anything below 1 when you use the same inputs as into Keenan<br/>ambiguity.</p><p>Instead, what Hahn uses in his formula are the signed errors of the<br/>N:1 consonances. The difference of the highest signed error and the<br/>lowest signed error is what goes into half a step X times, where X is<br/>the Hahn consistency level.</p></div><h3><a id=14540 href="#14540">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 3:33:43 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; &gt;&gt; I did finally get it right.<br/>&gt; &gt;&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; &gt;&gt; -Carl<br/>&gt; &gt;&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;&gt; &gt;Where?<br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; <a href="http://groups.yahoo.com/group/tuning-math/message/14422">http://groups.yahoo.com/group/tuning-math/message/14422</a><br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; -C.<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;That&apos;s incorrect, as I remarked in my reply. Are you caught up?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; What&apos;s incorrect about it?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; -Carl<br/>&gt; &gt;<br/>&gt; &gt;I guess you haven&apos;t followed the thread.  Keenan and Hahn use a<br/>&gt; &gt;different set of intervals as input into their formulae.<br/>&gt;<br/>&gt; What I saw were brief, vague assertions from you that this was<br/>&gt; the case.<br/>&gt;<br/>&gt; -Carl</p><p>(See <a href="http://groups.yahoo.com/group/tuning-math/message/14538">http://groups.yahoo.com/group/tuning-math/message/14538</a>)</p></div><h3><a id=14541 href="#14541">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 3:57:51 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt;&gt; size S and reporting<br/>&gt;&gt;<br/>&gt;&gt; E/(S-E)<br/>&gt;&gt;<br/>&gt;&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>&gt;&gt; worst interval&apos;s error goes into half a step; floor(S/2E).<br/>//<br/>&gt;The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>&gt;consistency level if you just use all the best approximations to the<br/>&gt;consonant intervals (which is what Keenan ambiguity uses).<br/>&gt;This is made clear when you consider that some ETs are inconsistent,<br/>&gt;so must have consistency level below 1, but your formula will never<br/>&gt;give anything below 1 when you use the same inputs as into Keenan<br/>&gt;ambiguity.</p><p>For example, 6:5 is the least-accurate 5-limit interval in 12-tET.<br/>The best approximation is 3 steps, or 300 cents.  The error is<br/>15.6 cents, so floor(S/2E) is 3.  In the 11-limit, the least-accurate<br/>interval is, I think, 11:8, with an error of 48.7 cents.<br/>Floor(S/2E) is now zero.</p><p>&gt;Instead, what Hahn uses in his formula are the signed errors of the<br/>&gt;N:1 consonances. The difference of the highest signed error and the<br/>&gt;lowest signed error is what goes into half a step X times, where X is<br/>&gt;the Hahn consistency level.</p><p>That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>intervals.</p><p>-Carl</p></div><h3><a id=14544 href="#14544">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/24/2006 4:17:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:</p><p>&gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt; intervals.</p><p>Consistency is equivalent to the patent val always giving the best<br/>tuning for every element of the appropriate tonality diamond. How do<br/>you get that?</p></div><h3><a id=14545 href="#14545">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 4:22:09 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt; &gt;&gt; size S and reporting<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; E/(S-E)<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>&gt; &gt;&gt; worst interval&apos;s error goes into half a step; floor(S/2E).<br/>&gt; //<br/>&gt; &gt;The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>&gt; &gt;consistency level if you just use all the best approximations to the<br/>&gt; &gt;consonant intervals (which is what Keenan ambiguity uses).<br/>&gt; &gt;This is made clear when you consider that some ETs are inconsistent,<br/>&gt; &gt;so must have consistency level below 1, but your formula will never<br/>&gt; &gt;give anything below 1 when you use the same inputs as into Keenan<br/>&gt; &gt;ambiguity.<br/>&gt;<br/>&gt; For example, 6:5 is the least-accurate 5-limit interval in 12-tET.<br/>&gt; The best approximation is 3 steps, or 300 cents.  The error is<br/>&gt; 15.6 cents, so floor(S/2E) is 3.  In the 11-limit, the least-accurate<br/>&gt; interval is, I think, 11:8, with an error of 48.7 cents.<br/>&gt; Floor(S/2E) is now zero.</p><p>Consider the 7-limit. The worst errors, for 7:4 and 7:6, are over 30 cents, so floor (S/2E) is again zero. But 12-equal *is* consistent in the 7-limit!</p><p>&gt; &gt;Instead, what Hahn uses in his formula are the signed errors of the<br/>&gt; &gt;N:1 consonances. The difference of the highest signed error and the<br/>&gt; &gt;lowest signed error is what goes into half a step X times, where X is<br/>&gt; &gt;the Hahn consistency level.<br/>&gt;<br/>&gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt; intervals.</p><p>It&apos; equivalent to checking all the triads direcly as per his definition. It&apos;s *not* equivalent to plugging all the intervals&apos; errors into his clever formula which takes the largest difference between the signed errors of the N:1 intervals as input. Hopefully I showed that with the 12-equal 7-limit example above, but many other examples can be found.</p></div><h3><a id=14548 href="#14548">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 4:49:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt;&gt; intervals.<br/>&gt;<br/>&gt;Consistency is equivalent to the patent val always giving the best<br/>&gt;tuning for every element of the appropriate tonality diamond.</p><p>In odd limits &gt; 7, that&apos;s only true if:</p><p>() The val maps odd composites directly.<br/>() There is &gt; level-1 consistency.</p><p>&gt;How do you get that?</p><p>I forget why it works, but it does.</p><p>-Carl</p></div><h3><a id=14549 href="#14549">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 4:50:19 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt;&gt; &gt;&gt; size S and reporting<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; E/(S-E)<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>&gt;&gt; &gt;&gt; worst interval&apos;s error goes into half a step; floor(S/2E).<br/>&gt;&gt; //<br/>&gt;&gt; &gt;The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>&gt;&gt; &gt;consistency level if you just use all the best approximations to the<br/>&gt;&gt; &gt;consonant intervals (which is what Keenan ambiguity uses).<br/>&gt;&gt; &gt;This is made clear when you consider that some ETs are inconsistent,<br/>&gt;&gt; &gt;so must have consistency level below 1, but your formula will never<br/>&gt;&gt; &gt;give anything below 1 when you use the same inputs as into Keenan<br/>&gt;&gt; &gt;ambiguity.<br/>&gt;&gt;<br/>&gt;&gt; For example, 6:5 is the least-accurate 5-limit interval in 12-tET.<br/>&gt;&gt; The best approximation is 3 steps, or 300 cents.  The error is<br/>&gt;&gt; 15.6 cents, so floor(S/2E) is 3.  In the 11-limit, the least-accurate<br/>&gt;&gt; interval is, I think, 11:8, with an error of 48.7 cents.<br/>&gt;&gt; Floor(S/2E) is now zero.<br/>&gt;<br/>&gt;Consider the 7-limit. The worst errors, for 7:4 and 7:6, are over 30<br/>&gt;cents, so floor (S/2E) is again zero. But 12-equal *is* consistent in<br/>&gt;the 7-limit!<br/>&gt;<br/>&gt;&gt; &gt;Instead, what Hahn uses in his formula are the signed errors of the<br/>&gt;&gt; &gt;N:1 consonances. The difference of the highest signed error and the<br/>&gt;&gt; &gt;lowest signed error is what goes into half a step X times, where X is<br/>&gt;&gt; &gt;the Hahn consistency level.<br/>&gt;&gt;<br/>&gt;&gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt;&gt; intervals.<br/>&gt;<br/>&gt;It&apos; equivalent to checking all the triads direcly as per his<br/>&gt;definition. It&apos;s *not* equivalent to plugging all the intervals&apos;<br/>&gt;errors into his clever formula which takes the largest difference<br/>&gt;between the signed errors of the N:1 intervals as input. Hopefully I<br/>&gt;showed that with the 12-equal 7-limit example above, but many other<br/>&gt;examples can be found.</p><p>Ah.</p><p>-Carl</p></div><h3><a id=14550 href="#14550">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 4:55:40 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;wallyesterpaulrus&quot; &lt;perlich@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; &gt;&gt; Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt; &gt; &gt;&gt; size S and reporting<br/>&gt; &gt; &gt;&gt;<br/>&gt; &gt; &gt;&gt; E/(S-E)<br/>&gt; &gt; &gt;&gt;<br/>&gt; &gt; &gt;&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>&gt; &gt; &gt;&gt; worst interval&apos;s error goes into half a step; floor(S/2E).<br/>&gt; &gt; //<br/>&gt; &gt; &gt;The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>&gt; &gt; &gt;consistency level if you just use all the best approximations to the<br/>&gt; &gt; &gt;consonant intervals (which is what Keenan ambiguity uses).<br/>&gt; &gt; &gt;This is made clear when you consider that some ETs are inconsistent,<br/>&gt; &gt; &gt;so must have consistency level below 1, but your formula will never<br/>&gt; &gt; &gt;give anything below 1 when you use the same inputs as into Keenan<br/>&gt; &gt; &gt;ambiguity.<br/>&gt; &gt;<br/>&gt; &gt; For example, 6:5 is the least-accurate 5-limit interval in 12-tET.<br/>&gt; &gt; The best approximation is 3 steps, or 300 cents.  The error is<br/>&gt; &gt; 15.6 cents, so floor(S/2E) is 3.  In the 11-limit, the least-accurate<br/>&gt; &gt; interval is, I think, 11:8, with an error of 48.7 cents.<br/>&gt; &gt; Floor(S/2E) is now zero.</p><p>Carl, I let this one slip by and fool me! Floor(100/97,4) is not zero, it&apos;s one!</p><p>&gt; Consider the 7-limit. The worst errors, for 7:4 and 7:6, are over 30 cents, so floor (S/2E) is again zero.</p><p>It&apos;s again *one*.</p><p>&gt; But 12-equal *is* &gt;consistent in the &gt;7-limit!</p><p>And it&apos;s inconsistent in the 11-limit, where your formula again gives a one. So what you&apos;re calculating is *not* telling you the consistency or consistency level.</p><p>&gt; &gt; &gt;Instead, what Hahn uses in his formula are the signed errors of the<br/>&gt; &gt; &gt;N:1 consonances. The difference of the highest signed error and the<br/>&gt; &gt; &gt;lowest signed error is what goes into half a step X times, where X is<br/>&gt; &gt; &gt;the Hahn consistency level.<br/>&gt; &gt;<br/>&gt; &gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt; &gt; intervals.<br/>&gt;<br/>&gt; It&apos; equivalent to checking all the triads direcly as per his definition. It&apos;s *not* equivalent to plugging all the intervals&apos; errors into his clever formula which takes the largest difference between the signed errors of the N:1 intervals as input. Hopefully I showed that with the 12-equal 7-limit example above, but many other examples can be found.<br/>&gt;</p></div><h3><a id=14551 href="#14551">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 5:01:39 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt; &gt;&gt; Keenan&apos;s taking the worst interval&apos;s error, E, and the step<br/>&gt;&gt; &gt; &gt;&gt; size S and reporting<br/>&gt;&gt; &gt; &gt;&gt;<br/>&gt;&gt; &gt; &gt;&gt; E/(S-E)<br/>&gt;&gt; &gt; &gt;&gt;<br/>&gt;&gt; &gt; &gt;&gt; Paul H.&apos;s consistency is the integer part of how many times the<br/>&gt;&gt; &gt; &gt;&gt; worst interval&apos;s error goes into half a step; floor(S/2E).<br/>&gt;&gt; &gt; //<br/>&gt;&gt; &gt; &gt;The calculation you&apos;re referring to doesn&apos;t give you Hahn&apos;s<br/>&gt;&gt; &gt; &gt;consistency level if you just use all the best approximations to<br/>&gt;&gt; &gt; &gt;the consonant intervals (which is what Keenan ambiguity uses).<br/>&gt;&gt; &gt; &gt;This is made clear when you consider that some ETs are inconsistent,<br/>&gt;&gt; &gt; &gt;so must have consistency level below 1, but your formula will never<br/>&gt;&gt; &gt; &gt;give anything below 1 when you use the same inputs as into Keenan<br/>&gt;&gt; &gt; &gt;ambiguity.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt; For example, 6:5 is the least-accurate 5-limit interval in 12-tET.<br/>&gt;&gt; &gt; The best approximation is 3 steps, or 300 cents.  The error is<br/>&gt;&gt; &gt; 15.6 cents, so floor(S/2E) is 3.  In the 11-limit, the least-accurate<br/>&gt;&gt; &gt; interval is, I think, 11:8, with an error of 48.7 cents.<br/>&gt;&gt; &gt; Floor(S/2E) is now zero.<br/>&gt;<br/>&gt;Carl, I let this one slip by and fool me! Floor(100/97,4) is not zero,<br/>&gt;it&apos;s one!<br/>&gt;<br/>&gt;&gt; Consider the 7-limit. The worst errors, for 7:4 and 7:6, are over 30<br/>&gt;cents, so floor (S/2E) is again zero.<br/>&gt;<br/>&gt;It&apos;s again *one*.</p><p>Heh.</p><p>&gt;&gt; But 12-equal *is* consistent in the 7-limit!<br/>&gt;<br/>&gt;And it&apos;s inconsistent in the 11-limit, where your formula again gives<br/>&gt;a one. So what you&apos;re calculating is *not* telling you the consistency<br/>&gt;or consistency level.</p><p>True that.</p><p>-Carl</p></div><h3><a id=14552 href="#14552">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/24/2006 5:08:43 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:</p><p>&gt; &gt;Consistency is equivalent to the patent val always giving the best<br/>&gt; &gt;tuning for every element of the appropriate tonality diamond.<br/>&gt;<br/>&gt; In odd limits &gt; 7, that&apos;s only true if:<br/>&gt;<br/>&gt; () The val maps odd composites directly.<br/>&gt; () There is &gt; level-1 consistency.</p><p>I don&apos;t know what you mean. What is an example?</p></div><h3><a id=14553 href="#14553">ðŸ”—</a>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>2/24/2006 5:09:06 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; That&apos;s true, but it&apos;s equivalent, he claims, to checking all the<br/>&gt; &gt;&gt; intervals.<br/>&gt; &gt;<br/>&gt; &gt;Consistency is equivalent to the patent val always giving the best<br/>&gt; &gt;tuning for every element of the appropriate tonality diamond.<br/>&gt;<br/>&gt; In odd limits &gt; 7, that&apos;s only true if:<br/>&gt;<br/>&gt; () The val maps odd composites directly.</p><p>What does that mean?</p><p>&gt; () There is &gt; level-1 consistency.</p><p>No, it&apos;s true even if the consistency level is just 1.</p></div><h3><a id=14555 href="#14555">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/24/2006 6:25:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;Consistency is equivalent to the patent val always giving the best<br/>&gt;&gt; &gt;tuning for every element of the appropriate tonality diamond.<br/>&gt;&gt;<br/>&gt;&gt; In odd limits &gt; 7, that&apos;s only true if:<br/>&gt;&gt;<br/>&gt;&gt; () The val maps odd composites directly.<br/>&gt;&gt; () There is &gt; level-1 consistency.<br/>&gt;<br/>&gt;I don&apos;t know what you mean. What is an example?</p><p>Do vals usually map odd composites like 9?  If not, level-1<br/>9-limit consistency would actually require that the 3-limit<br/>be level-2 consistent to make your statement true.  Yes?</p><p>-Carl</p></div>