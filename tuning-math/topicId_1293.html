<a href="/tuning-math">back to list</a><h1>Relative error theorems</h1><h3>genewardsmith@juno.com</h3><span>10/19/2001 7:02:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Relative Error Theorem:</p><p>Let l be an odd number, h a val and f a note of the p-limit, where p<br/>is the largest prime less than or equal to l. Define the error in<br/>relative cents of the val h for the note f to be<br/>e(h, f) = 100*(h(f) - h(2) log2(f)). Define the badness measures<br/>e_inf(h, l) = max |e(h, r)|, e_2(h, l) = sqrt(sum e(h, r)^2), and<br/>e_1(h, l) = sum |e(h, r)|, where in each case we take r to range over<br/>rational numbers p/q where p/q is reduced to its lowest terms,<br/>1&lt;p/q, and 1&lt;=p,q&lt;=l. Let j and k be two vals such that j+k=h; we<br/>then have</p><p>e_inf(h, l) &lt;= e_inf(j, l) + e_inf(k, l)</p><p>e_2(h, l) &lt;= e_2(j, l) + e_2(k, l)</p><p>e_1(h, l) &lt;= e_1(j, l) + e_1(k, l)</p><p>Proof:  It follows from its definition and the multiplicative<br/>linearity of the logarithm that relative error is linear, ie that<br/>e(j+k, l) = e(j, l) + e(k, l). We may define error vectors by<br/>defining a fixed ordering of the numbers r, so that if vj is the<br/>vector of errors for j, vk is the vector of errors for k, and vh for<br/>h=j+k, the linearity of relative error entails that vh = vj+vk. We<br/>may consider these vectors to reside in a normed vector space with<br/>norm L_inf (the maximum of the absolute values of the coordinates),<br/>L_2 (Euclidean norm), or L_1 (the sum of the absolute values.) These<br/>norms are respectively e_inf, e_2 and e_1; from this and the triangle<br/>inequality for each of these norms, the result follows.</p><p>Definition: Associated generator</p><p>Let j and k be valid vals, such that a=j(2) &lt; b=k(2) are distinct, so<br/>that j and k are 2-distinct. We reduce to lowest terms by dividing<br/>through by the gcd, so that if d = gcd(a, b), q=a/d, s=b/d. We form<br/>the fraction s/q, and define r/p by the condition that p is the least<br/>denominator for which we have |rq - sp| = 1. Exchanging the names of<br/>j and k, q and s, p and r if necessary we define matters so that<br/>rq - sp = 1. We then define the associated generator val g as<br/>j(2) k - k(2) j, and the reduced generator val as g/d.</p><p>Theorem: Let l, h, f, and p be as above. Define<br/>com_inf(h, l) = max |h(r)|, com_2(h, l) = sqrt(sum h(r)^2) and<br/>com_1(h, l) = sum |h(r)|, where r is defined as before. If j and k are<br/>2-distinct valid vals, and if g is the associated generator val, then</p><p>com_inf(g, l) &lt;= (k(2) e_inf(j, l) + j(2) e_inf(k, l))/100 (Graham&apos;s<br/>complexity)</p><p>com_2(g, l) &lt;= (k(2) e_2(j, l) + j(2) e_2(k, l))/100</p><p>com_1(g, l) &lt;= (k(2) e_1(j, l) + j(2) e_1(k, l))/100</p><p>Proof:</p><p>Defining p and f as before, we define a linear temperament associated<br/>to j and k by A^j(f) B^k(f) for some fixed A and B. If we set<br/>G = A^p  B^r and E = A^q B^s we may also express the linear<br/>temperament in terms of G and E, since the transformation matrix<br/>[[p r] [q s]]has determinant -1, and hence is invertible as an<br/>integral matrix. Inverting it, we find that in terms of G and E, the<br/>approximation to f is given by G^h(f) E^e(f), where h = q k - s j is<br/>the reduced generator val and e = r j - p k is the interval of<br/>equivalence val. (In the particular case of the j+k et, we have<br/>j(2)+k(2)= n, A = B = 2^(1/n), so that G= 2^((p+r)/n) is the<br/>generator within an interval of equivalence of<br/>E = 2^((q+s)/n) = 2^(1/d).)</p><p>We have</p><p>e(j, f) = 100(j(f) - j(2) log2(f)),</p><p>e(k, f) = 100(k(f) - k(2) log2(f)).</p><p>From this we get</p><p>j(f) = j(2) log2(f) + e(j, f)/100,</p><p>k(f) = k(2) log2(f) + e(k, f)/100</p><p>Then</p><p>h(f) = j(2) k - k(2) j = j(2) (k(2) log2(f) + e(k, f)/100) -<br/>k(2) (j(2) log2(f) + e(j, f)1/00) = (e(j, f) + e(k, f))/100.</p><p>Forming vectors as before and applying the triangle inequality, we<br/>get the theorem.</p></div>