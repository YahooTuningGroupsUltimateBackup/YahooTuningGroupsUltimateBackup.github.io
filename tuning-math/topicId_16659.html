<a href="/tuning-math">back to list</a><h1>for Gene - computing harmonic entropy</h1><h3><a id=16659 href="#16659">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>6/22/2007 1:57:24 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene,</p><p>Have you seen this?</p><p>-Carl</p><p>________________________________________________________________________</p><p>Date: Sun, 1 Feb 2004.<br/>Subject: Re: Newbie question: How is harmonic entropy computed?</p><p>&gt; 1) I must be missing something, but I did not find any clear<br/>&gt; indication of a how harmonic entropy is computed.  What I see is a lot<br/>&gt; of nice figures and diagrams but I am still looking for clues to how<br/>&gt; they are obtained...  I understand that it is directly related to<br/>&gt; Shannon&apos;s entropy definition Xlog(X), where X is a probability density<br/>&gt; function, but could not derive an algorithm by myself.  If available,<br/>&gt; could anyone give a pointer to a place where the computations are<br/>&gt; explained more precisely?  The best introductory site I could find<br/>&gt; is at &lt; <a href="http://tonalsoft.com/enc/harmentr.htm">http://tonalsoft.com/enc/harmentr.htm</a> &gt; but this page is<br/>&gt; lacking the maths I am looking for.</p><p>The probability function you are looking for begins as a three-parameter<br/>function, where the first parameter (call it j) is an index into your<br/>big series of ratios (Farey series or n*d &lt; 10000 or 65536 or whatever)<br/>and the second (call it i) is the interval actually being heard.</p><p>We define p(j,i,s) as 1/(s*sqrt(2*pi)) times the integral from<br/>mediant(j-1,j) to mediant(j,j+1) of</p><p>exp( -(cents(t)-cents(i))^2 / (2s^2) ) dt</p><p>IMPORTANT: the units of t should be logarithmic, e.g., cents.</p><p>The function you&apos;re integrating is nearly constant, so it makes little<br/>difference if we replace this with</p><p>(mediant(j+1,j)-mediant(j,j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2) )</p><p>and a constant of proportionality determined such that</p><p>SUM (p(j,i,s)) = 1<br/>j</p><p>It apparently also makes little difference in the result, if for the<br/>partitioning, we replace the mediants with means (which will allow us to<br/>use voronoi cells in the generalization to higher dimensions), giving</p><p>1/2*(cents(j+1)-cents(j-1))*exp( -(cents(j)-cents(i))^2 / (2s^2)</p><p>In any case, the harmonic entropy HE(i,s) is then simply</p><p>-SUM (p(j,i,s) log(p(j,i,s)))<br/>j</p><p>&gt; 2) Actually I am interested in finding a way to measure the<br/>&gt; consonance/dissonance of complex chords: not only for dyads, triads<br/>&gt; or tetrads, but for an arbitrary multi-component cluster.  I<br/>&gt; understand that in this case the decomposition into individual dyads<br/>&gt; does not lead to a correct measure of harmonic entropy, as stated at<br/>&gt; &lt; <a href="http://tonalsoft.com/td/erlich/entropy.htm">http://tonalsoft.com/td/erlich/entropy.htm</a> &gt;.  How did you obtain<br/>&gt; the HE for triads and tetrads ?</p><p>I haven&apos;t yet, but I&apos;ve described how the calculation is to be done,<br/>shown some partitionings for triads using Voronoi cells, and how the<br/>sizes of these partitions follow the same dependence on the product of<br/>the numbers in the three-term ratio as, in the dyadic case, the mediants<br/>follow on the product of the numbers in the ordinary ratio.  So some<br/>features of triadic and tetradic harmonic entropy are already known with<br/>a high degree of certainty.</p><p>&gt; 3) Last question, how closely is harmonic entropy related to Barlow&apos;s<br/>&gt; harmonicity? Harmonicity seems less mathematically founded than<br/>&gt; harmonic entropy, but it has a straightforward generalization to<br/>&gt; complex chords by substituting LCM/GCD to the original p/q ratio.</p><p>For a two-note chord, LCM/GCD gives you p*q, not p/q, so I&apos;m not sure I<br/>understand this substitution.  But the harmonic entropy calculations and<br/>graphs seem to be saying that for any selection of not-too-complex<br/>ratios or chords, the product of the numbers in the ratio (be it two-<br/>term, three-term . . .) gives the correct dissonance ranking, and in<br/>fact the log of the product gives the approximate relative entropy.  See<br/>the &quot;files&quot; folder of this group.</p><p>But that doesn&apos;t explain what you then *do* with the ratio in Barlow&apos;s<br/>Harmonicity, which doesn&apos;t seem related to how we perceive consonance,<br/>especially in that it assumes that simple ratios have zero tolerance for<br/>mistuning, and can&apos;t even accept irrational inputs!  This substitution<br/>you mention, to the extent that it&apos;s valid, really has little to do with<br/>the rest of Barlow&apos;s formulation, correct?</p><p>Best,<br/>Paul</p></div><h3><a id=16660 href="#16660">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>6/22/2007 1:11:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;<br/>&gt; Gene,<br/>&gt;<br/>&gt; Have you seen this?</p><p>Doesn&apos;t seem familiar. Where&apos;s it from?</p></div><h3><a id=16661 href="#16661">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>6/23/2007 1:36:20 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 01:11 PM 6/22/2007, you wrote:<br/>&gt;--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@...&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; Gene,<br/>&gt;&gt;<br/>&gt;&gt; Have you seen this?<br/>&gt;<br/>&gt;Doesn&apos;t seem familiar. Where&apos;s it from?</p><p>Beats me.  Is it something you wanted to know?</p><p>-Carl</p></div><h3><a id=16662 href="#16662">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>6/23/2007 8:09:14 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Reason I ask is, I feel discussion here often gets bogged<br/>down in minutia like which averaging function to use (it doesn&apos;t<br/>matter much) or which lattice distance to use (ditto), while<br/>the big questions go untouched.  I&apos;m far from guiltless in this.</p><p>What would it take to calculate the entropy of a 7-note chord?<br/>I should think we&apos;ll need:</p><p>* A space in which 7-ads can be embedded.<br/>* A way to partition the space into cells associated with<br/>each chord.<br/>* A generalization of the gaussian distribution to said space.<br/>* A way to integrate under said distribution between cell<br/>boundaries.</p><p>-Carl</p><p>&gt;&gt;&gt; Gene,<br/>&gt;&gt;&gt;<br/>&gt;&gt;&gt; Have you seen this?<br/>&gt;&gt;<br/>&gt;&gt;Doesn&apos;t seem familiar. Where&apos;s it from?<br/>&gt;<br/>&gt;Beats me.  Is it something you wanted to know?<br/>&gt;<br/>&gt;-Carl</p></div><h3><a id=16663 href="#16663">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>6/27/2007 8:14:23 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>For triads, Paul tried a triangular dyads plot partitioned<br/>by voronoi cells.  But after the n*d product worked so well<br/>to estimate the mediant-mediant widths for dyads, I believe<br/>he abandoned voronoi cells in favor of a*b*c for triads.</p><p>-Carl</p><p>At 08:09 PM 6/23/2007, you wrote:<br/>&gt;Reason I ask is, I feel discussion here often gets bogged<br/>&gt;down in minutia like which averaging function to use (it doesn&apos;t<br/>&gt;matter much) or which lattice distance to use (ditto), while<br/>&gt;the big questions go untouched.  I&apos;m far from guiltless in this.<br/>&gt;<br/>&gt;What would it take to calculate the entropy of a 7-note chord?<br/>&gt;I should think we&apos;ll need:<br/>&gt;<br/>&gt;* A space in which 7-ads can be embedded.<br/>&gt;* A way to partition the space into cells associated with<br/>&gt;each chord.<br/>&gt;* A generalization of the gaussian distribution to said space.<br/>&gt;* A way to integrate under said distribution between cell<br/>&gt;boundaries.<br/>&gt;<br/>&gt;-Carl<br/>&gt;<br/>&gt;&gt;&gt;&gt; Gene,<br/>&gt;&gt;&gt;&gt;<br/>&gt;&gt;&gt;&gt; Have you seen this?<br/>&gt;&gt;&gt;<br/>&gt;&gt;&gt;Doesn&apos;t seem familiar. Where&apos;s it from?<br/>&gt;&gt;<br/>&gt;&gt;Beats me.  Is it something you wanted to know?<br/>&gt;&gt;<br/>&gt;&gt;-Carl</p></div>