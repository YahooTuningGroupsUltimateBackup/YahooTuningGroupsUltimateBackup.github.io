<a href="/tuning-math">back to list</a><h1>tiptop.py constraint matrices</h1><h3>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>9/10/2012 6:13:54 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>To Keenan and others tuned into this: I spent some time today going<br/>over your <a href="http://tiptop.py">tiptop.py</a> code. I get what&apos;s going on at a high level, and<br/>what you&apos;re doing with &quot;constraint matrices&quot;, which look basically<br/>like V-maps that you&apos;ve chosen to help find the generator tuning map<br/>for all the tunings that split the error evenly among r primes, where<br/>r is the rank of the temperament. Then you pick the generator maps<br/>with the lowest max weighted prime error as evaluated on the original<br/>mapping matrix, and then these serve as endpoints to the TOP tuning<br/>polytope.</p><p>After this, you do some magic involving A_back and b_back that I<br/>couldn&apos;t figure out at all. What&apos;s going on here? I&apos;m talking about<br/>this chunk of code:</p><p>  # Used to get &quot;back&quot; to the solution x of the original<br/>  # problem at the end<br/>  A_back = numpy.eye(A.shape[1]) #diagonal matrix of the same rank as<br/>the mapping matrix<br/>  b_back = numpy.zeros([A.shape[1], 1]) #zero matrix</p><p>  while len(xs) &gt; 1: #keep doing this until we only have one candidate</p><p>    b = b - A * xs[0] #error map<br/>    X = numpy.hstack([xs[i] - xs[0] for i in range(1,len(xs))]) #all<br/>generator maps<br/>    A = A * X # all tuning maps</p><p>    b_back = A_back * xs[0] + b_back #make b_back equal to<br/>    A_back = A_back * X</p><p>    xs = candidates(A, b, tol, num_already)<br/>    num_already += A.shape[1] + 1</p><p>What the heck is this?</p><p>-Mike</p></div><h3>Keenan Pepper &#x3C;keenanpepper@gmail.com&#x3E;</h3><span>9/14/2012 2:11:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; To Keenan and others tuned into this: I spent some time today going<br/>&gt; over your <a href="http://tiptop.py">tiptop.py</a> code. I get what&apos;s going on at a high level, and<br/>&gt; what you&apos;re doing with &quot;constraint matrices&quot;, which look basically<br/>&gt; like V-maps that you&apos;ve chosen to help find the generator tuning map<br/>&gt; for all the tunings that split the error evenly among r primes, where<br/>&gt; r is the rank of the temperament. Then you pick the generator maps<br/>&gt; with the lowest max weighted prime error as evaluated on the original<br/>&gt; mapping matrix, and then these serve as endpoints to the TOP tuning<br/>&gt; polytope.<br/>&gt;<br/>&gt; After this, you do some magic involving A_back and b_back that I<br/>&gt; couldn&apos;t figure out at all. What&apos;s going on here? I&apos;m talking about<br/>&gt; this chunk of code:<br/>&gt;<br/>&gt;   # Used to get &quot;back&quot; to the solution x of the original<br/>&gt;   # problem at the end<br/>&gt;   A_back = numpy.eye(A.shape[1]) #diagonal matrix of the same rank as<br/>&gt; the mapping matrix<br/>&gt;   b_back = numpy.zeros([A.shape[1], 1]) #zero matrix<br/>&gt;<br/>&gt;   while len(xs) &gt; 1: #keep doing this until we only have one candidate<br/>&gt;<br/>&gt;     b = b - A * xs[0] #error map<br/>&gt;     X = numpy.hstack([xs[i] - xs[0] for i in range(1,len(xs))]) #all<br/>&gt; generator maps<br/>&gt;     A = A * X # all tuning maps<br/>&gt;<br/>&gt;     b_back = A_back * xs[0] + b_back #make b_back equal to<br/>&gt;     A_back = A_back * X<br/>&gt;<br/>&gt;     xs = candidates(A, b, tol, num_already)<br/>&gt;     num_already += A.shape[1] + 1<br/>&gt;<br/>&gt; What the heck is this?</p><p>Okay, so you try out all these tunings that might minimize the Linf norm, and some subset of them actually do; those are the &quot;candidates&quot;, which in the first step are the vertices of the TOP polytope. The TIP-TOP tuning is always in the affine subspace spanned by the candidates. In the second iteration, we want to do the same thing again, but we&apos;re constrained to this affine subspace, and now we&apos;re trying to minimize the second-greatest error rather than the greatest error. In the third iteration we&apos;re restricted to a smaller affine subspace and are trying to minimize the third-greatest error, and so on, until there is only one candidate tuning left (the TIP-TOP).</p><p>The &quot;candidates&quot; function just finds the best approximate solutions to Ax=b, where &quot;best&quot; means the Linf norm of Ax-b is minimized. We want to reuse this function for all the iterations, so for all the later iterations we&apos;re giving it a transformed version of the problem. Look at the return value of the &quot;tiptop&quot; function - it&apos;s (A_back * xs[0] + b_back), so that&apos;s the real tuning vector x we want (not xs[0] itself).</p><p>Note that the matrix capital X consists of a set of vectors that span the linear subspace parallel to the affine subspace we&apos;re restricting ourselves to, so by updating A to A*X and b to b - A*xs[0], we&apos;re giving the &quot;candidates&quot; function a new version of the problem that is restricted to the affine subspace, and the only problem is that the xs it returns are in these funny coordinates now. A_back and b_back just keep track of the affine transformation needed to convert those xs back into the standard coordinates. You need a new affine transformation for each iteration but they are accumulated into the single affine transformation (A_back, b_back) in order not to keep a big list of A_back&apos;s and b_back&apos;s.</p><p>Keenan</p></div>