<a href="/tuning-math">back to list</a><h1>More lists</h1><h3><a id=1686 href="#1686">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/6/2001 12:48:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve updated the script at &lt;<a href="http://www.microtonal.co.uk/temper.html">http://www.microtonal.co.uk/temper.html</a>&gt; to<br/>produce files using Dave Keenan&apos;s new figure of demerit.  That is</p><p>width**2 * math.exp((error/self.stdError*3)**2)</p><p>The stdError is from some complexity calculations we did before.  I forget<br/>what, but it&apos;s 17 cents.  The results are at</p><p>&lt;<a href="http://www.microtonal.co.uk/limit5.gauss">http://www.microtonal.co.uk/limit5.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit7.gauss">http://www.microtonal.co.uk/limit7.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit9.gauss">http://www.microtonal.co.uk/limit9.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit11.gauss">http://www.microtonal.co.uk/limit11.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit13.gauss">http://www.microtonal.co.uk/limit13.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit15.gauss">http://www.microtonal.co.uk/limit15.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit17.gauss">http://www.microtonal.co.uk/limit17.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit19.gauss">http://www.microtonal.co.uk/limit19.gauss</a>&gt;<br/>&lt;<a href="http://www.microtonal.co.uk/limit21.gauss">http://www.microtonal.co.uk/limit21.gauss</a>&gt;</p><p>They seem to make good enough sense.  I haven&apos;t taken the training wheels<br/>off completely, but loosened them as far as I did for the<br/>microtemperaments.  The other files haven&apos;t been updated, and I&apos;m not even<br/>calculating the MOS-rated list any more.</p><p>I&apos;ve also changed the program to print out equivalences between<br/>second-order ratios instead of unison vectors.  That means the higher<br/>limits have a huge number of equivalences.  For example, at the bottom of<br/>the 21-limit list there&apos;s an 11-limit unique temperament consistent with<br/>111 and 282.  It has a complexity of 174 and all intervals to within 2<br/>cents of just.  With something that complex, are there any second-order<br/>equivalences?  Yes, lots.  Including one interval that can be taken 11<br/>different ways:</p><p>144:143 =~ 196:195 =~ 171:170 =~ 210:209 =~ 225:224 =~ 209:208 =~ 221:220<br/>=~ 170:169 =~ 273:272 =~ 289:288 =~ 190:189</p><p>and that picked out of 197 lines of numerical vomit.  I could clean it up,<br/>but I don&apos;t know if I should.  If anybody thought the extended 21-limit<br/>was pretty, they can&apos;t have been paying attention.</p><p>It should be possible to get some unison vectors without torsion from this<br/>list!  If the temperament&apos;s second-order unique, I&apos;ll have to use the<br/>original method.  Some 5-limit temperaments are, but they aren&apos;t a problem<br/>anyway.  A few 7-limit temperaments are too, notably including shrutar.<br/>Ennealimmal for all its complexity has</p><p>49:40 =~ 60:49<br/>50:49 =~ 49:48</p><p>One problem with calculating the unison vectors from these equivalences is<br/>I&apos;d have to check they were linearly independent without using Numeric.<br/>Or move the generating function to <a href="http://vectors.py">vectors.py</a>.  But I don&apos;t know if I&apos;ll<br/>bother, because the equivalences are the important things anyway.</p><p>Another idea would be to take all the intervals between second-order<br/>intervals below a certain size, and use them as unison vectors to generate<br/>temperaments.  I might try that.</p><p>Oh yes.  Seeing as a 7-limit microtemperament is now causing something of<br/>a storm, notice that the top 11-limit one is 26+46 (complexity of 30,<br/>errors within 2.5 cents).  And the simplest with all errors below a cent<br/>is 118+152 (complexity of 74).</p><p>                               Graham</p></div><h3><a id=1693 href="#1693">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/6/2001 6:45:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; I&apos;ve updated the script at<br/>&lt;<a href="http://www.microtonal.co.uk/temper.html">http://www.microtonal.co.uk/temper.html</a>&gt; to<br/>&gt; produce files using Dave Keenan&apos;s new figure of demerit.  That is<br/>&gt;<br/>&gt; width**2 * math.exp((error/self.stdError*3)**2)</p><p>I thought Dave Keenan wanted to use Gene&apos;s &quot;step&quot; measure. In<br/>addition, I think it should be weighted to favor the simpler<br/>consonances.<br/>&gt;<br/>&gt; The stdError is from some complexity calculations we did before.  I<br/>forget<br/>&gt; what, but it&apos;s 17 cents.  The results are at<br/>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit5.gauss">http://www.microtonal.co.uk/limit5.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit7.gauss">http://www.microtonal.co.uk/limit7.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit9.gauss">http://www.microtonal.co.uk/limit9.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit11.gauss">http://www.microtonal.co.uk/limit11.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit13.gauss">http://www.microtonal.co.uk/limit13.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit15.gauss">http://www.microtonal.co.uk/limit15.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit17.gauss">http://www.microtonal.co.uk/limit17.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit19.gauss">http://www.microtonal.co.uk/limit19.gauss</a>&gt;<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/limit21.gauss">http://www.microtonal.co.uk/limit21.gauss</a>&gt;<br/>&gt;<br/>&gt; They seem to make good enough sense.</p><p>Are you missing any &quot;slippery&quot; examples that don&apos;t come easily out of<br/>two ETs?</p><p>Since you&apos;re doing so much work to get the unison vectors, shouldn&apos;t<br/>we be thinking about _starting_ with unison vectors?<br/>&gt;<br/>&gt; Another idea would be to take all the intervals between second-<br/>order<br/>&gt; intervals below a certain size, and use them as unison vectors to<br/>generate<br/>&gt; temperaments.  I might try that.</p><p>That should plug a lot of holes.</p></div><h3><a id=1696 href="#1696">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/6/2001 7:12:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; I&apos;ve updated the script at &lt;<a href="http://www.microtonal.co.uk/temper.html">http://www.microtonal.co.uk/temper.html</a>&gt;<br/>to<br/>&gt; produce files using Dave Keenan&apos;s new figure of demerit.  That is<br/>&gt;<br/>&gt; width**2 * math.exp((error/self.stdError*3)**2)</p><p>Thanks for doing that Graham.</p><p>I note that Graham is using maximum width and (optimised) maximum<br/>error where Gene is using rms width and (optimised) rms error. It will<br/>be interesting to see if this alone makes much difference to the<br/>rankings. I doubt it.</p><p>&gt; The stdError is from some complexity calculations we did before.  I<br/>forget<br/>&gt; what, but it&apos;s 17 cents.</p><p>Actually that looks like the 1% std dev in frequency that came from<br/>some dude&apos;s experiments with actual live humans experiencing actual<br/>air vibrations. Paul can you remind us who it was and what s/he<br/>measured?</p><p>So I see that while the gaussian with std error of 17 cents seems to<br/>do the right thing in eliminating temperaments with tiny errors but<br/>huge numbers of generators, it is too hard on those with larger<br/>errors. Notice that Ennealimmal is still in the 7-limit list (about<br/>number 22). The problem is that Paultone isn&apos;t there at all! It has<br/>17.5 c error with 6 gens per tetrad.</p><p>Those lists don&apos;t contain any temperament with errors greater than 10<br/>cents. The 5-limit 163 cent neutral second temperament has the largest<br/>at 9.8 cents, with 5 generators per triad.</p><p>So I have to agree with Paul that<br/>  badness = num_gens^2 / gaussian(error/17c)<br/>doesn&apos;t work.</p><p>I realised there&apos;s no need to have non-linear functions of _both_<br/>num_gens and error (steps and cents) in this badness metric. e.g.<br/>This will give the same ranking as the above:</p><p>  badness = num_gens / gaussian(error/(17c * sqrt(2)))</p><p>So all we really want to know is the relationship between error and<br/>num_gens. What shape is a line of constant badness (an isobad) on a<br/>plot of number of generators needed for a complete otonality (or<br/>diamond) against error in cents.</p><p>The simplest badness, num_gens * error, would mean the isobads are<br/>hyperbolas, (and num_gens^2 * error or equivalently num_gens *<br/>sqrt(error) is of course very similar) but I think it is clear that,<br/>for constant badness, as error goes to zero, num-gens does _not_ go to<br/>infinity, but levels off. Even for zero error there is a limit to how<br/>many generators you can tolerate. I find it difficult to imagine<br/>anyone being seriously interested in using a temperament that needs 30<br/>generators to get a single complete otonality, no matter how small the<br/>error is. And I think this limiting number of generators decreases as<br/>the odd-limit decreases.</p><p>We can introduce this as a sudden limit as Gene suggested, or we can<br/>use some continuous function to make it come on gradually</p><p>An isobad will also have a maximum number of cents error that can be<br/>tolerated even when everything is approximated by a single generator.<br/>Notice that the number of generators can&apos;t go below 1 (even for rms),<br/>so we don&apos;t care what an isobad does for num_gens &lt; 1.</p><p>What&apos;s a nice simple badness metric that will give us these effects?</p><p>&gt; Oh yes.  Seeing as a 7-limit microtemperament is now causing<br/>something of<br/>&gt; a storm, notice that the top 11-limit one is 26+46 (complexity of<br/>30,<br/>&gt; errors within 2.5 cents).  And the simplest with all errors below a<br/>cent<br/>&gt; is 118+152 (complexity of 74).</p><p>Yes, even though we don&apos;t consider it a microtemperament at the<br/>11-limit, Miracle temperament is really a serious 11-limit optimum, by<br/>any (reasonable) goodness measure. You have to pay an enormous cost in<br/>extra complexity to get the max error even _slightly_ lower than<br/>11-limit-Miracle&apos;s 3.3 cents, or an enormous cost in cents to get the<br/>complexity down even slightly below 11-limit-Miracle&apos;s 22 generators.<br/>Is that what you are indicating?</p></div><h3><a id=1698 href="#1698">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/6/2001 7:38:36 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;dkeenanuqnetau&quot; &lt;d.keenan@u...&gt; wrote:</p><p>&gt; Actually that looks like the 1% std dev in frequency that came from<br/>&gt; some dude&apos;s experiments with actual live humans experiencing actual<br/>&gt; air vibrations. Paul can you remind us who it was and what s/he<br/>&gt; measured?</p><p>It measured the typical uncertainties with which sine-wave partials<br/>in an optimal frequency range were heard, based on the uncertainties<br/>with which the virtual fundamentals were heard.</p></div><h3><a id=1699 href="#1699">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/6/2001 7:47:20 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;dkeenanuqnetau&quot; &lt;d.keenan@u...&gt; wrote:<br/>&gt; What&apos;s a nice simple badness metric that will give us these effects?</p><p>Hey! What&apos;s wrong with simply</p><p>  badness = num_gens + error_in_cents</p><p>(i.e. steps + cents)</p><p>or if that seems too arbitrary, how about agreeing on some value of k<br/>in</p><p>  badness = k * num_gens + error_in_cents, where k ~= 1</p><p>or maybe even</p><p>  badness = k/odd_limit * num_gens + error_in_cents, where k ~= 5</p><p>Wanna give this one a spin Graham?</p></div><h3><a id=1747 href="#1747">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/7/2001 1:14:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Dave Keenan wrote:</p><p>&gt; I note that Graham is using maximum width and (optimised) maximum<br/>&gt; error where Gene is using rms width and (optimised) rms error. It will<br/>&gt; be interesting to see if this alone makes much difference to the<br/>&gt; rankings. I doubt it.</p><p>I&apos;ve implemented RMS error now.  It&apos;s actually faster than the minimax, so<br/>I&apos;ve made it the default.  I&apos;ve uploaded new copies of the .txt and .gauss<br/>files.  There are also other changes to the code to make it more<br/>efficient.  As it stands, the ET matching is broken.  I&apos;ve fixed that, but<br/>not uploaded.</p><p>You could implement the RMS width easily enough, but I expect it&apos;ll slow<br/>down execution, so you can do it on your own time.</p><p>&gt; So I see that while the gaussian with std error of 17 cents seems to<br/>&gt; do the right thing in eliminating temperaments with tiny errors but<br/>&gt; huge numbers of generators, it is too hard on those with larger<br/>&gt; errors. Notice that Ennealimmal is still in the 7-limit list (about<br/>&gt; number 22). The problem is that Paultone isn&apos;t there at all! It has<br/>&gt; 17.5 c error with 6 gens per tetrad.</p><p>I&apos;m dividing the 17 cents by 3 in this case, to give a figure more like<br/>what you asked for.</p><p>&gt; Those lists don&apos;t contain any temperament with errors greater than 10<br/>&gt; cents. The 5-limit 163 cent neutral second temperament has the largest<br/>&gt; at 9.8 cents, with 5 generators per triad.<br/>&gt;<br/>&gt; So I have to agree with Paul that<br/>&gt;   badness = num_gens^2 / gaussian(error/17c)<br/>&gt; doesn&apos;t work.</p><p>It works fine.  You asked for errors of around 6 cents, so why should you<br/>expect errors greater than 10 cents?</p><p>                         Graham</p></div><h3><a id=1748 href="#1748">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/7/2001 1:14:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p><a href="mailto:paul@stretch-music.com">paul@stretch-music.com</a> (paulerlich) wrote:</p><p>&gt; --- In tuning-math@y..., graham@m... wrote:<br/>&gt; &gt; I&apos;ve updated the script at<br/>&gt; &lt;<a href="http://www.microtonal.co.uk/temper.html">http://www.microtonal.co.uk/temper.html</a>&gt; to<br/>&gt; &gt; produce files using Dave Keenan&apos;s new figure of demerit.  That is<br/>&gt; &gt;<br/>&gt; &gt; width**2 * math.exp((error/self.stdError*3)**2)<br/>&gt;<br/>&gt; I thought Dave Keenan wanted to use Gene&apos;s &quot;step&quot; measure. In<br/>&gt; addition, I think it should be weighted to favor the simpler<br/>&gt; consonances.</p><p>Yes, but width is the analog for the way I&apos;m calculating it.</p><p>&gt; Are you missing any &quot;slippery&quot; examples that don&apos;t come easily out of<br/>&gt; two ETs?</p><p>Yes, as always.</p><p>&gt; Since you&apos;re doing so much work to get the unison vectors, shouldn&apos;t<br/>&gt; we be thinking about _starting_ with unison vectors?</p><p>Yes, I&apos;ve done that, and so has Gene.</p><p>&gt; &gt; Another idea would be to take all the intervals between second-<br/>&gt; order<br/>&gt; &gt; intervals below a certain size, and use them as unison vectors to<br/>&gt; generate<br/>&gt; &gt; temperaments.  I might try that.<br/>&gt;<br/>&gt; That should plug a lot of holes.</p><p>I need to be able to take all combinations.  So far, I can only do that<br/>for the 7-limit, where they&apos;re pairs.  I&apos;ll have to think about the<br/>general case.  It&apos;ll probably involve recursion.  I&apos;m also worried about<br/>the speed of this search, because there are going to be a lot more unison<br/>vector combinations that ET pairs for the higher limits.</p><p>It may be more efficient to take different readings of inconsistent ETs.</p><p>                    Graham</p></div><h3><a id=1749 href="#1749">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/7/2001 1:33:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Dave Keenan wrote:</p><p>&gt; I note that Graham is using maximum width and (optimised) maximum<br/>&gt; error where Gene is using rms width and (optimised) rms error. It will<br/>&gt; be interesting to see if this alone makes much difference to the<br/>&gt; rankings. I doubt it.</p><p>Oh yes, I forgot to say before.  Here&apos;s the difference RMS errors make in<br/>the 11-limit:<br/>&#x9;<br/>1 1<br/>2 2<br/>4 4<br/>3 3<br/>7 6<br/>5 9<br/>15 5<br/>12 14<br/>6 16<br/>14 17</p><p>The left hand column is the minimax ranking in terms of the RMS one, and<br/>the other one is the other way round.  So they mostly agree on the best<br/>ones, but disagree on the mediocre ones.</p><p>To check my RMS optimization&apos;s working, is a 116.6722643 cent generator<br/>right for Miracle in the 11-limit?  RMS error of 1.9732 cents.</p><p>                 Graham</p></div><h3><a id=1750 href="#1750">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/7/2001 2:33:29 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; Oh yes, I forgot to say before.  Here&apos;s the difference RMS errors<br/>make in<br/>&gt; the 11-limit:<br/>...<br/>&gt; The left hand column is the minimax ranking in terms of the RMS one,<br/>and<br/>&gt; the other one is the other way round.  So they mostly agree on the<br/>best<br/>&gt; ones, but disagree on the mediocre ones.</p><p>Ok. Thanks. That was a good way of showing it.</p><p>&gt; To check my RMS optimization&apos;s working, is a 116.6722643 cent<br/>generator<br/>&gt; right for Miracle in the 11-limit?  RMS error of 1.9732 cents.</p><p>I get 116.678 and 1.9017. Did you include the squared error for 1:3<br/>twice? I think you should since it occurs twice in an 11-limit hexad,<br/>as both 1:3 and 3:9. So then you must divide by 15, not 14, to get the<br/>mean.</p><p>Actually, I see that this doesn&apos;t explain our discrepancy.</p></div><h3><a id=1751 href="#1751">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/7/2001 4:30:26 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:</p><p>&gt; I need to be able to take all combinations.  So far, I can only do<br/>that<br/>&gt; for the 7-limit, where they&apos;re pairs.  I&apos;ll have to think about the<br/>&gt; general case.  It&apos;ll probably involve recursion.</p><p>It&apos;s certainly possible to start with a certain prime limit, and use<br/>that for the next one; I&apos;ve been thinking about that from the point<br/>of view of 5--&gt;7.</p><p>  I&apos;m also worried about<br/>&gt; the speed of this search, because there are going to be a lot more<br/>unison<br/>&gt; vector combinations that ET pairs for the higher limits.</p><p>Already for the 11-limit you need to wedge three unison vectors to<br/>get the wedgie for a linear temperament, but only two ets. However,<br/>to get the wedgie for a *planar* temperament, it is two unisons vs<br/>three ets, and in higher limits it gets more involved yet.</p></div><h3><a id=1753 href="#1753">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/7/2001 4:58:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;dkeenanuqnetau&quot; &lt;d.keenan@u...&gt; wrote:<br/>&gt; --- In tuning-math@y..., graham@m... wrote:<br/>&gt; &gt; Oh yes, I forgot to say before.  Here&apos;s the difference RMS errors<br/>&gt; make in<br/>&gt; &gt; the 11-limit:<br/>&gt; ...<br/>&gt; &gt; The left hand column is the minimax ranking in terms of the RMS<br/>one,<br/>&gt; and<br/>&gt; &gt; the other one is the other way round.  So they mostly agree on<br/>the<br/>&gt; best<br/>&gt; &gt; ones, but disagree on the mediocre ones.<br/>&gt;<br/>&gt; Ok. Thanks. That was a good way of showing it.<br/>&gt;<br/>&gt; &gt; To check my RMS optimization&apos;s working, is a 116.6722643 cent<br/>&gt; generator<br/>&gt; &gt; right for Miracle in the 11-limit?  RMS error of 1.9732 cents.<br/>&gt;<br/>&gt; I get 116.678 and 1.9017.</p><p>I got 116.672264296056... which checks with Graham, so that&apos;s<br/>progress of some kind.</p></div><h3><a id=1755 href="#1755">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/7/2001 6:11:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:<br/>&gt; I got 116.672264296056... which checks with Graham, so that&apos;s<br/>&gt; progress of some kind.</p><p>So what&apos;s wrong with this spreadsheet?</p><p><a href="http://uq.net.au/~zzdkeena/Music/Miracle/Miracle11RMS.xls">http://uq.net.au/~zzdkeena/Music/Miracle/Miracle11RMS.xls</a></p></div><h3><a id=1756 href="#1756">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/7/2001 6:12:56 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:</p><p>&gt; I got 116.672264296056... which checks with Graham, so that&apos;s<br/>&gt; progress of some kind.</p><p>I get 116.6775720762089, which agrees with Dave. Gene, did you have<br/>15 error terms like we did?</p></div><h3><a id=1765 href="#1765">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/7/2001 9:01:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Me:<br/>&gt; &gt; To check my RMS optimization&apos;s working, is a 116.6722643 cent<br/>&gt; generator<br/>&gt; &gt; right for Miracle in the 11-limit?  RMS error of 1.9732 cents.</p><p>Dave:<br/>&gt; I get 116.678 and 1.9017. Did you include the squared error for 1:3<br/>&gt; twice? I think you should since it occurs twice in an 11-limit hexad,<br/>&gt; as both 1:3 and 3:9. So then you must divide by 15, not 14, to get the<br/>&gt; mean.</p><p>I include 1:3 and 1:9</p><p>&gt; Actually, I see that this doesn&apos;t explain our discrepancy.</p><p>It may depend on whether or not you include the zero error for 1/1 in the<br/>mean.</p><p>                   Graham</p></div><h3><a id=1770 href="#1770">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/7/2001 10:22:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; It may depend on whether or not you include the zero error for 1/1<br/>in the<br/>&gt; mean.</p><p>I don&apos;t. Seems like a silly idea. And that wouldn&apos;t change _where_ the<br/>minimum occurs.</p><p>Are you able to look at the Excel spreadsheet I gave the URL for in my<br/>previous message in this thread?</p></div><h3><a id=1788 href="#1788">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/8/2001 1:32:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Me:<br/>&gt; &gt; It may depend on whether or not you include the zero error for 1/1<br/>&gt; in the<br/>&gt; &gt; mean.</p><p>Dave:<br/>&gt; I don&apos;t. Seems like a silly idea. And that wouldn&apos;t change _where_ the<br/>&gt; minimum occurs.</p><p>Yes, won&apos;t change the position.  But, looking carefully at your previous<br/>mail, I see you&apos;re including 1/3, 9/3 and 9/1, so that&apos;ll be it.  I remove<br/>the duplicates.</p><p>&gt; Are you able to look at the Excel spreadsheet I gave the URL for in my<br/>&gt; previous message in this thread?</p><p>I&apos;ll be able to look at it on Monday, when I get back to work.  I *might*<br/>be able to check it in Star Office first, but probably won&apos;t.</p><p>                  Graham</p></div><h3><a id=1804 href="#1804">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/9/2001 7:35:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; Yes, won&apos;t change the position.  But, looking carefully at your<br/>previous<br/>&gt; mail, I see you&apos;re including 1/3, 9/3 and 9/1, so that&apos;ll be it.  I<br/>remove<br/>&gt; the duplicates.</p><p>Whoa there! It&apos;s arguable that one could omit 3:9 since it is a<br/>duplicate of 1:3 (although Paul and I agree it should stay duplicated<br/>because the interval really does occur at two different places in the<br/>complete chord) but 1:9 isn&apos;t a duplicate of anything. You can&apos;t<br/>ignore the 1:9 error. It really exists. When you listen to a bare 4:9<br/>you don&apos;t hear two 2:3 errors.</p></div><h3><a id=1806 href="#1806">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/9/2001 7:47:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:<br/>&gt; Me:<br/>&gt; &gt; &gt; It may depend on whether or not you include the zero error for<br/>1/1<br/>&gt; &gt; in the<br/>&gt; &gt; &gt; mean.<br/>&gt;<br/>&gt; Dave:<br/>&gt; &gt; I don&apos;t. Seems like a silly idea. And that wouldn&apos;t change<br/>_where_ the<br/>&gt; &gt; minimum occurs.<br/>&gt;<br/>&gt; Yes, won&apos;t change the position.  But, looking carefully at your<br/>previous<br/>&gt; mail, I see you&apos;re including 1/3, 9/3 and 9/1, so that&apos;ll be it.  I<br/>remove<br/>&gt; the duplicates.</p><p>9/3 is not a duplicate of 3/1 -- all saturated chords in the 11-limit<br/>contain not one, but two of the intervals that this is equal to. And<br/>9/1 is a duplicate of what?</p></div><h3><a id=1807 href="#1807">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/9/2001 9:07:46 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;dkeenanuqnetau&quot; &lt;d.keenan@u...&gt; wrote:</p><p>&gt; Whoa there! It&apos;s arguable that one could omit 3:9 since it is a<br/>&gt; duplicate of 1:3 (although Paul and I agree it should stay<br/>duplicated<br/>&gt; because the interval really does occur at two different places in<br/>the<br/>&gt; complete chord) but 1:9 isn&apos;t a duplicate of anything.</p><p>Since Graham checked with me, he must be including 9, and counting 3<br/>once (on the grounds that there is only one 3 among all the numbers.)<br/>Since we already double up on 3 a lot by having 3,9,5/3,9/5,7/3,9/7,<br/>11/3,11/9 we are giving it quite a lot of weight as it is. I don&apos;t<br/>think there are any pragmatic arguments either way, but counting it<br/>twice seems a little random to me.</p></div><h3><a id=1809 href="#1809">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/10/2001 5:06:00 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>In-Reply-To: &lt;<a href="mailto:9v1b8g+f9bd@eGroups.com">9v1b8g+f9bd@eGroups.com</a>&gt;<br/>Me:<br/>&gt; &gt; Yes, won&apos;t change the position.  But, looking carefully at your<br/>&gt; previous<br/>&gt; &gt; mail, I see you&apos;re including 1/3, 9/3 and 9/1, so that&apos;ll be it.  I<br/>&gt; remove<br/>&gt; &gt; the duplicates.</p><p>Paul:<br/>&gt; 9/3 is not a duplicate of 3/1 -- all saturated chords in the 11-limit<br/>&gt; contain not one, but two of the intervals that this is equal to. And<br/>&gt; 9/1 is a duplicate of what?</p><p>9/3 and 3/1 are duplicates.  The both simplify to be the same.  9/1 isn&apos;t<br/>a duplicate, so I don&apos;t remove it.</p><p>What about 3/3, 5/5, 7/7, 9/9 and 11/11?  Should they be included in the<br/>average?  How about 11/8, 16/11, 32/11, 11/4, 11/2, etc?</p><p>                 Graham</p></div><h3><a id=1814 href="#1814">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/10/2001 12:35:55 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:<br/>&gt; --- In tuning-math@y..., &quot;dkeenanuqnetau&quot; &lt;d.keenan@u...&gt; wrote:<br/>&gt;<br/>&gt; &gt; Whoa there! It&apos;s arguable that one could omit 3:9 since it is a<br/>&gt; &gt; duplicate of 1:3 (although Paul and I agree it should stay<br/>&gt; duplicated<br/>&gt; &gt; because the interval really does occur at two different places in<br/>&gt; the<br/>&gt; &gt; complete chord) but 1:9 isn&apos;t a duplicate of anything.<br/>&gt;<br/>&gt; Since Graham checked with me, he must be including 9, and counting<br/>3<br/>&gt; once (on the grounds that there is only one 3 among all the<br/>numbers.)<br/>&gt; Since we already double up on 3 a lot by having 3,9,5/3,9/5,7/3,9/7,<br/>&gt; 11/3,11/9 we are giving it quite a lot of weight as it is. I don&apos;t<br/>&gt; think there are any pragmatic arguments either way, but counting it<br/>&gt; twice seems a little random to me.</p><p>Sorry, I have to disagree. Graham is specifically considering the<br/>harmonic entity that consists of the first N odd numbers, in a chord.<br/>If a particular interval occurs twice, then we _have_ to weight it<br/>twice. And this is to say nothing of all the other saturated chords<br/>Graham is not using!</p></div><h3><a id=1815 href="#1815">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/10/2001 12:37:57 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:</p><p>&gt; What about 3/3, 5/5, 7/7, 9/9 and 11/11?  Should they be included<br/>in the<br/>&gt; average?</p><p>It wouldn&apos;t affect the result.</p><p>&gt; How about 11/8, 16/11, 32/11, 11/4, 11/2, etc?</p><p>If you included an equal number of octave-equivalents to each ratio,<br/>again, the result wouldn&apos;t be affected.</p></div><h3><a id=1817 href="#1817">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/10/2001 12:42:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:</p><p>&gt; Sorry, I have to disagree. Graham is specifically considering the<br/>&gt; harmonic entity that consists of the first N odd numbers, in a<br/>chord.</p><p>That may be what Graham was doing, but it wasn&apos;t what I was doing; I<br/>seldom go beyond four parts.</p></div><h3><a id=1819 href="#1819">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/10/2001 2:49:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:<br/>&gt; --- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:<br/>&gt;<br/>&gt; &gt; Sorry, I have to disagree. Graham is specifically considering the<br/>&gt; &gt; harmonic entity that consists of the first N odd numbers, in a<br/>&gt; chord.<br/>&gt;<br/>&gt; That may be what Graham was doing, but it wasn&apos;t what I was doing;<br/>I<br/>&gt; seldom go beyond four parts.</p><p>Even if you don&apos;t, don&apos;t you think chords like</p><p>1:3:5:9<br/>1:3:7:9<br/>1:3:9:11<br/>10:12:15:18<br/>12:14:18:21<br/>18:22:24:33</p><p>which contain only 11-limit consonant intervals, would be important<br/>to your music?</p></div><h3><a id=1822 href="#1822">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/10/2001 10:49:47 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:</p><p>&gt; 1:3:5:9<br/>&gt; 1:3:7:9<br/>&gt; 1:3:9:11<br/>&gt; 10:12:15:18<br/>&gt; 12:14:18:21<br/>&gt; 18:22:24:33<br/>&gt;<br/>&gt; which contain only 11-limit consonant intervals, would be important<br/>&gt; to your music?</p><p>Indeed they are, but they are taken care of.</p></div><h3><a id=1823 href="#1823">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/10/2001 11:30:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:<br/>&gt; --- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:<br/>&gt;<br/>&gt; &gt; 1:3:5:9<br/>&gt; &gt; 1:3:7:9<br/>&gt; &gt; 1:3:9:11<br/>&gt; &gt; 10:12:15:18<br/>&gt; &gt; 12:14:18:21<br/>&gt; &gt; 18:22:24:33<br/>&gt; &gt;<br/>&gt; &gt; which contain only 11-limit consonant intervals, would be<br/>important<br/>&gt; &gt; to your music?<br/>&gt;<br/>&gt; Indeed they are, but they are taken care of.</p><p>Shouldn&apos;t you weight them _twice_ if they&apos;re occuring twice as often?<br/>How can you justify equal-weighting?</p></div><h3><a id=1824 href="#1824">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@juno.com&#x3E;</h3><span>12/10/2001 11:58:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:</p><p>&gt; Shouldn&apos;t you weight them _twice_ if they&apos;re occuring twice as<br/>often?<br/>&gt; How can you justify equal-weighting?</p><p>I do weight them twice, more or less, depending on how you define<br/>this. 3 is weighted once as a 3, and then its error is doubled, so it<br/>is weighted again 4 times as much from 3 and 9 together; so 3 is<br/>weighted 5 times, or 9 1.25 times, from one point of view. Then we<br/>double dip with 5/3, 5/9 etc. with similar effect.</p></div><h3><a id=1825 href="#1825">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/11/2001 12:06:23 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;genewardsmith&quot; &lt;genewardsmith@j...&gt; wrote:<br/>&gt; --- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:<br/>&gt;<br/>&gt; &gt; Shouldn&apos;t you weight them _twice_ if they&apos;re occuring twice as<br/>&gt; often?<br/>&gt; &gt; How can you justify equal-weighting?<br/>&gt;<br/>&gt; I do weight them twice, more or less, depending on how you define<br/>&gt; this. 3 is weighted once as a 3, and then its error is doubled, so<br/>it<br/>&gt; is weighted again 4 times as much from 3 and 9 together; so 3 is<br/>&gt; weighted 5 times, or 9 1.25 times, from one point of view. Then we<br/>&gt; double dip with 5/3, 5/9 etc. with similar effect.</p><p>I don&apos;t see it that way. 9:1 is an interval of its own and needs to<br/>be weighted independently of whether any 3:1s or 9:3s are actually<br/>used. 5/3 and 5/9 could be seen as weighting 5 commensurately more, I<br/>don&apos;t buy the &quot;double dip&quot; bit one bit!</p><p>These are conclusions I&apos;ve reached after years of playing with<br/>tunings with large errors and comparing them and thinking hard about<br/>this problem.</p></div><h3><a id=1828 href="#1828">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/11/2001 4:55:00 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>In-Reply-To: &lt;<a href="mailto:9v3e5l+102l7@eGroups.com">9v3e5l+102l7@eGroups.com</a>&gt;<br/>Paul:<br/>&gt; &gt; &gt; Sorry, I have to disagree. Graham is specifically considering the<br/>&gt; &gt; &gt; harmonic entity that consists of the first N odd numbers, in a<br/>&gt; &gt; chord.</p><p>I&apos;m considering a set of consonant intervals with equal weighting.  9:3<br/>and 3:1 are the same interval.  Perhaps I should improve my minimax<br/>algorithm so the whole debate becomes moot.</p><p>Gene:<br/>&gt; &gt; That may be what Graham was doing, but it wasn&apos;t what I was doing;<br/>&gt; I<br/>&gt; &gt; seldom go beyond four parts.</p><p>Paul:<br/>&gt; Even if you don&apos;t, don&apos;t you think chords like<br/>&gt;<br/>&gt; 1:3:5:9<br/>&gt; 1:3:7:9<br/>&gt; 1:3:9:11<br/>&gt; 10:12:15:18<br/>&gt; 12:14:18:21<br/>&gt; 18:22:24:33<br/>&gt;<br/>&gt; which contain only 11-limit consonant intervals, would be important<br/>&gt; to your music?</p><p>Yes, but so is 3:4:5:6 which involves both 2:3 and 3:4.  And 1/1:11/9:3/2,<br/>which has two neutral thirds (and so far I&apos;ve not used 11-limit<br/>temperaments in which this doesn&apos;t work) so should they be weighted<br/>double?  My experience so far of Miracle is that the &quot;wolf fourth&quot; of 4<br/>secors is also important, but I don&apos;t have a rational approximation (21:16<br/>isn&apos;t quite right).  It may be that chords of 0-2-4-6-8 secors become<br/>important, in which case 8:7 should be weighted three times as high as<br/>12:7 and twice as high as 3:2.</p><p>I&apos;d much rather stay with the simple rule that all consonant intervals are<br/>weighted equally until we can come up with an improved, subjective<br/>weighting.  For that, I&apos;m thinking of taking Partch at his word weighting<br/>more complex intervals higher.  But Paul was talking about a Tenney<br/>metric, which would have the opposite effect.  So it looks like we&apos;re not<br/>going to agree on that one.</p><p>                            Graham</p></div><h3><a id=1829 href="#1829">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/11/2001 6:21:53 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:</p><p>&gt; Yes, but so is 3:4:5:6 which involves both 2:3 and 3:4.</p><p>But you can do that with _any_ interval.</p><p>&gt; And 1/1:11/9:3/2,<br/>&gt; which has two neutral thirds (and so far I&apos;ve not used 11-limit<br/>&gt; temperaments in which this doesn&apos;t work) so should they be weighted<br/>&gt; double?</p><p>Only if that were your target harmony. I thought hexads were your<br/>target harmony.</p><p>&gt; My experience so far of Miracle is that the &quot;wolf fourth&quot; of 4<br/>&gt; secors is also important, but I don&apos;t have a rational approximation<br/>(21:16<br/>&gt; isn&apos;t quite right).</p><p>What do you mean it&apos;s important?</p><p>&gt; It may be that chords of 0-2-4-6-8 secors become<br/>&gt; important, in which case 8:7 should be weighted three times as high<br/>as<br/>&gt; 12:7 and twice as high as 3:2.</p><p>If that was the harmony you were targeting, sure.</p><p>&gt; I&apos;d much rather stay with the simple rule that all consonant<br/>intervals are<br/>&gt; weighted equally until we can come up with an improved, subjective<br/>&gt; weighting.  For that, I&apos;m thinking of taking Partch at his word<br/>weighting<br/>&gt; more complex intervals higher.  But Paul was talking about a Tenney<br/>&gt; metric, which would have the opposite effect.  So it looks like<br/>we&apos;re not<br/>&gt; going to agree on that one.</p><p>If you don&apos;t agree with me that you&apos;re targeting the hexad (I thought<br/>you had said as much at one point, when I asked you to consider<br/>running some lists for other saturated chords), then maybe we better<br/>go to minimax (of course, we&apos;ll still have a problem in cases like<br/>paultone, where the maximum error is fixed -- what do we do then, go<br/>to 2nd-worst error?).</p></div><h3><a id=1831 href="#1831">ðŸ”—</a>dkeenanuqnetau &#x3C;d.keenan@uq.net.au&#x3E;</h3><span>12/11/2001 8:13:51 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., &quot;paulerlich&quot; &lt;paul@s...&gt; wrote:<br/>&gt; If you don&apos;t agree with me that you&apos;re targeting the hexad (I<br/>thought<br/>&gt; you had said as much at one point, when I asked you to consider<br/>&gt; running some lists for other saturated chords), then maybe we better<br/>&gt; go to minimax (of course, we&apos;ll still have a problem in cases like<br/>&gt; paultone, where the maximum error is fixed -- what do we do then, go<br/>&gt; to 2nd-worst error?).</p><p>Yes. That&apos;s what I do. You still give the error as the worst one, but<br/>you give the optimum generator based on the worst error that actually<br/>_depends_ on the generator (as opposed to being fixed because it only<br/>depends on the period).</p></div><h3><a id=1834 href="#1834">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/12/2001 2:47:00 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>In-Reply-To: &lt;<a href="mailto:9v6f01+1bdh@eGroups.com">9v6f01+1bdh@eGroups.com</a>&gt;<br/>Paul wrote:</p><p>&gt; If you don&apos;t agree with me that you&apos;re targeting the hexad (I thought<br/>&gt; you had said as much at one point, when I asked you to consider<br/>&gt; running some lists for other saturated chords), then maybe we better<br/>&gt; go to minimax (of course, we&apos;ll still have a problem in cases like<br/>&gt; paultone, where the maximum error is fixed -- what do we do then, go<br/>&gt; to 2nd-worst error?).</p><p>The hexads are targetted by the complexity formula.  But that&apos;s because<br/>it&apos;s the simplest such measure, not because I actually think they&apos;re<br/>musically useful.  I&apos;m coming to the opinion that anything over a 7-limit<br/>tetrad is quite ugly, but some smaller 11-limit chords (and some chords<br/>with the for secor wolf) are strikingly beautiful, if they&apos;re tuned right.<br/> So Blackjack is a good 11-limit scale although it doesn&apos;t contain any<br/>hexads.</p><p>I&apos;ve always preferred minimax as a measure, but currently speed is the<br/>most important factor.  The RMS optimum can be calculated much faster, and<br/>although I can improve the minimax algorithm I don&apos;t think it can be made<br/>as fast.</p><p>Scales such as Paultone can be handled by excluding all intervals that<br/>don&apos;t depend on the generator.  But the value used for rankings still has<br/>to include all intervals.  My program should be doing this, but I&apos;m not<br/>sure if it is working correctly, so if you&apos;d like to check this should be<br/>Paultone minimax:</p><p>2/11, 106.8 cent generator</p><p>basis:<br/>(0.5, 0.089035952556318909)</p><p>mapping by period and generator:<br/>[(2, 0), (3, 1), (5, -2), (6, -2)]</p><p>mapping by steps:<br/>[(12, 10), (19, 16), (28, 23), (34, 28)]</p><p>highest interval width: 3<br/>complexity measure: 6  (8 for smallest MOS)<br/>highest error: 0.014573  (17.488 cents)<br/>unique</p><p>9:7 =~ 32:25 =~ 64:49<br/>8:7 =~ 9:8<br/>4:3 =~ 21:16<br/>35:32 =~ 10:9</p><p>consistent with: 10, 12, 22</p><p>Hmm, why isn&apos;t 7:5 =~ 10:7 on that list?</p><p>                          Graham</p></div><h3><a id=1839 href="#1839">ðŸ”—</a>paulerlich &#x3C;paul@stretch-music.com&#x3E;</h3><span>12/12/2001 1:04:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In tuning-math@y..., graham@m... wrote:</p><p>&gt; so if you&apos;d like to check this should be<br/>&gt; Paultone minimax:<br/>&gt;<br/>&gt;<br/>&gt; 2/11, 106.8 cent generator</p><p>That&apos;s clearly wrong, as the 7:4 is off by 17.5 cents!</p><p>&gt; basis:<br/>&gt; (0.5, 0.089035952556318909)<br/>&gt;<br/>&gt; mapping by period and generator:<br/>&gt; [(2, 0), (3, 1), (5, -2), (6, -2)]<br/>&gt;<br/>&gt; mapping by steps:<br/>&gt; [(12, 10), (19, 16), (28, 23), (34, 28)]<br/>&gt;<br/>&gt; highest interval width: 3<br/>&gt; complexity measure: 6  (8 for smallest MOS)<br/>&gt; highest error: 0.014573  (17.488 cents)<br/>&gt; unique</p><p>I don&apos;t think it should count as unique since</p><p>&gt; 7:5 =~ 10:7</p></div><h3><a id=1858 href="#1858">ðŸ”—</a>graham@microtonal.co.uk</h3><span>12/13/2001 3:47:00 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>In-Reply-To: &lt;<a href="mailto:9v8gp5+lv0o@eGroups.com">9v8gp5+lv0o@eGroups.com</a>&gt;<br/>Me:<br/>&gt; &gt; so if you&apos;d like to check this should be<br/>&gt; &gt; Paultone minimax:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; 2/11, 106.8 cent generator</p><p>Paul:<br/>&gt; That&apos;s clearly wrong, as the 7:4 is off by 17.5 cents!</p><p>Well, that is interesting.  It turns out the minimax temperament<br/>corresponds to a just 35:24.  But I&apos;d previously assumed that the minimax<br/>had to have a just interval within the consonance limit.  Well, I&apos;ve added<br/>some sticking tape to the algorithm, but I&apos;m not sure it&apos;ll hold.</p><p>This is what I get now</p><p>2/11, 109.4 cent generator</p><p>basis:<br/>(0.5, 0.09113589675523795)</p><p>mapping by period and generator:<br/>[(2, 0), (3, 1), (5, -2), (6, -2)]</p><p>mapping by steps:<br/>[(12, 10), (19, 16), (28, 23), (34, 28)]</p><p>highest interval width: 3<br/>complexity measure: 6  (8 for smallest MOS)<br/>highest error: 0.014573  (17.488 cents)</p><p>7:5 =~ 10:7</p><p>consistent with: 10, 12, 22</p><p>&gt; I don&apos;t think it should count as unique since<br/>&gt;<br/>&gt; &gt; 7:5 =~ 10:7</p><p>Yes, that was a different problem.  I wasn&apos;t including<br/>tritone-equivalences as duplicates.</p><p>                Graham</p></div>