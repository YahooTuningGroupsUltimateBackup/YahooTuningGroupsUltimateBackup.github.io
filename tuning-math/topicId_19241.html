<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning-math Bayesian tuning</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning-math">back to list</a><h1>Bayesian tuning</h1><h3><a id=19241 href="#19241">ðŸ”—</a>funwithedo19 &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/22/2011 2:04:28 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Bayesian tuning</p><p>This regards a Bayesian tuning I mentioned on the tuning list. Gene &amp; Mike B suggested this list as a more suitable forum, so I&apos;m restating it here more succinctly. In &quot;tuning&quot;, I&apos;d also talked about a related tuning called Kaneko, but it&apos;s a secondary notion that won&apos;t be mentioned here.</p><p>The Bayes theorem is the founding principle for understanding a large set of methods used in scientific investigation, or, it might be argued, any informative cognitive judgment made at all (cf. E.T. Jaynes, Larry Bretthorst, Harold Jeffreys, et al). The theorem is closely related to psychological principles such as the Weber law (and more-specialized principles such as the Hick, Ricco, etc laws).</p><p>While there are many interesting ways to use and to interpret the Bayes theorem, I will talk about one particular usage of the theorem in one particular case, the iterative binary Bayes. The binary case is perhaps the most useful in general and the case that best shows association with the Weber law and other psychological judgment laws. Two-hypothesis testing just consists of two exhaustive, mutually exclusive possibilities that are to be chosen between and with a likelihood that can be determined from these hypothetical statements. Sampling is done to test them, which should be random enough, of course, to be meaningful.</p><p>Even within this one simple case, there are different possibilities available. One could use symmetric or asymmetric forms of the theorem. Also, certain variables might be held constant or considered to change with time, affecting which evaluation methods are considered best. We will consider a typical approach while defining this tuning, and I will state assumptions along the way.</p><p>A common form of the binary Bayes formula is to evaluate posterior probability as</p><p>P(R|S) = P(R) P(S|R) / P(S) = P(R) / ( P(R) + P(~R) Lmb(S) ).</p><p>where &quot;~&quot;=&quot;not&quot;, &quot;|&quot;=&quot;given that&quot;, and Lmb(S) is a &quot;likelihood ratio&quot; = P(S|~R) / P(S|R).<br/>This is actually the reciprocal of the common definition for likelihood ratio, but I use it because it tends to make things a little nicer to write.</p><p>Another typical approach is to use the posterior odds</p><p>O(R|S) = P(R|S) / P(~R|S) = [ P(R) P(S|R) / P(S) ] / [ P(~R) P(S|~R) / P(S) ] = O(R) / Lmb(S).</p><p>Taking the log allows us to say something very nice and elegant:</p><p>log O(R|S) = log O(R) Â– log Lmb(S).</p><p>log O(R) concerns the prior odds, and log Lmb(S) concerns the evidential data gathered (and its interpretation). If we assume that all samples are logically independent from one another and represent equal amounts of information, then we can consider Lmb(S) simply to be a constant. This is nice, because it allows us to take nice constant-sized steps toward one or another hypothesis on each iteration. Actually, there will be two constant step sizes - one if we are walking toward hypothesis A and another if we are moving toward B (cf. <a href="http://www-biba.inrialpes.fr/Jaynes/cc04q.pdf">http://www-biba.inrialpes.fr/Jaynes/cc04q.pdf</a>).</p><p>DEFINING THE TUNING</p><p>Assume we are performing a given number N iterations.</p><p>Assume initial prior probability P(R) = 1/2, and P(~R) = 1/2.</p><p>Also assume that, of our two hypotheses A and B, every single piece of evidence gathered points to one of them Â– say A.</p><p>This leaves us only one parameter we can adjust to affect our results - our likelihood ratio Lmb(S). This would be defined presumably by our hypotheses, but we will assume here that we have control over writing our likelihood guess into our hypotheses A and B.</p><p>Let us say that after N=13 iterations, P(R|S) should equal 1/4 (half of the initial probability). We can find a Lmb(S) that produces this result:</p><p>First, we note that on a single iteration,</p><p>P(R|S) = P(R) / ( P(R) + P(~R) Lmb(S) ) = 1 / ( 1 + Lmb(S) / O(R) ).</p><p>Without making proof (since it is readily proven; cf. <a href="http://en.wikipedia.org/wiki/Bayesian_inference#Evidence_and_changing_beliefs">http://en.wikipedia.org/wiki/Bayesian_inference#Evidence_and_changing_beliefs</a>), I&apos;ll state that, in this case,</p><p>P(R|S) = 1/4 = 1 / ( 1 + [Lmb(S)]^13 / O(R) ),</p><p>and then</p><p>Lmb(S) = ( 3 O(R) )^(1/13) = ( 3 P(R) / (1-P(R)) )^(1/13)</p><p>Now consider the probabilities that result from each step in this iterative process (in cents):<br/>-1200.0<br/>-1274.697152144154<br/>-1352.47887210531<br/>-1433.328721208953<br/>-1517.219560086023<br/>-1604.113928195837<br/>-1693.964559363352<br/>-1786.715018493686<br/>-1882.300441673644<br/>-1980.648359747189<br/>-2081.679584212231<br/>-2185.309133925829<br/>-2291.447181567758<br/>-2400.0</p><p>We see that we have defined the octave range we sought. It might be interesting to see what happens were we to invert the likelihood ratio and run the same steps:<br/>-1200.0<br/>-1128.392921308355<br/>-1059.870410433712<br/>-994.4160287015554<br/>-932.0026367428263<br/>-872.5927740168412<br/>-816.1391743485577<br/>-762.5854026430926<br/>-711.8665949872516<br/>-663.9102822249965<br/>-618.6372758542402<br/>-575.9625947320386<br/>-535.7964115381681<br/>-498.0449991346121</p><p>Here we have defined a fifth in the opposing direction.</p><p>Consider what happens if we subtract the first set of values from the second (1200*(log P2 Â– log P1), or, equivalently, 1200*log(P2/P1)):<br/> 0.0<br/> 146.3042308357992<br/> 292.6084616715984<br/> 438.9126925073974<br/> 585.2169233431964<br/> 731.5211541789956<br/> 877.8253850147947<br/> 1024.129615850594<br/> 1170.433846686393<br/> 1316.738077522192<br/> 1463.042308357991<br/> 1609.34653919379<br/> 1755.650770029589<br/> 1901.955000865388</p><p>Now we have defined 13-ED3 (i.e. equal BP). In general we will always produce N-EDn, initially given some chosen constants N and n.</p><p>____________</p><p>BASIC PROGRAM BAYESTUN.BAS<br/>&grave;A Bayesian sequence in cents assuming all evidence is of same kind<br/>DEFDBL A-Z<br/>CLS<br/>CONST recip = 1 &apos;Set to -1 for inverse likelihood ratio<br/>CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents<br/>p = 1 / 2<br/>lmb = (3 * p / (1 - p)) ^ (recip * 1 / 13)<br/>FOR i = 0 TO 13<br/>  PRINT 1200 * LOG(p) / LOG(2) + offset<br/>  p = 1 / (1 + lmb * (1 - p) / p)<br/>NEXT i</p></div><h3><a id=19242 href="#19242">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/22/2011 1:03:51 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This program takes N and m and gives the cents listings in file &quot;out.txt&quot;.</p><p>______________________</p><p>&apos;BAYESTU2.BAS</p><p>&apos;Produces N-EDm<br/>CONST N = 13<br/>CONST m = 3<br/>CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents</p><p>DEFDBL A-Z<br/>OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>CLS<br/>PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>P&lt;1/Lmb&gt;   P&lt;Lmb&gt;-P&lt;1/Lmb&gt;&quot;<br/>p = 1 / 2<br/>p2 = 1 / 2<br/>c = (m + 1) / 2<br/>lmb = ((2 * c - 1) * p / (1 - p)) ^ (1 / N)<br/>FOR i = 0 TO 13<br/>  x = 1200 * LOG(p) / LOG(2) + offset<br/>  x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>  PRINT USING &quot;########.####&quot;; x; x2; x2 - x<br/>  PRINT #1, USING &quot;########.####&quot;; x; x2; x2 - x<br/>  p = 1 / (1 + lmb * (1 - p) / p)<br/>  p2 = 1 / (1 + (1 / lmb) * (1 - p2) / p2)<br/>NEXT i<br/>CLOSE #1</p></div><h3><a id=19243 href="#19243">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/22/2011 1:40:47 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Made a superficial typo in the program! FIXED BELOW</p><p>___________</p><p>&apos;BAYESTU2.BAS</p><p>&apos;Produces N-EDm<br/>CONST N = 13<br/>CONST m = 3<br/>CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents</p><p>DEFDBL A-Z<br/>OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>CLS<br/>PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>P&lt;1/Lmb&gt;   P&lt;1/Lmb&gt;-P&lt;Lmb&gt;&quot; &apos;&lt;-FIXED TYPO HERE<br/>p = 1 / 2<br/>p2 = 1 / 2<br/>c = (m + 1) / 2<br/>lmb = ((2 * c - 1) * p / (1 - p)) ^ (1 / N)<br/>FOR i = 0 TO 13<br/>  x = 1200 * LOG(p) / LOG(2) + offset<br/>  x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>  PRINT USING &quot;########.####&quot;; x; x2; x2 - x<br/>  PRINT #1, USING &quot;########.####&quot;; x; x2; x2 - x<br/>  p = 1 / (1 + lmb * (1 - p) / p)<br/>  p2 = 1 / (1 + (1 / lmb) * (1 - p2) / p2)<br/>NEXT i<br/>CLOSE #1</p></div><h3><a id=19245 href="#19245">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/22/2011 11:31:18 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Promise I won&apos;t post any more corrections of this stupid thing, but ANOTHER<br/>TYPO FIXED BELOW:</p><p>&apos;BAYESTU2.BAS</p><p>&gt;<br/>&gt; &apos;Produces N-EDm<br/>&gt; CONST N = 13<br/>&gt; CONST m = 3<br/>&gt; CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents<br/>&gt;<br/>&gt; DEFDBL A-Z<br/>&gt; OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>&gt; CLS<br/>&gt; PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>&gt; P&lt;1/Lmb&gt;   P&lt;1/Lmb&gt;-P&lt;Lmb&gt;&quot;<br/>&gt; p = 1 / 2<br/>&gt; p2 = 1 / 2<br/>&gt; c = (m + 1) / 2<br/>&gt; lmb = ((2 * c - 1) * p / (1 - p)) ^ (1 / N)<br/>&gt; FOR i = 0 TO N                                     &apos;&lt;---NOT 13 (duh)<br/>&gt;   x = 1200 * LOG(p) / LOG(2) + offset<br/>&gt;   x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>&gt;   PRINT USING &quot;########.####&quot;; x; x2; x2 - x<br/>&gt;   PRINT #1, USING &quot;########.####&quot;; x; x2; x2 - x<br/>&gt;   p = 1 / (1 + lmb * (1 - p) / p)<br/>&gt;   p2 = 1 / (1 + (1 / lmb) * (1 - p2) / p2)<br/>&gt; NEXT i<br/>&gt; CLOSE #1<br/>&gt;<br/>&gt;</p></div><h3><a id=19247 href="#19247">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>5/23/2011 9:51:41 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Daniel Nielsen &lt;nielsed@...&gt; wrote:<br/>&gt;<br/>&gt; Promise I won&apos;t post any more corrections of this stupid thing, but ANOTHER<br/>&gt; TYPO FIXED BELOW:</p><p>I don&apos;t get it: neither x nor x2 seem to depend on i.</p></div><h3><a id=19250 href="#19250">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/23/2011 10:37:51 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Due to a cool equivalence, there are (at least) two ways this could have<br/>been written: one dependent on i and one iterative (recursive). I wrote it<br/>iteratively in the program.</p><p>Dependent on i:</p><p>P_i = 1 / ( 1 + Lmb^i / O_0 )</p><p>Iterative:</p><p>P_i = 1 / ( 1 + Lmb / O_(i-1) )</p><p>where odds O_k = P_k / (1 - P_k)</p><p>Here it is dependent on i:</p><p>&apos;BAYESTU3.BAS</p><p>&apos;Produces N-EDm<br/>CONST N = 13<br/>CONST m = 3<br/>CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents</p><p>DEFDBL A-Z<br/>OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>CLS<br/>PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>P&lt;1/Lmb&gt;   P&lt;1/Lmb&gt;-P&lt;Lmb&gt;&quot;<br/>pinit = 1 / 2: p2init = 1 / 2<br/>c = (m + 1) / 2<br/>lmb = ((2 * c - 1) * pinit / (1 - pinit)) ^ (1 / N)<br/>FOR i = 0 TO N<br/>  p = 1 / (1 + lmb ^ i * (1 - pinit) / pinit)<br/>  p2 = 1 / (1 + (1 / lmb) ^ i * (1 - p2init) / p2init)<br/>  x = 1200 * LOG(p) / LOG(2) + offset<br/>  x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>  PRINT USING &quot;########.####&quot;; x; x2; x2 - x<br/>  PRINT #1, USING &quot;########.####&quot;; x; x2; x2 - x<br/>NEXT i<br/>CLOSE #1</p></div><h3><a id=19251 href="#19251">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/23/2011 11:26:51 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Don&apos;t ask me why I wrote</p><p>c = (m + 1) / 2<br/>lmb = ((2 * c - 1) * pinit / (1 - pinit)) ^ (1 / N)</p><p>instead of</p><p>lmb = ((m * pinit / (1 - pinit)) ^ (1 / N)</p><p>I think there was some reason, but it doesn&apos;t matter now.</p><p>Dan N</p><p>On Mon, May 23, 2011 at 12:37 PM, Daniel Nielsen &lt;<a href="mailto:nielsed@uah.edu">nielsed@uah.edu</a>&gt; wrote:</p><p>&gt; Due to a cool equivalence, there are (at least) two ways this could have<br/>&gt; been written: one dependent on i and one iterative (recursive). I wrote it<br/>&gt; iteratively in the program.<br/>&gt;<br/>&gt; Dependent on i:<br/>&gt;<br/>&gt; P_i = 1 / ( 1 + Lmb^i / O_0 )<br/>&gt;<br/>&gt; Iterative:<br/>&gt;<br/>&gt; P_i = 1 / ( 1 + Lmb / O_(i-1) )<br/>&gt;<br/>&gt; where odds O_k = P_k / (1 - P_k)<br/>&gt;<br/>&gt; Here it is dependent on i:<br/>&gt;<br/>&gt; &apos;BAYESTU3.BAS<br/>&gt;<br/>&gt; &apos;Produces N-EDm<br/>&gt; CONST N = 13<br/>&gt; CONST m = 3<br/>&gt; CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents<br/>&gt;<br/>&gt; DEFDBL A-Z<br/>&gt; OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>&gt; CLS<br/>&gt; PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>&gt; P&lt;1/Lmb&gt;   P&lt;1/Lmb&gt;-P&lt;Lmb&gt;&quot;<br/>&gt; pinit = 1 / 2: p2init = 1 / 2<br/>&gt; c = (m + 1) / 2<br/>&gt; lmb = ((2 * c - 1) * pinit / (1 - pinit)) ^ (1 / N)<br/>&gt; FOR i = 0 TO N<br/>&gt;   p = 1 / (1 + lmb ^ i * (1 - pinit) / pinit)<br/>&gt;   p2 = 1 / (1 + (1 / lmb) ^ i * (1 - p2init) / p2init)<br/>&gt;   x = 1200 * LOG(p) / LOG(2) + offset<br/>&gt;   x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>&gt;   PRINT USING &quot;########.####&quot;; x; x2; x2 - x<br/>&gt;   PRINT #1, USING &quot;########.####&quot;; x; x2; x2 - x<br/>&gt; NEXT i<br/>&gt; CLOSE #1<br/>&gt;<br/>&gt;</p></div><h3><a id=19253 href="#19253">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>5/23/2011 3:07:06 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>So I realized what I said about the EDm being produced by uniform<br/>log-of-odds updating was correct (probably should have been pretty obvious<br/>to me, but I&apos;ll blame distractions).</p><p>What I&apos;m talking about is this:</p><p>Since in the program P and P2 are exhaustive probabilities covering the two<br/>possibilities, and P only changes uniformly (since all evidence is assumed<br/>to be of one type) using Lmb, and P2 only changes using the opposite<br/>evidence likelihood ratio (1/Lmb), then P2 is simply the opposite<br/>probability (1-P).</p><p>Therefore</p><p>log(P2/P) = log( (1-P) / P ) = inverse log of odds</p><p>This equivalence can be seen between x2 and x3 in the program below.</p><p>The point is that the EDm scale represents odds, while the others represent<br/>probabilities. (Possible extensions might be a scale based on some odds<br/>ratio R, or maybe using hypotheses that are not exhaustive).</p><p>___________</p><p>&apos;BAYESTU4.BAS</p><p>&apos;Produces N-EDm<br/>CONST N = 13<br/>CONST m = 3<br/>CONST offset = 0 &apos;Set to 1200 to begin values at 0 cents</p><p>DEFDBL A-Z<br/>OPEN &quot;out.txt&quot; FOR OUTPUT AS #1<br/>CLS<br/>PRINT &quot;              (With P in cents)&quot;: PRINT : PRINT &quot;       P&lt;Lmb&gt;<br/>P&lt;1/Lmb&gt;       1-P&lt;Lmb&gt;  P&lt;1/Lmb&gt;-P&lt;Lmb&gt;&quot;<br/>pinit = 1 / 2: p2init = 1 - pinit<br/>lmb = (m * pinit / (1 - pinit)) ^ (1 / N)<br/>FOR i = 0 TO N<br/>  p = 1 / (1 + lmb ^ i * (1 - pinit) / pinit)<br/>  p2 = 1 / (1 + (1 / lmb) ^ i * (1 - p2init) / p2init)<br/>  x = 1200 * LOG(p) / LOG(2) + offset<br/>  x2 = 1200 * LOG(p2) / LOG(2) + offset<br/>  x3 = 1200 * LOG(1 - p) / LOG(2) + offset<br/>  PRINT USING &quot;########.####&quot;; x; x2; x3; x2 - x<br/>  PRINT #1, USING &quot;########.####&quot;; x; x2; x3; x2 - x<br/>NEXT i<br/>CLOSE #1</p></div><h3><a id=19254 href="#19254">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/1/2011 11:04:03 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I noticed that the previously given Bayes tuning is very close to Shaahin<br/>Mohajeri&apos;s ADO, differing by less than 3 cents at max, when the initial ADO<br/>constant (what is called A1 at<br/><a href="http://sites.google.com/site/240edo/arithmeticrationaldivisionsofoctave">http://sites.google.com/site/240edo/arithmeticrationaldivisionsofoctave</a>) is<br/>set to the first iteration value of the Bayes sequence.</p><p>n divisions of m/1</p><p>(In cents)</p><p>m=2; n=12</p><p>Bayes         ADO             Difference</p><p> 0                   0                  0.0000000<br/> 81.06122      81.06122        0.0000000<br/> 165.7415      165.5659        0.1756287<br/> 254.0175      253.5139        0.5036011<br/> 345.8528      344.9054        0.9473877<br/> 441.1953      439.7402        1.4551086<br/> 539.9812      538.0186        1.9626465<br/> 642.1331      639.7402        2.3928833<br/> 747.5635      744.9053        2.6582031<br/> 856.1749      853.5139        2.6609497<br/> 967.8609      965.5659        2.2950439<br/> 1082.509      1081.061        1.4475098<br/> 1200            1200              0.0000000</p><p>For m&gt;2, difference increases very ostensibly (but in a predictable<br/>fashion). Here is m=3 (n=12):</p><p> 0                 0                   0.0000000<br/> 119.9863      119.9863        0.0000000<br/> 247.7172      246.9744        0.7428284<br/> 383.0897      380.9643        2.1253662<br/> 525.9362      521.9561        3.9801025<br/> 676.0291      669.9496        6.0795288<br/> 833.0903      824.9449        8.1453247<br/> 996.7983      986.9421        9.8562012<br/> 1166.798      1155.941       10.8568115<br/> 1342.71       1331.942       10.7680664<br/> 1524.14       1514.944        9.1958008<br/> 1710.689      1704.949        5.7398682<br/> 1901.955      1901.955        0.0001221</p><p>Sorry if the email server reformats that all screwy.</p><p>Dan N</p><p>_______________</p><p>&apos;ARIBAYES.BAS</p><p>DECLARE FUNCTION lb! (x!)<br/>DEFSNG A-Z</p><p>CLS</p><p>m = 2 &apos;Period interval size<br/>n = 12 &apos;Number of divisions<br/>p = 1 / 2 &apos; Initial probability for Bayes</p><p>FOR k = 0 TO n</p><p>&apos;Bayes<br/>x = 1200 * lb(1 + ((2 * m - 1) * p / (1 - p)) ^ (k / n) * (1 - p) / p) -<br/>1200<br/>PRINT x,</p><p>&apos;Arithmetic sum<br/>IF k = 1 THEN a = x<br/>IF k &gt; 0 THEN y = k * (a + (k - 1) * (1200 * lb(m) - n * a) / (n * (n - 1)))<br/>ELSE y = 0<br/>PRINT y,</p><p>PRINT USING &quot;####.#######&quot;; x - y</p><p>NEXT</p><p>FUNCTION lb (x)<br/>lb = LOG(x) / LOG(2)<br/>END FUNCTION</p></div><h3><a id=19255 href="#19255">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/1/2011 11:09:39 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jun 2, 2011 at 1:04 AM, Daniel Nielsen &lt;<a href="mailto:nielsed@uah.edu">nielsed@uah.edu</a>&gt; wrote:</p><p>&gt; I noticed that the previously given Bayes tuning is very close to Shaahin<br/>&gt; Mohajeri&apos;s ADO, differing by less than 3 cents at max, when the initial ADO<br/>&gt; constant (what is called A1 at<br/>&gt; <a href="http://sites.google.com/site/240edo/arithmeticrationaldivisionsofoctave">http://sites.google.com/site/240edo/arithmeticrationaldivisionsofoctave</a>)<br/>&gt; is set to the first iteration value of the Bayes sequence.<br/>&gt;</p><p>Oops! - meant to link to<br/><a href="http://sites.google.com/site/240edo/arithmeticirrationaldivisions(aid)">http://sites.google.com/site/240edo/arithmeticirrationaldivisions(aid)</a></p></div><h3><a id=19256 href="#19256">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/2/2011 7:00:10 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Some .SCL files of the Bayes stuff is at DanNielsen/bayes.zip</p><p>It was generated with..</p><p>DECLARE FUNCTION lb! (x!)<br/>DEFDBL A-Z</p><p>CLS</p><p>DIM div(5)</p><p>div(0) = 12<br/>div(1) = 15<br/>div(2) = 17<br/>div(3) = 19<br/>div(4) = 22</p><p>FOR i = 0 TO 4</p><p>m = 2<br/>n = div(i)<br/>p = 1 / 2</p><p>file$ = &quot;bayes&quot; + RIGHT$(STR$(n), 2) + &quot;.scl&quot;</p><p>OPEN file$ FOR OUTPUT AS #1</p><p>PRINT #1, &quot;! bayes&quot;;<br/>PRINT #1, USING &quot;##&quot;; n;<br/>PRINT #1, &quot;.scl&quot;<br/>PRINT #1, &quot;!&quot;<br/>PRINT #1, &quot;Iterative binary Bayes (uniform data, initial prior=.5)&quot;<br/>PRINT #1, n<br/>PRINT #1, &quot;!&quot;</p><p>FOR k = 1 TO n - 1</p><p>&apos;Bayes<br/>x = 1200 * lb(1 + ((2 * m - 1) * p / (1 - p)) ^ (k / n) * (1 - p) / p) -<br/>1200<br/>PRINT #1, USING &quot;#####.#############&quot;; x</p><p>NEXT k</p><p>PRINT #1, &quot; &quot;;<br/>PRINT #1, USING &quot;#&quot;; m;<br/>PRINT #1, &quot;/1&quot;</p><p>CLOSE #1</p><p>NEXT i</p><p>DEFSNG A-Z<br/>FUNCTION lb (x)<br/>lb = LOG(x) / LOG(2)<br/>END FUNCTION</p></div><h3><a id=19257 href="#19257">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/2/2011 6:32:24 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This time Bayes was compared to both ADO (as before) and ADL. ADO was a<br/>closer fit, as expected, since it&apos;s so friggin&apos; close to begin with:</p><p>(in cents; 12 divisions of 2/1)</p><p>BAYES        ADO        Diff           ADL         Diff<br/>     0.0000     0.0000     0.0000     0.0000     0.0000<br/>    81.0613    81.0613     0.0000    81.0613    -0.0000<br/>   165.7414   165.5660     0.1754   164.6934     1.0479<br/>   254.0176   253.5141     0.5035   251.0874     2.9302<br/>   345.8528   344.9056     0.9472   340.4574     5.3954<br/>   441.1955   439.7405     1.4549   433.0443     8.1511<br/>   539.9812   538.0188     1.9624   529.1211    10.8601<br/>   642.1331   639.7405     2.3926   628.9982    13.1350<br/>   747.5635   744.9056     2.6579   733.0311    14.5324<br/>   856.1748   853.5141     2.6607   841.6295    14.5452<br/>   967.8609   965.5660     2.2949   955.2692    12.5916<br/>  1082.5088  1081.0613     1.4475  1074.5069     8.0019<br/>  1200.0000  1200.0000     0.0000  1200.0000     0.0000</p><p>Produced with..<br/>_____________</p><p>&apos;ARIBAYES.BAS</p><p>DECLARE FUNCTION lb# (x#)<br/>DEFDBL A-Z</p><p>CLS</p><p>m = 2<br/>n = 12<br/>p = .5</p><p>FOR k = 0 TO n</p><p>&apos;Bayes<br/>x = 1200 * lb(1 + ((2 * m - 1) * p / (1 - p)) ^ (k / n) * (1 - p) / p) -<br/>1200<br/>PRINT USING &quot;######.####&quot;; x;</p><p>&apos;ADm<br/>IF k = 1 THEN a = x<br/>IF k &gt; 0 THEN y = k * (a + (k - 1) * (1200 * lb(m) - n * a) / (n * (n - 1)))<br/>ELSE y = 0<br/>PRINT USING &quot;######.####&quot;; y;</p><p>PRINT USING &quot;######.####&quot;; x - y;</p><p>&apos;ADL<br/>IF k = 1 THEN b = 1 - 2 ^ (-x / 1200)<br/>IF k &gt; 0 THEN z = 1200 * lb(1 / (1 - (k * (b + (k - 1) * ((1 - 1 / m) - n *<br/>b) / (n * (n - 1)))))) ELSE z = 0<br/>PRINT USING &quot;######.####&quot;; z;</p><p>PRINT USING &quot;######.####&quot;; x - z</p><p>IF k MOD 24 = 23 THEN SLEEP</p><p>NEXT</p><p>FUNCTION lb (x)<br/>lb = LOG(x) / LOG(2)<br/>END FUNCTION</p></div><h3><a id=19258 href="#19258">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@gmail.com&#x3E;</h3><span>6/2/2011 8:54:35 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This is way over my head. Could you explain what you&apos;re doing in more<br/>layman&apos;s terms? I don&apos;t know anything about Bayesian inference or<br/>Binary Bayes or Weber&apos;s law.</p><p>In general, you will find that this is a very new, experimental, and<br/>interdisciplinary community - things you might assume that everyone<br/>knows are often things that only you know, and hence can contribute to<br/>the theory. So you might have to explain things from the start<br/>sometimes, or reference us to some further reading on it.</p><p>It seems really interesting though, and since you&apos;re claiming it has<br/>some kind of relationship to the Father example I posted, it looks<br/>worth getting into.</p><p>-Mike</p><p>On Thu, Jun 2, 2011 at 9:32 PM, Daniel Nielsen &lt;<a href="mailto:nielsed@uah.edu">nielsed@uah.edu</a>&gt; wrote:<br/>&gt;<br/>&gt; This time Bayes was compared to both ADO (as before) and ADL. ADO was a closer fit, as expected, since it&apos;s so friggin&apos; close to begin with:<br/>&gt; (in cents; 12 divisions of 2/1)<br/>&gt; BAYES &nbsp; &nbsp; &nbsp; &nbsp;ADO &nbsp; &nbsp; &nbsp; &nbsp;Diff &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ADL &nbsp; &nbsp; &nbsp; &nbsp; Diff<br/>&gt; &nbsp; &nbsp; &nbsp;0.0000 &nbsp; &nbsp; 0.0000 &nbsp; &nbsp; 0.0000 &nbsp; &nbsp; 0.0000 &nbsp; &nbsp; 0.0000<br/>&gt; &nbsp; &nbsp; 81.0613 &nbsp; &nbsp;81.0613 &nbsp; &nbsp; 0.0000 &nbsp; &nbsp;81.0613 &nbsp; &nbsp;-0.0000<br/>&gt; &nbsp; &nbsp;165.7414 &nbsp; 165.5660 &nbsp; &nbsp; 0.1754 &nbsp; 164.6934 &nbsp; &nbsp; 1.0479<br/>&gt; &nbsp; &nbsp;254.0176 &nbsp; 253.5141 &nbsp; &nbsp; 0.5035 &nbsp; 251.0874 &nbsp; &nbsp; 2.9302<br/>&gt; &nbsp; &nbsp;345.8528 &nbsp; 344.9056 &nbsp; &nbsp; 0.9472 &nbsp; 340.4574 &nbsp; &nbsp; 5.3954<br/>&gt; &nbsp; &nbsp;441.1955 &nbsp; 439.7405 &nbsp; &nbsp; 1.4549 &nbsp; 433.0443 &nbsp; &nbsp; 8.1511<br/>&gt; &nbsp; &nbsp;539.9812 &nbsp; 538.0188 &nbsp; &nbsp; 1.9624 &nbsp; 529.1211 &nbsp; &nbsp;10.8601<br/>&gt; &nbsp; &nbsp;642.1331 &nbsp; 639.7405 &nbsp; &nbsp; 2.3926 &nbsp; 628.9982 &nbsp; &nbsp;13.1350<br/>&gt; &nbsp; &nbsp;747.5635 &nbsp; 744.9056 &nbsp; &nbsp; 2.6579 &nbsp; 733.0311 &nbsp; &nbsp;14.5324<br/>&gt; &nbsp; &nbsp;856.1748 &nbsp; 853.5141 &nbsp; &nbsp; 2.6607 &nbsp; 841.6295 &nbsp; &nbsp;14.5452<br/>&gt; &nbsp; &nbsp;967.8609 &nbsp; 965.5660 &nbsp; &nbsp; 2.2949 &nbsp; 955.2692 &nbsp; &nbsp;12.5916<br/>&gt; &nbsp; 1082.5088 &nbsp;1081.0613 &nbsp; &nbsp; 1.4475 &nbsp;1074.5069 &nbsp; &nbsp; 8.0019<br/>&gt; &nbsp; 1200.0000 &nbsp;1200.0000 &nbsp; &nbsp; 0.0000 &nbsp;1200.0000 &nbsp; &nbsp; 0.0000</p></div><h3><a id=19259 href="#19259">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/3/2011 12:11:40 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jun 2, 2011 at 10:54 PM, Mike Battaglia &lt;<a href="mailto:battaglia01@gmail.com">battaglia01@gmail.com</a>&gt;wrote:<br/>&gt;<br/>&gt;  This is way over my head. Could you explain what you&apos;re doing in more<br/>&gt; layman&apos;s terms? I don&apos;t know anything about Bayesian inference or<br/>&gt; Binary Bayes or Weber&apos;s law.<br/>&gt;<br/>&gt; In general, you will find that this is a very new, experimental, and<br/>&gt; interdisciplinary community - things you might assume that everyone<br/>&gt; knows are often things that only you know, and hence can contribute to<br/>&gt; the theory. So you might have to explain things from the start<br/>&gt; sometimes, or reference us to some further reading on it.<br/>&gt;<br/>I did link to a chapter (Elementary Hypothesis Testing, Probability Theory:<br/>The Logic of Science, <a href="http://www-biba.inrialpes.fr/Jaynes/cc04q.pdf">http://www-biba.inrialpes.fr/Jaynes/cc04q.pdf</a>) in ET<br/>Jaynes&apos; posthumously published book, but I can understand if you don&apos;t want<br/>to go through those pages. I haven&apos;t read it in a good while, so take<br/>everything I say with a grain of salt. The statistical devil is often in the<br/>details.</p><p>Lemme try to put it in a nutshell.</p><p>We have 2 hypotheses in mind concerning the state of an urn full of balls,<br/>where each ball is known to be either white or black, as well as some<br/>knowledge about how we deal with this urn. The hypotheses are statements<br/>that are mutually exclusive and exhaustive; i.e., they cover all possible<br/>conclusions without overlapping. For instance, we might know that we woke up<br/>in either Whitetown or Blackville (please forgive unintentional racial<br/>otones! :/), but not which. If we know that urns in Whitetown are 80% white,<br/>and urns in Blackville are 90% black, then the two hypotheses are<br/>Hyp. A:              p(white) = 80%<br/>Hyp. B = not A:  p(white) = 10%</p><p>Let&apos;s say that every time we draw a ball we replace it and shake<br/>(&quot;randomize&quot;) the urn again. This is called sampling with replacement, and<br/>it leads to much easier likelihood calculations, since each sampling is now<br/>&quot;logically independent&quot; of every other.</p><p>FOOTNOTE PARAGRAPH:<br/>(The sampling distribution that results from this assumption is the binomial<br/>one. In general, a sampling distribution just describes how many ways there<br/>are of arriving at our current state as compared to the other possible<br/>states; for instance, drawing 100 balls from an urn with 50% probability of<br/>giving white, it is less likely that we would draw 100 white balls than 50<br/>white and 50 black, because there are more &quot;paths&quot; that arrive at 50/100<br/>than 100/100. If we were sampling without replacement, the hypergeometric<br/>distribution would result, unless the urn contains so many balls we are only<br/>&quot;taking a drop from the bucket&quot;. The sampling distribution does not matter<br/>for our purposes here, but I mention it because it could be useful in<br/>generating sequences of tones. Here&apos;s something completely unrelated that<br/>shows this idea of binomial numbers representing paths to a state:<br/><a href="http://reocities.com/Vienna/9349/combinatorics.html#pascal">http://reocities.com/Vienna/9349/combinatorics.html#pascal</a>)</p><p>So now we use the Bayes formula to update our conceived &quot;probability&quot; of<br/>drawing white every time we draw. If we draw white a lot, we will tend to<br/>think that p(white) is high, and consequently will tend toward hypothesis A.<br/>If we draw black a lot, we will judge p(white) as low, and tend toward<br/>hypothesis B.</p><p>At least, that would be one way to do it, but it is not the best<br/>computationally.</p><p>Instead, we can use the log-of-odds=log(p/(1-p)), and add the<br/>log-of-likelihood-ratio every time we draw (NOTE: here I&apos;m using the typical<br/>definition of likelihood ratio, not the reciprocal used in previous posts).<br/>Since each draw is independent, the log-of-likelihood-ratio will take one of<br/>2 constant values each draw, depending on whether white or black was drawn.<br/>Let&apos;s say these 2 values are L and s. (No, this is not a MOS, but why not<br/>co-opt the symbols, since one of them is likely to be smaller than the<br/>other?)</p><p>L = log-likelihood-ratio of white = lb [ p(white | A) / p(white | B) ] = lb<br/>.8 - lb .1 = 3<br/>s = log-likelihood-ratio of black = lb [ p(black | A) / p(black | B) ] = lb<br/>.2 - lb .9 = -2.17</p><p>We could compute all our samples in one fatally simple blow instead by</p><p>log-of-odds = prior-log-of-odds + M*L + N*s</p><p>where M and N are the number of white samples and black samples<br/>respectively.</p><p>WHY THIS IS IMPORTANT: Jaynes argues that this is how perception is done -<br/>that, for better or worse, we are hardwired for rapid binary Bayesian<br/>judgement-making. That is why we see the logarithmic relation so often, as<br/>in pitch perception - and it is related to the Weber and Shannon laws of<br/>human information processing.</p><p>OKAY, WHETHER OR NOT YOU BUY THAT, HOW IS THE BAYES TUNING CONSTRUCTED?</p><p>The Bayes tuning assumes that all the balls drawn from the urn are the same<br/>color; this gives us a nice sweep in one direction with which to define a<br/>tuning. One might guess that humans are used to processing such streams of<br/>uniform data results.</p><p>However, it would not be very interesting simply to use the formula</p><p>cents = 1200 * log-of-odds = 1200 * (prior-log-of-odds + k*L)   for k = 0..M</p><p>All that gives us is ED of some equivalence interval. By adjusting L, we can<br/>define what that equivalence interval would be, but what&apos;s the point of all<br/>this, if we only wind up at EDm?</p><p>Let us consider instead defining cents by log-of-probability, another useful<br/>value.<br/>By definition,</p><p>prob = 1 / (1 + 1 / (likelihood-ratio^k * initial-prior-odds))<br/>..so..<br/>cents = 1200 * log-of-prob = -1200 * lb(1 + 1 / (likelihood-ratio^k *<br/>initial-prior-odds))</p><p>Now that&apos;s what I&apos;m talking about :) The initial-prior-probability chosen<br/>was 1/2, a very sensible value, and the target value was set an octave away<br/>at 1/4. Since 1/2 to 1/4 is a range from -1200 to -2400 cents, the formula<br/>was ALTERED slightly to</p><p>cents = 1200 * [ lb(1 + 1 / (likelihood-ratio^k * initial-prior-odds)) - 1 ]</p><p>Okay, so that&apos;s the derivation.</p><p>What happens if we instead used the reciprocal of the likelihood ratio? What<br/>would that represent? Well, for instance, instead of<br/>L = lb[ p(white | A) / p(white | B) ],<br/>we would have<br/>L = lb[ p(white | B) / p(white | A) ],<br/>which would be exactly the likelihood ratio of white we would have assigned<br/>had Whitetown and Blackville traded places so that we were more likely to<br/>draw white in Blackville and vice-versa.</p><p>What is the point? I dunno, for some reason it seemed like a very<br/>straightforward modification, and it especially caught my interest since<br/>this reciprocal-likelihood-ratio version gave a range of 3/2 when the<br/>original&apos;s range was set to 2/1. Also, what it produces is a tuning that,<br/>when subtracted from the other tuning cent value by cent value, gives the<br/>EDm that resulted in the log-of-odds expression. I tried to explain why in<br/>another post, but I don&apos;t know how much sense it made (even to myself).</p><p>I do believe it&apos;s a valuable decomposition of EDm, and the fact that the<br/>Bayes tuning over 2/1 is almost exactly the same as Shahiin&apos;s ADO seems to<br/>indicate that it may be even more significant as regards typical sensory<br/>progressions.</p></div><h3><a id=19260 href="#19260">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>6/4/2011 11:42:12 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Daniel Nielsen &lt;nielsed@...&gt; wrote:<br/>&gt;<br/>&gt; Some .SCL files of the Bayes stuff is at DanNielsen/bayes.zip</p><p>I haven&apos;t been able to make much of these as tuning systems. Maybe 5 or 7 notes would be good to look at as a scale, if you want to add those.</p></div><h3><a id=19261 href="#19261">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/4/2011 2:27:12 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thanks, Gene, here is the new file:<br/><a href="/tuning-math/files/DanNielsen/">/tuning-math/files/DanNielsen/</a></p><p>________</p><p>On Sat, Jun 4, 2011 at 1:42 PM, genewardsmith<br/>&lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt;wrote:</p><p>&gt;<br/>&gt;<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Daniel Nielsen &lt;nielsed@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Some .SCL files of the Bayes stuff is at DanNielsen/bayes.zip<br/>&gt;<br/>&gt; I haven&apos;t been able to make much of these as tuning systems. Maybe 5 or 7<br/>&gt; notes would be good to look at as a scale, if you want to add those.<br/>&gt;</p></div><h3><a id=19262 href="#19262">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/4/2011 2:40:17 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;<br/>&gt; On Sat, Jun 4, 2011 at 1:42 PM, genewardsmith &lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a><br/>&gt; &gt; wrote:<br/>&gt;<br/>I haven&apos;t been able to make much of these as tuning systems. Maybe 5 or 7<br/>&gt;&gt; notes would be good to look at as a scale, if you want to add those.<br/>&gt;&gt;<br/>&gt;&gt;</p><p>Funny, something that stands out right away due to the round number of<br/>cents: From a 12-tET POV, 5-tBayes and 7-tBayes seem potentially interesting<br/>(or related?), since they respectively hit 200. and 1000. almost dead on.</p></div><h3><a id=19263 href="#19263">ðŸ”—</a>Daniel Nielsen &#x3C;nielsed@uah.edu&#x3E;</h3><span>6/5/2011 1:23:42 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Does anything look interesting with the following scale?</p><p>It uses a slightly different approach:</p><p>* Consider 3/2 to be the beginning position</p><p>* Use the typical Bayesian construction given previously with N=7 divisions<br/>and a range of m=3/2. This fills in 7 notes from 3/2 to 1/1. (We had<br/>previously inverted results so that they ascended, but these pitches<br/>descend, so don&apos;t require the artificial inversion, but still use an offset<br/>to center at 3/2.)</p><p>* Take the reciprocal of the likelihood ratio and use N=5. These notes<br/>naturally fill in the space from 3/2 to 2/1 with 5 notes.</p><p>* Taken together, we have a 12 note scale over 2/1 (of course, that isn&apos;t a<br/>requirement):</p><p>! bayes_alt12.scl<br/>!<br/>Alternate Bayesian construction<br/>12<br/>!<br/>   112.3794<br/>   220.8697<br/>   325.3674<br/>   425.7850<br/>   522.0527<br/>   614.1197<br/>   3/2<br/>   817.7994<br/>   925.3725<br/>  1024.7917<br/>  1116.2483<br/>   2/1</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            