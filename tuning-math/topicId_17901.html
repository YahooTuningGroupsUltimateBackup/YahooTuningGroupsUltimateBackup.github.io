<a href="/tuning-math">back to list</a><h1>Observations about wedgie defined measures</h1><h3><a id=17901 href="#17901">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>5/29/2010 10:56:23 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>If W is an equal temperament val for n-edo, then ||W|| is about n, which is right for complexity. ||W^J|| is proportional to standard deviation, and ||W^J||/||W|| makes sense as an error measure; the adjustment to a variance from the mean value means that tendencies towards sharpness(eg 27edo) or flatness (eg 19edo) are taken into account. So it clearly works.</p><p>||W^J|| is defined on the Kees subspace or whatever we should call it, where k*J is projected down to 0 and the rest gives us a positive definite inner product and so a Euclidean space.</p><p>What about higher ranks? Once again, there&apos;s a projection down to a lower dimensional space with a Euclidean structure on that space going on, because the subspace of all u^J (for bivals), or all u^v^J (trivals) and etc. will be sent to zero. So you start out with a semidefinite form, associated to a symmetric matrix with some eigenvalues zero and the rest positive, and can convert it into a definite one by projection. What that all really means for us I have not thought about as yet. But there certainly seems to be no barrier to defining logflatness starting from here.</p></div><h3><a id=17902 href="#17902">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>5/29/2010 11:01:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 30 May 2010 09:56, genewardsmith &lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt; If W is an equal temperament val for n-edo, then ||W|| is about n,<br/>&gt; which is right for complexity. ||W^J|| is proportional to standard<br/>&gt; deviation, and ||W^J||/||W|| makes sense as an error measure;<br/>&gt; the adjustment to a variance from the mean value means that<br/>&gt; tendencies towards sharpness(eg 27edo) or flatness (eg 19edo)<br/>&gt; are taken into account. So it clearly works.</p><p>Proportional to what standard deviation?</p><p>&gt; ||W^J|| is defined on the Kees subspace or whatever we should<br/>&gt; call it, where k*J is projected down to 0 and the rest gives us a<br/>&gt; positive definite inner product and so a Euclidean space.</p><p>Kess subspace?  You mean it&apos;s positive definite if you define away the<br/>zeros (just intonation)?</p><p>&gt; What about higher ranks? Once again, there&apos;s a projection down<br/>&gt; to a lower dimensional space with a Euclidean structure on<br/>&gt; that space going on, because the subspace of all u^J (for bivals),<br/>&gt; or all u^v^J (trivals) and etc. will be sent to zero. So you start out<br/>&gt; with a semidefinite form, associated to a symmetric matrix with<br/>&gt; some eigenvalues zero and the rest positive, and can convert it<br/>&gt; into a definite one by projection. What that all really means for us<br/>&gt; I have not thought about as yet. But there certainly seems to be<br/>&gt; no barrier to defining logflatness starting from here.</p><p>Yes, it&apos;s positive semidefinite, and also a projection into a space<br/>with one less rank.</p><p>                       Graham</p></div><h3><a id=17903 href="#17903">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>5/29/2010 11:48:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@...&gt; wrote:<br/>&gt;<br/>&gt; On 30 May 2010 09:56, genewardsmith &lt;genewardsmith@...&gt; wrote:<br/>&gt; &gt; If W is an equal temperament val for n-edo, then ||W|| is about n,<br/>&gt; &gt; which is right for complexity. ||W^J|| is proportional to standard<br/>&gt; &gt; deviation, and ||W^J||/||W|| makes sense as an error measure;<br/>&gt; &gt; the adjustment to a variance from the mean value means that<br/>&gt; &gt; tendencies towards sharpness(eg 27edo) or flatness (eg 19edo)<br/>&gt; &gt; are taken into account. So it clearly works.<br/>&gt;<br/>&gt; Proportional to what standard deviation?</p><p>The standard deviation of the coordinates of W.</p><p>&gt; &gt; ||W^J|| is defined on the Kees subspace or whatever we should<br/>&gt; &gt; call it, where k*J is projected down to 0 and the rest gives us a<br/>&gt; &gt; positive definite inner product and so a Euclidean space.<br/>&gt;<br/>&gt; Kess subspace?  You mean it&apos;s positive definite if you define away the<br/>&gt; zeros (just intonation)?</p><p>I don&apos;t know what you mean. I was talking about the space of &quot;zero sized intervals&quot;, where J maps the whole thing to 0 (in other words, it&apos;s the null space for J.)</p></div><h3><a id=17906 href="#17906">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>5/30/2010 10:01:54 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 30 May 2010 10:48, genewardsmith &lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@...&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; On 30 May 2010 09:56, genewardsmith &lt;genewardsmith@...&gt; wrote:<br/>&gt;&gt; &gt; If W is an equal temperament val for n-edo, then ||W|| is about n,<br/>&gt;&gt; &gt; which is right for complexity. ||W^J|| is proportional to standard<br/>&gt;&gt; &gt; deviation, and ||W^J||/||W|| makes sense as an error measure;<br/>&gt;&gt; &gt; the adjustment to a variance from the mean value means that<br/>&gt;&gt; &gt; tendencies towards sharpness(eg 27edo) or flatness (eg 19edo)<br/>&gt;&gt; &gt; are taken into account. So it clearly works.<br/>&gt;&gt;<br/>&gt;&gt; Proportional to what standard deviation?<br/>&gt;<br/>&gt; The standard deviation of the coordinates of W.</p><p>The full rule is that the scalar badness is the square root of the<br/>determinant of the covariance matrix of the weighted mapping.  This is<br/>a standard deviation on the special case of rank 1.</p><p>                         Graham</p></div><h3><a id=17907 href="#17907">ðŸ”—</a>genewardsmith &#x3C;genewardsmith@sbcglobal.net&#x3E;</h3><span>5/30/2010 10:31:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@...&gt; wrote:</p><p>&gt; The full rule is that the scalar badness is the square root of the<br/>&gt; determinant of the covariance matrix of the weighted mapping.  This is<br/>&gt; a standard deviation on the special case of rank 1.</p><p>It&apos;s also probability and statistics terminology, which is confusing since it isn&apos;t appropriate in this case. I would prefer to say that you take a list of vals Vi in weighted coordinates, subtract off the average value times the JIP, obtaining a new list Ui, and then form the square matrix of dot products [Ui.Uj]. The determinant of that is, you are claiming, the same as ||W^J||? Or what?</p></div><h3><a id=17908 href="#17908">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>5/30/2010 10:44:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 31 May 2010 09:31, genewardsmith &lt;<a href="mailto:genewardsmith@sbcglobal.net">genewardsmith@sbcglobal.net</a>&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@...&gt; wrote:<br/>&gt;<br/>&gt;&gt; The full rule is that the scalar badness is the square root of the<br/>&gt;&gt; determinant of the covariance matrix of the weighted mapping. &nbsp;This is<br/>&gt;&gt; a standard deviation on the special case of rank 1.<br/>&gt;<br/>&gt; It&apos;s also probability and statistics terminology, which is confusing<br/>&gt; since it isn&apos;t appropriate in this case. I would prefer to say that<br/>&gt; you take a list of vals Vi in weighted coordinates, subtract off the<br/>&gt; average value times the JIP, obtaining a new list Ui, and then form<br/>&gt; the square matrix of dot products [Ui.Uj]. The determinant of that<br/>&gt; is, you are claiming, the same as ||W^J||? Or what?</p><p>You started the statistics by mentioning the standard deviation.  In<br/>this case it&apos;s a special case of the covariance matrix.  And the fact<br/>that this term &quot;covariance matrix&quot; exists for exactly what we want is<br/>surely worth mentioning, once statistics comes into it.</p><p>I&apos;m assuming Tenney weighting, so the JIP is what implies the sum.<br/>Otherwise, I think you&apos;re describing it correctly.</p><p>It&apos;s also an orthogonal projection.  It shouldn&apos;t be a surprise that<br/>wedge products are linked to orthogonal projections.  It happens that<br/>the covariances are also related.  But because they&apos;re in different<br/>books these relationships aren&apos;t usually remarked on.</p><p>                          Graham</p></div>