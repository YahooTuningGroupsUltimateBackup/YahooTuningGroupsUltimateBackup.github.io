<a href="/tuning-math">back to list</a><h1>More TOP/prime RMS code</h1><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/27/2005 4:25:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve got an algebraic solution to the rank 2 TOP now, using PuLP and<br/>GLPK.  First, though, here&apos;s the updated code for equal (rank 1)<br/>temperaments.  Usual caveats about indentation in Yahoo Groups apply.</p><p>  def getPORMSWE(self):<br/>    &quot;&quot;&quot;Return the prime, optimum, RMS, weighted error.</p><p>    This is the RMS of the prime intervals where octave stretching<br/>    is allowed, with each prime interval weighted according to its size.<br/>    &quot;&quot;&quot;<br/>    avgStretches, avgSquares = self.getPrimeStretching()<br/>    return sqrt(1.0 - (avgStretches**2 / avgSquares))</p><p>  def getPORMSWEStep(self):<br/>    &quot;&quot;&quot;Return the stretched step size<br/>       for the prime, optimum, RMS, weighted error.<br/>    &quot;&quot;&quot;<br/>    return self.getPORMSWEStretch() / self.basis[0]</p><p>  def getPORMSWEStretch(self):<br/>    &quot;&quot;&quot;Return the stretch for the prime, optimum, RMS, weighted error.<br/>    &quot;&quot;&quot;<br/>    avgStretches, avgSquares = self.getPrimeStretching()<br/>    return avgStretches / avgSquares</p><p>  def getPrimeStretching(self):<br/>    &quot;&quot;&quot;Used by getPORMSWE() and getPORMSWEStretch().<br/>    Not likely to be much use on its own.<br/>    &quot;&quot;&quot;<br/>    sumStretches = sumSquares = 0.0<br/>    for stretch in self.weightedPrimes():<br/>        sumStretches = sumStretches + stretch<br/>        sumSquares = sumSquares + stretch**2<br/>    return sumStretches/len(self.basis), sumSquares/len(self.basis)</p><p>  def getTOPError(self, stretch, wPrimes=None):<br/>    &quot;&quot;&quot;TOP Error for a given octave stretch (non-optimal)&quot;&quot;&quot;<br/>    worst = 0.0<br/>    for w in wPrimes or self.weightedPrimes():<br/>      w = w*stretch<br/>      if abs(1-w)&gt;worst:<br/>        worst = abs(1-w)<br/>    return worst</p><p>  def getTOP(self):<br/>    &quot;&quot;&quot;Return the TOP error and the optimum stretch&quot;&quot;&quot;<br/>    best, bestStretch = 1e50, 1.0<br/>    wPrimes = self.weightedPrimes()<br/>    for prime1 in wPrimes:<br/>      for prime2 in wPrimes:<br/>        stretch = prime1/prime2<br/>        error = self.getTOPError(stretch, wPrimes)<br/>        if error &lt; best:<br/>          best = error<br/>          bestStretch = stretch<br/>    return best, bestStretch</p><p>  def weightedPrimes(self):<br/>    &quot;&quot;&quot;Used for calculating and optimizing weighted prime errors&quot;&quot;&quot;<br/>    result = [1.0]<br/>    for i in range(1,len(self.basis)):<br/>      result.append(self.basis[i]/self.primes[i-1]/self.basis[0])<br/>    return result</p><p>Now here&apos;s the rank 2 code:</p><p>  def optimizePORMSWE(self):<br/>    &quot;&quot;&quot;Set the prime, optimum, RMS, weighted errors optimum&quot;&quot;&quot;<br/>    sx0 = sx1 = sx02 = sx12 = sx01 = 0.0</p><p>    primes = [1.0]+self.primes<br/>    for i in range(len(primes)):<br/>      m = self.mapping[i]<br/>      x0 = m[0]/primes[i]<br/>      x1 = m[1]/primes[i]<br/>      sx0 = sx0 + x0<br/>      sx1 = sx1 + x1<br/>      sx02 = sx02 + x0**2<br/>      sx12 = sx12 + x1**2<br/>      sx01 = sx01 + x0*x1</p><p>    denom = sx02*sx12 - sx01**2<br/>    self.basis = ((sx0*sx12 - sx1*sx01)/denom,<br/>        (sx1*sx02 - sx0*sx01)/denom)</p><p>  def getPRMSWError(self):<br/>    &quot;&quot;&quot;Get the prime, RMS, weighted error&quot;&quot;&quot;<br/>    primes = [1.0]+self.primes<br/>    total = 0.0<br/>    for i in range(len(primes)):<br/>      m = self.mapping[i]<br/>      error = (self.basis[0]*m[0] + self.basis[1]*m[1])/primes[i] - 1<br/>      total = total + error*error<br/>    return sqrt(total/len(primes))</p><p>  def optimizeTOP(self):<br/>    &quot;&quot;&quot;Set the TOP generators</p><p>    Requires PuLP and GLPK<br/>    &quot;&quot;&quot;<br/>    import pulp<br/>    prob = pulp.LpProblem(&quot;top&quot;, pulp.LpMinimize)</p><p>    # set three variables: the generators and the thing to minimize<br/>    # all of them have to be positive<br/>    period = pulp.LpVariable(&quot;period&quot;, 0, None)<br/>    generator = pulp.LpVariable(&quot;generator&quot;, 0, None)<br/>    error = pulp.LpVariable(&quot;error&quot;, 0, None)</p><p>    # specify that it&apos;s the error we want to minimize<br/>    prob.__iadd__((error, &quot;obj&quot;))<br/>    # uses __iadd__() instead of += for syntax compatibility<br/>    # with Python 1.5.2</p><p>    # now set the errors of the temperament as constraints<br/>    primes = [1.0]+self.primes<br/>    for i in range(len(primes)):<br/>      weightedPrime = (period*self.mapping[i][0] +<br/>              generator*self.mapping[i][1])/primes[i]<br/>      # set two error constraints for an overall absolute error<br/>      prob.__iadd__(error &gt;= weightedPrime - 1)<br/>      prob.__iadd__(error &gt;= 1 - weightedPrime)</p><p>      prob.solve(pulp.GLPK(msg=0))</p><p>      self.basis = period.varValue, generator.varValue</p><p>  def getTOPError(self):<br/>    &quot;&quot;&quot;Get the TOP (not necessarily optimum) error&quot;&quot;&quot;<br/>    primes = [1.0]+self.primes<br/>    max = 0.0<br/>    for i in range(len(primes)):<br/>      m = self.mapping[i]<br/>      error = (self.basis[0]*m[0] + self.basis[1]*m[1])/primes[i] - 1<br/>      if abs(error)&gt;max:<br/>          max = abs(error)<br/>    return max</p><p>It happens that the optimization code has the same number of lines for<br/>both TOP and prime RMS.  The RMS doesn&apos;t return the optimal error as<br/>a side effect, but then it doesn&apos;t rely on thousands of lines of<br/>library code either.  I&apos;ve benchmarked the TOP optimization at a<br/>shocking three -- count &apos;em -- three orders of magnitute slower than<br/>the prime RMS.  Here are the results of that.</p><p> 5-limit ET RMS in   1.0 ms<br/> 5-limit ET TOP in   3.4 ms<br/> 5-limit R2 RMS in  14.5 ms<br/> 5-limit R2 TOP in  26.5 s<br/> 7-limit ET RMS in   1.1 ms<br/> 7-limit ET TOP in   6.6 ms<br/> 7-limit R2 RMS in  17.4 ms<br/> 7-limit R2 TOP in  35.8 s<br/>11-limit ET RMS in   1.3 ms<br/>11-limit ET TOP in  11.4 ms<br/>11-limit R2 RMS in  20.4 ms<br/>11-limit R2 TOP in  44.9 s<br/>13-limit ET RMS in   1.4 ms<br/>13-limit ET TOP in  18.1 ms<br/>13-limit R2 RMS in  23.2 ms<br/>13-limit R2 TOP in  54.2 s<br/>17-limit ET RMS in   1.6 ms<br/>17-limit ET TOP in  26.9 ms<br/>17-limit R2 RMS in  26.1 ms<br/>17-limit R2 TOP in  63.7 s<br/>19-limit ET RMS in   1.8 ms<br/>19-limit ET TOP in  38.4 ms<br/>19-limit R2 RMS in  29.2 ms<br/>19-limit R2 TOP in  73.6 s</p><p>I don&apos;t know what&apos;s going wrong, seeing that GLPK uses a simplex algorithm,<br/>which everybody says is fast.  Still, slow it is.  Every time I interrupted<br/>it, it was in the C coded optimization, so that part must be significantly<br/>slow.  I ran the test using 30 equal temperaments and all the rank 2<br/>temperaments generated from them.  Here&apos;s the full test code:</p><p>import temper, time<br/>limits = 1, 3, 5, 7, 11, 13, 17, 19<br/>for d in range(2,8):<br/>    ets = [temper.PrimeET(n, temper.primes[:d]) for n in range(30,60)]<br/>    timestamp = time.time()<br/>    for n in range(1000):<br/>        for et in ets:<br/>            et.getPORMSWE()<br/>    print &quot;%2i-limit ET RMS in %5.1f ms&quot; % (limits[d], time.time()-timestamp)<br/>    timestamp = time.time()<br/>    for n in range(1000):<br/>        for et in ets:<br/>            et.getTOP()<br/>    print &quot;%2i-limit ET TOP in %5.1f ms&quot; % (limits[d], time.time()-timestamp)</p><p>    # now for rank 2 temperamnts (linear temperaments and their kin)<br/>    temper.Temperament(7,5,temper.limit5).optimizeTOP() # initialize PuLP<br/>    r2s = []<br/>    for i in range(len(ets)-1):<br/>        et1 = ets[i]<br/>        for j in range(i+1, len(ets)):<br/>            et2 = ets[j]<br/>            try:<br/>                r2 = et1 &amp; et2<br/>            except temper.TemperamentException:<br/>                continue<br/>            r2s.append(r2)<br/>    timestamp = time.time()<br/>    for n in range(1000):<br/>        for r2 in r2s:<br/>            r2.optimizePORMSWE()<br/>    print &quot;%2i-limit R2 RMS in %5.1f ms&quot; % (limits[d], time.time()-timestamp)<br/>    timestamp = time.time()<br/>    for r2 in r2s:<br/>        r2.optimizeTOP()<br/>    print &quot;%2i-limit R2 TOP in %5.1f s&quot; % (limits[d], time.time()-timestamp)</p><p>                   Graham</p></div><h3>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/28/2005 4:47:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Here&apos;s my 2-line MATLAB code for calculating the TOP tuning for ETs<br/>in the 11-limit:</p><p>%r contains the &apos;val&apos; . . .</p><p>tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log(2)];</p><p>tmp=r/((min(tmp)+max(tmp))/2);</p><p>%tmp contains the TOP tuning of the primes.</p><p>And the (minimized) damage is given by:</p><p>err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>(11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>(11)/log(2)]);</p><p>The symbol &quot;./&quot; means element-by-element division.</p><p>I have to run now, unfortunately!</p><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; I&apos;ve got an algebraic solution to the rank 2 TOP now, using PuLP and<br/>&gt; GLPK.  First, though, here&apos;s the updated code for equal (rank 1)<br/>&gt; temperaments.  Usual caveats about indentation in Yahoo Groups<br/>apply.<br/>&gt;<br/>&gt;   def getPORMSWE(self):<br/>&gt;     &quot;&quot;&quot;Return the prime, optimum, RMS, weighted error.<br/>&gt;<br/>&gt;     This is the RMS of the prime intervals where octave stretching<br/>&gt;     is allowed, with each prime interval weighted according to its<br/>size.<br/>&gt;     &quot;&quot;&quot;<br/>&gt;     avgStretches, avgSquares = self.getPrimeStretching()<br/>&gt;     return sqrt(1.0 - (avgStretches**2 / avgSquares))<br/>&gt;<br/>&gt;   def getPORMSWEStep(self):<br/>&gt;     &quot;&quot;&quot;Return the stretched step size<br/>&gt;        for the prime, optimum, RMS, weighted error.<br/>&gt;     &quot;&quot;&quot;<br/>&gt;     return self.getPORMSWEStretch() / self.basis[0]<br/>&gt;<br/>&gt;   def getPORMSWEStretch(self):<br/>&gt;     &quot;&quot;&quot;Return the stretch for the prime, optimum, RMS, weighted<br/>error.<br/>&gt;     &quot;&quot;&quot;<br/>&gt;     avgStretches, avgSquares = self.getPrimeStretching()<br/>&gt;     return avgStretches / avgSquares<br/>&gt;<br/>&gt;   def getPrimeStretching(self):<br/>&gt;     &quot;&quot;&quot;Used by getPORMSWE() and getPORMSWEStretch().<br/>&gt;     Not likely to be much use on its own.<br/>&gt;     &quot;&quot;&quot;<br/>&gt;     sumStretches = sumSquares = 0.0<br/>&gt;     for stretch in self.weightedPrimes():<br/>&gt;         sumStretches = sumStretches + stretch<br/>&gt;         sumSquares = sumSquares + stretch**2<br/>&gt;     return sumStretches/len(self.basis), sumSquares/len(self.basis)<br/>&gt;<br/>&gt;   def getTOPError(self, stretch, wPrimes=None):<br/>&gt;     &quot;&quot;&quot;TOP Error for a given octave stretch (non-optimal)&quot;&quot;&quot;<br/>&gt;     worst = 0.0<br/>&gt;     for w in wPrimes or self.weightedPrimes():<br/>&gt;       w = w*stretch<br/>&gt;       if abs(1-w)&gt;worst:<br/>&gt;         worst = abs(1-w)<br/>&gt;     return worst<br/>&gt;<br/>&gt;   def getTOP(self):<br/>&gt;     &quot;&quot;&quot;Return the TOP error and the optimum stretch&quot;&quot;&quot;<br/>&gt;     best, bestStretch = 1e50, 1.0<br/>&gt;     wPrimes = self.weightedPrimes()<br/>&gt;     for prime1 in wPrimes:<br/>&gt;       for prime2 in wPrimes:<br/>&gt;         stretch = prime1/prime2<br/>&gt;         error = self.getTOPError(stretch, wPrimes)<br/>&gt;         if error &lt; best:<br/>&gt;           best = error<br/>&gt;           bestStretch = stretch<br/>&gt;     return best, bestStretch<br/>&gt;<br/>&gt;   def weightedPrimes(self):<br/>&gt;     &quot;&quot;&quot;Used for calculating and optimizing weighted prime errors&quot;&quot;&quot;<br/>&gt;     result = [1.0]<br/>&gt;     for i in range(1,len(self.basis)):<br/>&gt;       result.append(self.basis[i]/self.primes[i-1]/self.basis[0])<br/>&gt;     return result<br/>&gt;<br/>&gt; Now here&apos;s the rank 2 code:<br/>&gt;<br/>&gt;   def optimizePORMSWE(self):<br/>&gt;     &quot;&quot;&quot;Set the prime, optimum, RMS, weighted errors optimum&quot;&quot;&quot;<br/>&gt;     sx0 = sx1 = sx02 = sx12 = sx01 = 0.0<br/>&gt;<br/>&gt;     primes = [1.0]+self.primes<br/>&gt;     for i in range(len(primes)):<br/>&gt;       m = self.mapping[i]<br/>&gt;       x0 = m[0]/primes[i]<br/>&gt;       x1 = m[1]/primes[i]<br/>&gt;       sx0 = sx0 + x0<br/>&gt;       sx1 = sx1 + x1<br/>&gt;       sx02 = sx02 + x0**2<br/>&gt;       sx12 = sx12 + x1**2<br/>&gt;       sx01 = sx01 + x0*x1<br/>&gt;<br/>&gt;     denom = sx02*sx12 - sx01**2<br/>&gt;     self.basis = ((sx0*sx12 - sx1*sx01)/denom,<br/>&gt;         (sx1*sx02 - sx0*sx01)/denom)<br/>&gt;<br/>&gt;   def getPRMSWError(self):<br/>&gt;     &quot;&quot;&quot;Get the prime, RMS, weighted error&quot;&quot;&quot;<br/>&gt;     primes = [1.0]+self.primes<br/>&gt;     total = 0.0<br/>&gt;     for i in range(len(primes)):<br/>&gt;       m = self.mapping[i]<br/>&gt;       error = (self.basis[0]*m[0] + self.basis[1]*m[1])/primes[i] -<br/>1<br/>&gt;       total = total + error*error<br/>&gt;     return sqrt(total/len(primes))<br/>&gt;<br/>&gt;   def optimizeTOP(self):<br/>&gt;     &quot;&quot;&quot;Set the TOP generators<br/>&gt;<br/>&gt;     Requires PuLP and GLPK<br/>&gt;     &quot;&quot;&quot;<br/>&gt;     import pulp<br/>&gt;     prob = pulp.LpProblem(&quot;top&quot;, pulp.LpMinimize)<br/>&gt;<br/>&gt;     # set three variables: the generators and the thing to minimize<br/>&gt;     # all of them have to be positive<br/>&gt;     period = pulp.LpVariable(&quot;period&quot;, 0, None)<br/>&gt;     generator = pulp.LpVariable(&quot;generator&quot;, 0, None)<br/>&gt;     error = pulp.LpVariable(&quot;error&quot;, 0, None)<br/>&gt;<br/>&gt;     # specify that it&apos;s the error we want to minimize<br/>&gt;     prob.__iadd__((error, &quot;obj&quot;))<br/>&gt;     # uses __iadd__() instead of += for syntax compatibility<br/>&gt;     # with Python 1.5.2<br/>&gt;<br/>&gt;     # now set the errors of the temperament as constraints<br/>&gt;     primes = [1.0]+self.primes<br/>&gt;     for i in range(len(primes)):<br/>&gt;       weightedPrime = (period*self.mapping[i][0] +<br/>&gt;               generator*self.mapping[i][1])/primes[i]<br/>&gt;       # set two error constraints for an overall absolute error<br/>&gt;       prob.__iadd__(error &gt;= weightedPrime - 1)<br/>&gt;       prob.__iadd__(error &gt;= 1 - weightedPrime)<br/>&gt;<br/>&gt;       prob.solve(pulp.GLPK(msg=0))<br/>&gt;<br/>&gt;       self.basis = period.varValue, generator.varValue<br/>&gt;<br/>&gt;   def getTOPError(self):<br/>&gt;     &quot;&quot;&quot;Get the TOP (not necessarily optimum) error&quot;&quot;&quot;<br/>&gt;     primes = [1.0]+self.primes<br/>&gt;     max = 0.0<br/>&gt;     for i in range(len(primes)):<br/>&gt;       m = self.mapping[i]<br/>&gt;       error = (self.basis[0]*m[0] + self.basis[1]*m[1])/primes[i] -<br/>1<br/>&gt;       if abs(error)&gt;max:<br/>&gt;           max = abs(error)<br/>&gt;     return max<br/>&gt;<br/>&gt; It happens that the optimization code has the same number of lines<br/>for<br/>&gt; both TOP and prime RMS.  The RMS doesn&apos;t return the optimal error as<br/>&gt; a side effect, but then it doesn&apos;t rely on thousands of lines of<br/>&gt; library code either.  I&apos;ve benchmarked the TOP optimization at a<br/>&gt; shocking three -- count &apos;em -- three orders of magnitute slower than<br/>&gt; the prime RMS.  Here are the results of that.<br/>&gt;<br/>&gt;  5-limit ET RMS in   1.0 ms<br/>&gt;  5-limit ET TOP in   3.4 ms<br/>&gt;  5-limit R2 RMS in  14.5 ms<br/>&gt;  5-limit R2 TOP in  26.5 s<br/>&gt;  7-limit ET RMS in   1.1 ms<br/>&gt;  7-limit ET TOP in   6.6 ms<br/>&gt;  7-limit R2 RMS in  17.4 ms<br/>&gt;  7-limit R2 TOP in  35.8 s<br/>&gt; 11-limit ET RMS in   1.3 ms<br/>&gt; 11-limit ET TOP in  11.4 ms<br/>&gt; 11-limit R2 RMS in  20.4 ms<br/>&gt; 11-limit R2 TOP in  44.9 s<br/>&gt; 13-limit ET RMS in   1.4 ms<br/>&gt; 13-limit ET TOP in  18.1 ms<br/>&gt; 13-limit R2 RMS in  23.2 ms<br/>&gt; 13-limit R2 TOP in  54.2 s<br/>&gt; 17-limit ET RMS in   1.6 ms<br/>&gt; 17-limit ET TOP in  26.9 ms<br/>&gt; 17-limit R2 RMS in  26.1 ms<br/>&gt; 17-limit R2 TOP in  63.7 s<br/>&gt; 19-limit ET RMS in   1.8 ms<br/>&gt; 19-limit ET TOP in  38.4 ms<br/>&gt; 19-limit R2 RMS in  29.2 ms<br/>&gt; 19-limit R2 TOP in  73.6 s<br/>&gt;<br/>&gt; I don&apos;t know what&apos;s going wrong, seeing that GLPK uses a simplex<br/>algorithm,<br/>&gt; which everybody says is fast.  Still, slow it is.  Every time I<br/>interrupted<br/>&gt; it, it was in the C coded optimization, so that part must be<br/>significantly<br/>&gt; slow.  I ran the test using 30 equal temperaments and all the rank 2<br/>&gt; temperaments generated from them.  Here&apos;s the full test code:<br/>&gt;<br/>&gt; import temper, time<br/>&gt; limits = 1, 3, 5, 7, 11, 13, 17, 19<br/>&gt; for d in range(2,8):<br/>&gt;     ets = [temper.PrimeET(n, temper.primes[:d]) for n in range<br/>(30,60)]<br/>&gt;     timestamp = time.time()<br/>&gt;     for n in range(1000):<br/>&gt;         for et in ets:<br/>&gt;             et.getPORMSWE()<br/>&gt;     print &quot;%2i-limit ET RMS in %5.1f ms&quot; % (limits[d], time.time()-<br/>timestamp)<br/>&gt;     timestamp = time.time()<br/>&gt;     for n in range(1000):<br/>&gt;         for et in ets:<br/>&gt;             et.getTOP()<br/>&gt;     print &quot;%2i-limit ET TOP in %5.1f ms&quot; % (limits[d], time.time()-<br/>timestamp)<br/>&gt;<br/>&gt;     # now for rank 2 temperamnts (linear temperaments and their kin)<br/>&gt;     temper.Temperament(7,5,temper.limit5).optimizeTOP() #<br/>initialize PuLP<br/>&gt;     r2s = []<br/>&gt;     for i in range(len(ets)-1):<br/>&gt;         et1 = ets[i]<br/>&gt;         for j in range(i+1, len(ets)):<br/>&gt;             et2 = ets[j]<br/>&gt;             try:<br/>&gt;                 r2 = et1 &amp; et2<br/>&gt;             except temper.TemperamentException:<br/>&gt;                 continue<br/>&gt;             r2s.append(r2)<br/>&gt;     timestamp = time.time()<br/>&gt;     for n in range(1000):<br/>&gt;         for r2 in r2s:<br/>&gt;             r2.optimizePORMSWE()<br/>&gt;     print &quot;%2i-limit R2 RMS in %5.1f ms&quot; % (limits[d], time.time()-<br/>timestamp)<br/>&gt;     timestamp = time.time()<br/>&gt;     for r2 in r2s:<br/>&gt;         r2.optimizeTOP()<br/>&gt;     print &quot;%2i-limit R2 TOP in %5.1f s&quot; % (limits[d], time.time()-<br/>timestamp)<br/>&gt;<br/>&gt;<br/>&gt;                    Graham<br/>&gt;</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/29/2005 7:49:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/29/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; Here&apos;s my 2-line MATLAB code for calculating the TOP tuning for ETs<br/>&gt; in the 11-limit:</p><p>This is good!  Is this in your paper?  It must have been so simple I<br/>skipped over it.  It certainly isn&apos;t what Gene had on his website.<br/>The advantages over my algorithm are numerous: it&apos;s shorter, faster,<br/>and gives the right answer!</p><p>&gt; %r contains the &apos;val&apos; . . .<br/>&gt;<br/>&gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log(2)];</p><p>Looks like this is the list of weighted primes, which I call w.</p><p>w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log(2)];</p><p>&gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt;<br/>&gt; %tmp contains the TOP tuning of the primes.<br/>&gt;<br/>&gt; And the (minimized) damage is given by:<br/>&gt;<br/>&gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; (11)/log(2)]);</p><p>That can be simplified to</p><p>err = (max(w)-min(w))/(max(w)+min(w));</p><p>I don&apos;t know what the j&apos;s for or why you multiply by 1200.</p><p>This is good, because it&apos;s an octave-equivalent formulation of the<br/>error: the error depends on the weighted primes regardless of the<br/>octave stretch.  It means you can calculate the damage with only one<br/>iteration over the primes, and may be helpful in reducing the rank 2<br/>temperament problem to a one dimensional optimization (assuming you<br/>don&apos;t have a simple solution to that as well).</p><p>It&apos;s similar to the RMS result in that it&apos;s a deviation divided by an<br/>average.  And it can be approximated as</p><p>err = (max(w) - min(w))/2;</p><p>for most purposes, because the average weighted prime will be close to<br/>1.  That means the error function for rank 2 temperaments is piecewise<br/>linear and so may be easier to optimize.  It may be possible to add<br/>this constraint to the linear programming problem.</p><p>That means the TOP optimization for equal temperaments really is<br/>simpler and faster than the RMS now, so I&apos;m impressed.  But it looks<br/>like this is another special case, like the optimization of one comma,<br/>and the general regular temperament problem is still much more<br/>difficult.</p><p>The Python code is updated at:</p><p><a href="http://microtonal.co.uk/temper.py">http://microtonal.co.uk/temper.py</a></p><p>                                Graham</p></div><h3>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/30/2005 3:20:47 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP tuning for<br/>ETs<br/>&gt; &gt; in the 11-limit:<br/>&gt;<br/>&gt; This is good!  Is this in your paper?</p><p>See footnote xxx.</p><p>&gt; It must have been so simple I<br/>&gt; skipped over it.  It certainly isn&apos;t what Gene had on his website.</p><p>I hope that Gene feels better and then reconsiders this point, which<br/>I&apos;ve made before.</p><p>&gt; The advantages over my algorithm are numerous: it&apos;s shorter, faster,<br/>&gt; and gives the right answer!<br/>&gt;<br/>&gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt;<br/>&gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log<br/>(2)];<br/>&gt;<br/>&gt; Looks like this is the list of weighted primes, which I call w.<br/>&gt;<br/>&gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log(2)];<br/>&gt;<br/>&gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt;<br/>&gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt;<br/>&gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt;<br/>&gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log(7)/log(2)<br/>log<br/>&gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; &gt; (11)/log(2)]);<br/>&gt;<br/>&gt; That can be simplified to<br/>&gt;<br/>&gt; err = (max(w)-min(w))/(max(w)+min(w));</p><p>Huh! How do you derive that?</p><p>&gt; I don&apos;t know what the j&apos;s</p><p>Whoops -- I actually deleted stuff from the code I use, and forgot to<br/>get rid of the j.</p><p>&gt; for or why you multiply by 1200.</p><p>Dave Keenan&apos;s units.</p><p>&gt; This is good, because it&apos;s an octave-equivalent formulation of the<br/>&gt; error: the error depends on the weighted primes regardless of the<br/>&gt; octave stretch.  It means you can calculate the damage with only one<br/>&gt; iteration over the primes, and may be helpful in reducing the rank 2<br/>&gt; temperament problem to a one dimensional optimization (assuming you<br/>&gt; don&apos;t have a simple solution to that as well).</p><p>Haven&apos;t thought about that yet . . .</p><p>&gt; It&apos;s similar to the RMS result in that it&apos;s a deviation divided by<br/>an<br/>&gt; average.  And it can be approximated as<br/>&gt;<br/>&gt; err = (max(w) - min(w))/2;<br/>&gt;<br/>&gt; for most purposes, because the average weighted prime will be close<br/>to<br/>&gt; 1.  That means the error function for rank 2 temperaments is<br/>piecewise<br/>&gt; linear</p><p>That seemed obvious to me before. But how does this reasoning allow<br/>you to come to that conclusion about rank 2 temperaments? I don&apos;t<br/>follow your jump.</p><p>&gt; and so may be easier to optimize.</p><p>&gt; It may be possible to add<br/>&gt; this constraint to the linear programming problem.<br/>&gt;<br/>&gt; That means the TOP optimization for equal temperaments really is<br/>&gt; simpler and faster than the RMS now, so I&apos;m impressed.  But it looks<br/>&gt; like this is another special case, like the optimization of one<br/>comma,<br/>&gt; and the general regular temperament problem is still much more<br/>&gt; difficult.<br/>&gt;<br/>&gt; The Python code is updated at:<br/>&gt;<br/>&gt; <a href="http://microtonal.co.uk/temper.py">http://microtonal.co.uk/temper.py</a><br/>&gt;<br/>&gt;<br/>&gt;                                 Graham<br/>&gt;</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/30/2005 6:47:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 12/1/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:</p><p>&gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt;<br/>&gt; Huh! How do you derive that?</p><p>Oh Lordy -- I worked it out in my head one morning.  Let&apos;s see ... the<br/>2/(max(w)+min(w)) fomula means you scale by the average of the largest<br/>and smallest weighted prime.  The result is that both errors are equal<br/>(hence the minimax).</p><p>The largest weighted prime is 2*max(w)/(max(w) + min(w))</p><p>The error in it is 2*max(w)/(max(w) + min(w)) - 1<br/>and this is one of the maximum errors.</p><p>That gives (2*max(w)-max(w)-min(w))/(max(w)+min(w))<br/>or (max(w)-min(w))/(max(w)+min(w))</p><p>&gt; &gt; It&apos;s similar to the RMS result in that it&apos;s a deviation divided by<br/>&gt; an<br/>&gt; &gt; average.  And it can be approximated as<br/>&gt; &gt;<br/>&gt; &gt; err = (max(w) - min(w))/2;<br/>&gt; &gt;<br/>&gt; &gt; for most purposes, because the average weighted prime will be close<br/>&gt; to<br/>&gt; &gt; 1.  That means the error function for rank 2 temperaments is<br/>&gt; piecewise<br/>&gt; &gt; linear<br/>&gt;<br/>&gt; That seemed obvious to me before. But how does this reasoning allow<br/>&gt; you to come to that conclusion about rank 2 temperaments? I don&apos;t<br/>&gt; follow your jump.</p><p>Any regular temperament can be written in terms of weighted primes,<br/>and the relationship still holds.</p><p>                                 Graham</p></div><h3>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>12/1/2005 1:20:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 12/1/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt; &gt;<br/>&gt; &gt; Huh! How do you derive that?<br/>&gt;<br/>&gt; Oh Lordy -- I worked it out in my head one morning.  Let&apos;s see ...<br/>the<br/>&gt; 2/(max(w)+min(w)) fomula means you scale by the average of the<br/>largest<br/>&gt; and smallest weighted prime.<br/>&gt; The result is that both errors are equal<br/>&gt; (hence the minimax).<br/>&gt; The largest weighted prime is 2*max(w)/(max(w) + min(w))</p><p>After the scaling.</p><p>&gt; The error in it is 2*max(w)/(max(w) + min(w)) - 1<br/>&gt; and this is one of the maximum errors.<br/>&gt;<br/>&gt; That gives (2*max(w)-max(w)-min(w))/(max(w)+min(w))<br/>&gt; or (max(w)-min(w))/(max(w)+min(w))</p><p>Wow.</p><p>&gt; &gt; &gt; It&apos;s similar to the RMS result in that it&apos;s a deviation divided<br/>by<br/>&gt; &gt; an<br/>&gt; &gt; &gt; average.  And it can be approximated as<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; err = (max(w) - min(w))/2;<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; for most purposes, because the average weighted prime will be<br/>close<br/>&gt; &gt; to<br/>&gt; &gt; &gt; 1.  That means the error function for rank 2 temperaments is<br/>&gt; &gt; piecewise<br/>&gt; &gt; &gt; linear<br/>&gt; &gt;<br/>&gt; &gt; That seemed obvious to me before. But how does this reasoning<br/>allow<br/>&gt; &gt; you to come to that conclusion about rank 2 temperaments? I don&apos;t<br/>&gt; &gt; follow your jump.<br/>&gt;<br/>&gt; Any regular temperament can be written in terms of weighted primes,</p><p>What exactly does that mean?</p><p>&gt; and the relationship still holds.<br/>&gt;<br/>&gt;<br/>&gt;                                  Graham</p></div><h3>Paul G Hjelmstad &#x3C;paul_hjelmstad@allianzlife.com&#x3E;</h3><span>12/1/2005 3:08:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt;<br/>&gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP tuning for<br/>&gt; ETs<br/>&gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt;<br/>&gt; &gt; This is good!  Is this in your paper?<br/>&gt;<br/>&gt; See footnote xxx.<br/>&gt;<br/>&gt; &gt; It must have been so simple I<br/>&gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on his website.<br/>&gt;<br/>&gt; I hope that Gene feels better and then reconsiders this point,<br/>which<br/>&gt; I&apos;ve made before.<br/>&gt;<br/>&gt; &gt; The advantages over my algorithm are numerous: it&apos;s shorter,<br/>faster,<br/>&gt; &gt; and gives the right answer!<br/>&gt; &gt;<br/>&gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log<br/>&gt; (2)];<br/>&gt; &gt;<br/>&gt; &gt; Looks like this is the list of weighted primes, which I call w.<br/>&gt; &gt;<br/>&gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log(2)];<br/>&gt; &gt;<br/>&gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log(7)/log<br/>(2)<br/>&gt; log<br/>&gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt;<br/>&gt; &gt; That can be simplified to<br/>&gt; &gt;<br/>&gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));</p><p>Maybe I have no place butting in here, but I get 1408.7 for err(j)<br/>and 0.37159 for err. This is using Octave. Just for fun, what are<br/>your values, and I will enjoy reverse engineering your formulas.</p><p>Paul Hj</p></div><h3>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>12/1/2005 3:16:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>&gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP tuning<br/>for<br/>&gt; &gt; ETs<br/>&gt; &gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; This is good!  Is this in your paper?<br/>&gt; &gt;<br/>&gt; &gt; See footnote xxx.<br/>&gt; &gt;<br/>&gt; &gt; &gt; It must have been so simple I<br/>&gt; &gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on his<br/>website.<br/>&gt; &gt;<br/>&gt; &gt; I hope that Gene feels better and then reconsiders this point,<br/>&gt; which<br/>&gt; &gt; I&apos;ve made before.<br/>&gt; &gt;<br/>&gt; &gt; &gt; The advantages over my algorithm are numerous: it&apos;s shorter,<br/>&gt; faster,<br/>&gt; &gt; &gt; and gives the right answer!<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>(11)/log<br/>&gt; &gt; (2)];<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Looks like this is the list of weighted primes, which I call w.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log<br/>(2)];<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log(7)/log<br/>&gt; (2)<br/>&gt; &gt; log<br/>&gt; &gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2)<br/>log<br/>&gt; &gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; That can be simplified to<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt;<br/>&gt; Maybe I have no place butting in here, but I get 1408.7 for err(j)</p><p>The (j) part should have been deleted, as we discussed.</p><p>&gt; and 0.37159 for err.</p><p>Which tuning did you plug in?</p><p>&gt; This is using Octave. Just for fun, what are<br/>&gt; your values,</p><p>For which tuning?</p><p>&gt; and I will enjoy reverse engineering your formulas.<br/>&gt;<br/>&gt; Paul Hj<br/>&gt;</p></div><h3>Paul G Hjelmstad &#x3C;paul_hjelmstad@allianzlife.com&#x3E;</h3><span>12/2/2005 8:27:40 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; &gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>&gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP tuning<br/>&gt; for<br/>&gt; &gt; &gt; ETs<br/>&gt; &gt; &gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; This is good!  Is this in your paper?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; See footnote xxx.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; It must have been so simple I<br/>&gt; &gt; &gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on his<br/>&gt; website.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; I hope that Gene feels better and then reconsiders this point,<br/>&gt; &gt; which<br/>&gt; &gt; &gt; I&apos;ve made before.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; The advantages over my algorithm are numerous: it&apos;s shorter,<br/>&gt; &gt; faster,<br/>&gt; &gt; &gt; &gt; and gives the right answer!<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; (11)/log<br/>&gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; Looks like this is the list of weighted primes, which I call<br/>w.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log(11)/log<br/>&gt; (2)];<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log<br/>(7)/log<br/>&gt; &gt; (2)<br/>&gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2)<br/>&gt; log<br/>&gt; &gt; &gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; That can be simplified to<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt; &gt;<br/>&gt; &gt; Maybe I have no place butting in here, but I get 1408.7 for err(j)<br/>&gt;<br/>&gt; The (j) part should have been deleted, as we discussed.</p><p>So you&apos;re not using the err(j) formula at all?</p><p>&gt; &gt; and 0.37159 for err.<br/>&gt;<br/>&gt; Which tuning did you plug in?<br/>&gt;<br/>&gt; &gt; This is using Octave. Just for fun, what are<br/>&gt; &gt; your values,<br/>&gt;<br/>&gt; For which tuning?</p><p>Didn&apos;t really know what I was doing so I put r=5. What would<br/>be a real ordinary value for r?</p><p>Paul Hj</p></div><h3>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>12/2/2005 2:49:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot;<br/>&lt;perlich@a...&gt;<br/>&gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed<br/>&lt;gbreed@g...&gt;<br/>&gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP<br/>tuning<br/>&gt; &gt; for<br/>&gt; &gt; &gt; &gt; ETs<br/>&gt; &gt; &gt; &gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; This is good!  Is this in your paper?<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; See footnote xxx.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; It must have been so simple I<br/>&gt; &gt; &gt; &gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on his<br/>&gt; &gt; website.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; I hope that Gene feels better and then reconsiders this<br/>point,<br/>&gt; &gt; &gt; which<br/>&gt; &gt; &gt; &gt; I&apos;ve made before.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; The advantages over my algorithm are numerous: it&apos;s<br/>shorter,<br/>&gt; &gt; &gt; faster,<br/>&gt; &gt; &gt; &gt; &gt; and gives the right answer!<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; &gt; (11)/log<br/>&gt; &gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; Looks like this is the list of weighted primes, which I<br/>call<br/>&gt; w.<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>(11)/log<br/>&gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log<br/>&gt; (7)/log<br/>&gt; &gt; &gt; (2)<br/>&gt; &gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log<br/>(2)<br/>&gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; That can be simplified to<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Maybe I have no place butting in here, but I get 1408.7 for err<br/>(j)<br/>&gt; &gt;<br/>&gt; &gt; The (j) part should have been deleted, as we discussed.<br/>&gt;<br/>&gt; So you&apos;re not using the err(j) formula at all?</p><p>I&apos;m using it without the (j).</p><p>&gt; &gt; &gt; and 0.37159 for err.<br/>&gt; &gt;<br/>&gt; &gt; Which tuning did you plug in?<br/>&gt; &gt;<br/>&gt; &gt; &gt; This is using Octave. Just for fun, what are<br/>&gt; &gt; &gt; your values,<br/>&gt; &gt;<br/>&gt; &gt; For which tuning?<br/>&gt;<br/>&gt; Didn&apos;t really know what I was doing so I put r=5. What would<br/>&gt; be a real ordinary value for r?</p><p>r has to be a vector with five entries, representing the mapping of<br/>the first five primes to steps in any ET, such as [22 35 51 62 76]. I<br/>have no idea how Octave would or could do element-by-element division<br/>of a scalar by a vector!</p></div><h3>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>12/3/2005 2:34:45 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 12/2/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; &gt; Any regular temperament can be written in terms of weighted primes,<br/>&gt;<br/>&gt; What exactly does that mean?</p><p>For a regular tuning, the size of each prime interval (a prime number<br/>ratio in JI) is always the same, and you can work out all other<br/>intervals from the primes.  (At least, if the temperament&apos;s defined by<br/>primes, let&apos;s leave special cases aside for now.)  The weighted primes<br/>list is the prime sizes (cents or octaves or whatever) scaled by the<br/>weighting.  With Tenney weighting, that means each JI interval has a<br/>size of 1 (if the size and weight are in the same units).  For<br/>temperaments, the nearer to 1 the better.  The formulae for equal<br/>temperaments don&apos;t depend on the equal steps, only the sizes of the<br/>prime intervals and the weighting.</p><p>For a rank 2 (formerly linear) temperament, the weighted primes depend<br/>on the generator.  Any given octave-equivalent generator size gives a<br/>list of weighted primes that you can apply the formulae to.</p><p>                  Graham</p></div><h3>Paul G Hjelmstad &#x3C;paul_hjelmstad@allianzlife.com&#x3E;</h3><span>12/5/2005 9:55:01 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; &gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &gt; &gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot;<br/>&gt; &lt;perlich@a...&gt;<br/>&gt; &gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed<br/>&gt; &lt;gbreed@g...&gt;<br/>&gt; &gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP<br/>&gt; tuning<br/>&gt; &gt; &gt; for<br/>&gt; &gt; &gt; &gt; &gt; ETs<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; This is good!  Is this in your paper?<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; See footnote xxx.<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; It must have been so simple I<br/>&gt; &gt; &gt; &gt; &gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on his<br/>&gt; &gt; &gt; website.<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; I hope that Gene feels better and then reconsiders this<br/>&gt; point,<br/>&gt; &gt; &gt; &gt; which<br/>&gt; &gt; &gt; &gt; &gt; I&apos;ve made before.<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; The advantages over my algorithm are numerous: it&apos;s<br/>&gt; shorter,<br/>&gt; &gt; &gt; &gt; faster,<br/>&gt; &gt; &gt; &gt; &gt; &gt; and gives the right answer!<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; &gt; &gt; (11)/log<br/>&gt; &gt; &gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; Looks like this is the list of weighted primes, which I<br/>&gt; call<br/>&gt; &gt; w.<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; (11)/log<br/>&gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2) log<br/>&gt; &gt; (7)/log<br/>&gt; &gt; &gt; &gt; (2)<br/>&gt; &gt; &gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log(7)/log<br/>&gt; (2)<br/>&gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; That can be simplified to<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; Maybe I have no place butting in here, but I get 1408.7 for<br/>err<br/>&gt; (j)<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; The (j) part should have been deleted, as we discussed.<br/>&gt; &gt;<br/>&gt; &gt; So you&apos;re not using the err(j) formula at all?<br/>&gt;<br/>&gt; I&apos;m using it without the (j).<br/>&gt;<br/>&gt; &gt; &gt; &gt; and 0.37159 for err.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Which tuning did you plug in?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; This is using Octave. Just for fun, what are<br/>&gt; &gt; &gt; &gt; your values,<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; For which tuning?<br/>&gt; &gt;<br/>&gt; &gt; Didn&apos;t really know what I was doing so I put r=5. What would<br/>&gt; &gt; be a real ordinary value for r?<br/>&gt;<br/>&gt; r has to be a vector with five entries, representing the mapping of<br/>&gt; the first five primes to steps in any ET, such as [22 35 51 62 76].<br/>I<br/>&gt; have no idea how Octave would or could do element-by-element<br/>division<br/>&gt; of a scalar by a vector!</p><p>Matlab did it too. But my trial ran out 11/30/05. It must have<br/>treated r like 5 5 5 5 5!<br/>&gt;</p></div><h3>wallyesterpaulrus &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>12/6/2005 1:25:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot;<br/>&lt;perlich@a...&gt;<br/>&gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul G Hjelmstad&quot;<br/>&gt; &gt; &gt; &gt; &lt;paul_hjelmstad@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot;<br/>&gt; &gt; &lt;perlich@a...&gt;<br/>&gt; &gt; &gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed<br/>&gt; &gt; &lt;gbreed@g...&gt;<br/>&gt; &gt; &gt; &gt; &gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; On 11/29/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Here&apos;s my 2-line MATLAB code for calculating the TOP<br/>&gt; &gt; tuning<br/>&gt; &gt; &gt; &gt; for<br/>&gt; &gt; &gt; &gt; &gt; &gt; ETs<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in the 11-limit:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; This is good!  Is this in your paper?<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; See footnote xxx.<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; It must have been so simple I<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; skipped over it.  It certainly isn&apos;t what Gene had on<br/>his<br/>&gt; &gt; &gt; &gt; website.<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; I hope that Gene feels better and then reconsiders this<br/>&gt; &gt; point,<br/>&gt; &gt; &gt; &gt; &gt; which<br/>&gt; &gt; &gt; &gt; &gt; &gt; I&apos;ve made before.<br/>&gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; The advantages over my algorithm are numerous: it&apos;s<br/>&gt; &gt; shorter,<br/>&gt; &gt; &gt; &gt; &gt; faster,<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; and gives the right answer!<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; %r contains the &apos;val&apos; . . .<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tmp=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2)<br/>log<br/>&gt; &gt; &gt; &gt; (11)/log<br/>&gt; &gt; &gt; &gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Looks like this is the list of weighted primes, which I<br/>&gt; &gt; call<br/>&gt; &gt; &gt; w.<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; w=r./[1 log(3)/log(2) log(5)/log(2) log(7)/log(2) log<br/>&gt; &gt; (11)/log<br/>&gt; &gt; &gt; &gt; (2)];<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tmp=r/((min(tmp)+max(tmp))/2);<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; %tmp contains the TOP tuning of the primes.<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; And the (minimized) damage is given by:<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; err(j)=1200*max((tmp-[1 log(3)/log(2) log(5)/log(2)<br/>log<br/>&gt; &gt; &gt; (7)/log<br/>&gt; &gt; &gt; &gt; &gt; (2)<br/>&gt; &gt; &gt; &gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)])./[1 log(3)/log(2) log(5)/log(2) log<br/>(7)/log<br/>&gt; &gt; (2)<br/>&gt; &gt; &gt; &gt; log<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; (11)/log(2)]);<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; That can be simplified to<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; &gt; &gt; err = (max(w)-min(w))/(max(w)+min(w));<br/>&gt; &gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; Maybe I have no place butting in here, but I get 1408.7 for<br/>&gt; err<br/>&gt; &gt; (j)<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; The (j) part should have been deleted, as we discussed.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; So you&apos;re not using the err(j) formula at all?<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m using it without the (j).<br/>&gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; and 0.37159 for err.<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; Which tuning did you plug in?<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; This is using Octave. Just for fun, what are<br/>&gt; &gt; &gt; &gt; &gt; your values,<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; For which tuning?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Didn&apos;t really know what I was doing so I put r=5. What would<br/>&gt; &gt; &gt; be a real ordinary value for r?<br/>&gt; &gt;<br/>&gt; &gt; r has to be a vector with five entries, representing the mapping<br/>of<br/>&gt; &gt; the first five primes to steps in any ET, such as [22 35 51 62<br/>76].<br/>&gt; I<br/>&gt; &gt; have no idea how Octave would or could do element-by-element<br/>&gt; division<br/>&gt; &gt; of a scalar by a vector!<br/>&gt;<br/>&gt; Matlab did it too. But my trial ran out 11/30/05. It must have<br/>&gt; treated r like 5 5 5 5 5!</p><p>Yes, it did.</p></div>