<a href="/tuning-math">back to list</a><h1>TOP arguments</h1><h3><a id=13096 href="#13096">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>10/29/2005 6:02:24 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>There was a lot of fuss about TOP tuning a while back.  I won&apos;t actually review it here, so if you don&apos;t know what I&apos;m talking about you&apos;ll have to search back.  I&apos;ve always had reservations about it and have voiced some of them here.  But I haven&apos;t taken the time to present them all together until now.  I don&apos;t mean to say that TOP isn&apos;t a valid way of fixing the tuning or assessing the accuracy, but that it isn&apos;t the best and shouldn&apos;t be the standard.</p><p>One thing TOP does get right is that weighting tends to go with octave-specific consonance.  You can get pretty good measures by only considering the primes so long as you weight them and allow the octaves to become impure.  This follows from the &quot;triangular vs rectangular&quot; thread, and I hope we agree on that now.  With rectangular, octave-equivalent lattices you have 5:3 and 15:8 with the same complexity, so you have to consider the whole ratio or make subtraction easier than addition.</p><p>Tenney weighting&apos;s good in so far as it goes.  The odd-limit can be seen as an octave-equivalent simplification of Tenney weighting after all. Tenney weighting is like a null-weighting for primes-based measures because it means all intervals are considered equivalent in so far as that&apos;s possible.  But if you make it the only criterion it gives far too much weight to complex intervals.  I don&apos;t think you even get a stable optimization.  That&apos;s why you have to enforce a prime limit with these weighted schemes.  It guarantees that intervals you don&apos;t consider are more consonant than most of those you do, by the very same complexity measure you&apos;re looking for.</p><p>You could try having the weighting drop off faster with complexity.  The problem with that is, to get a stable result over all intervals, you&apos;ll have to get it to fall off so quickly that only the simplest intervals are going to have any importance at all.  If you care about the greater 9-limit, you&apos;ll have to intervene somewhere, and you may as well do it with a hard limit.</p><p>So we end up back with Tenney weighting and a prime limit.  It&apos;s a pragmatic approach that gives sensible results.  But, as it&apos;s still a compromise, the precise mathematical properties of TOP aren&apos;t important.   It&apos;s a perfect structure build on imperfect foundations.</p><p>My big, conceptual problem with TOP is that the minimax isn&apos;t appropriate for a weighted scheme.  The point of the minimax is that you put a limit on how bad the tuning can get.  The point of weighting is that you favour the most important intervals, and don&apos;t worry so much about the less important ones.  If simple intervals carry most of the harmony you care about, their tuning is more important than the more complex ones.  And good intervals in a chord can make up for bad ones. These considerations lead to some kind of mean error as the thing to be optimized.  I really don&apos;t think that the weighted maximum error is anything we can hear.  Ideally (if we had a theory we could trust) we&apos;d do a weighted-mean optimization provided an unweighted-minimax hurdle is passed.  The best weighting schemes would target a particular piece of music, and depend on how often different intervals are used in it.  You could even use it to vary the tuning over the piece.  For the general case you can guess that simple intervals are used most often, and so some Tenney-weighted mean is the best approximation.</p><p>TOP doesn&apos;t solve the probem of optimum octave-stretch.  There are too many factors that aren&apos;t considered once you leave octave-equivalence. The biggest problem is the effect of the actual size of the interval. Quite small intervals sound most dissonant, and very large intervals are inaudible.  Yet they have the same Tenney weight, and so are considered equally in the optimization if that&apos;s your only criterion.  I don&apos;t know the solution to this, because there isn&apos;t enough theory to rest on, but for the best solution I&apos;d want some kind of size-weighted minimax to make sure the smallest important intervals don&apos;t get out of control. Use the octave stretching to optimize 9:8 and 8:7, and leave 3:2 and 7:4 to look after themselves.  Soon enough you get such a complex system that you may as well do the tuning subjectively.  So TOP is fine to give you a general idea of the ideal stretch.  But as we don&apos;t know which system is correct we may as well use the simplest vaguely correct one, and TOP isn&apos;t it.  However, it is simple enough to have some validity.</p><p>The simplest weighted, prime-limit scheme is to optimize the RMS of the prime errors themselves.  This is actually a mixture of a mean and minimax measure.  It assumes errors always add up, whereas sometimes they cancel out, so 5:3 and 15:1 are always given the same error. That&apos;s why you have to optimize the octave.  If the errors would tend to cancel out in an odd-limit, the RMS of primes is unrepresentative.  But the octave stretching will compensate for this.  So, paradoxically, the octave-stretched optimum may be most valid after you unstretch the octaves.</p><p>The cleverest part of TOP is that it can still be applied with pure octaves and the Kees metric.  Is the &quot;Kees&quot; metric from &quot;Kees van Prooijen&quot; or something?  In that case, you can call the octave-equivalent TOP &quot;Un-Tempered Octaves, Prooijen Ideal Accuracy&quot;. That gives the acronym &quot;UTOPIA&quot; which is better than &quot;TOP&quot; while showing the relationship.  Even if that doesn&apos;t work, there&apos;s still &quot;Un-Tempered Octaves, Please, In All Numbers&quot;.  Despite this advantage, it requires more effort to calculate than the weighted primes least squares, so you may as well do the latter and restretch.</p><p>I still think the easiest metric to understand is the odd-limit minimax.   I can see practical advantages in the weighted prime least squares, which I&apos;d overlooked before.  But TOP is too difficult to calculate, and rests on too dubious theory, that it doesn&apos;t fill any useful niche.</p><p>                    Graham</p></div><h3><a id=13115 href="#13115">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>10/31/2005 7:10:47 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; There was a lot of fuss about TOP tuning a while back.  I won&apos;t<br/>actually<br/>&gt; review it here, so if you don&apos;t know what I&apos;m talking about you&apos;ll<br/>have<br/>&gt; to search back.  I&apos;ve always had reservations about it and have<br/>voiced<br/>&gt; some of them here.  But I haven&apos;t taken the time to present them<br/>all<br/>&gt; together until now.  I don&apos;t mean to say that TOP isn&apos;t a valid way<br/>of<br/>&gt; fixing the tuning or assessing the accuracy, but that it isn&apos;t the<br/>best<br/>&gt; and shouldn&apos;t be the standard.<br/>&gt;<br/>&gt; One thing TOP does get right is that weighting tends to go with<br/>&gt; octave-specific consonance.  You can get pretty good measures by<br/>only<br/>&gt; considering the primes so long as you weight them and allow the<br/>octaves<br/>&gt; to become impure.  This follows from the &quot;triangular vs<br/>rectangular&quot;<br/>&gt; thread, and I hope we agree on that now.  With rectangular,<br/>&gt; octave-equivalent lattices you have 5:3 and 15:8 with the same<br/>&gt; complexity, so you have to consider the whole ratio or make<br/>subtraction<br/>&gt; easier than addition.<br/>&gt;<br/>&gt; Tenney weighting&apos;s good in so far as it goes.  The odd-limit can be<br/>seen<br/>&gt; as an octave-equivalent simplification of Tenney weighting after<br/>all.<br/>&gt; Tenney weighting is like a null-weighting for primes-based measures<br/>&gt; because it means all intervals are considered equivalent in so far<br/>as<br/>&gt; that&apos;s possible.  But if you make it the only criterion it gives<br/>far too<br/>&gt; much weight to complex intervals.  I don&apos;t think you even get a<br/>stable<br/>&gt; optimization.  That&apos;s why you have to enforce a prime limit with<br/>these<br/>&gt; weighted schemes.</p><p>I&apos;m dubious on this part of your claims. Can you set up an unstable<br/>optimization to show this?</p><p>&gt; It guarantees that intervals you don&apos;t consider are<br/>&gt; more consonant than most of those you do, by the very same<br/>complexity<br/>&gt; measure you&apos;re looking for.</p><p>I&apos;m not sure exactly what you mean by this.</p><p>&gt; You could try having the weighting drop off faster with<br/>complexity.  The<br/>&gt; problem with that is, to get a stable result over all intervals,<br/>you&apos;ll<br/>&gt; have to get it to fall off so quickly that only the simplest<br/>intervals<br/>&gt; are going to have any importance at all.  If you care about the<br/>greater<br/>&gt; 9-limit, you&apos;ll have to intervene somewhere, and you may as well do<br/>it<br/>&gt; with a hard limit.</p><p>This is the great thing about TOP. You can place the hard limit just<br/>about anywhere you want and it won&apos;t change the results! I admit that<br/>TOP may seem pretty unjustified if you don&apos;t realize this. But I<br/>tried to point it out in my paper . . . And the results are<br/>perfectly &quot;stable&quot;, in every sense.</p><p>&gt; So we end up back with Tenney weighting and a prime limit.  It&apos;s a<br/>&gt; pragmatic approach that gives sensible results.  But, as it&apos;s still<br/>a<br/>&gt; compromise,</p><p>I don&apos;t see it as a compromise. What is being compromised?</p><p>&gt; the precise mathematical properties of TOP aren&apos;t important.<br/>&gt;   It&apos;s a perfect structure build on imperfect foundations.<br/>&gt;<br/>&gt; My big, conceptual problem with TOP is that the minimax isn&apos;t<br/>&gt; appropriate for a weighted scheme.</p><p>Sure it is -- just as appropriate as with equal-weighting. I don&apos;t<br/>see why one would say otherwise.</p><p>&gt; The point of the minimax is that you<br/>&gt; put a limit on how bad the tuning can get.</p><p>Yes, and &quot;how bad&quot; is clearly something which can&apos;t be in units of<br/>cents for all the different intervals you&apos;re looking at, IMO.</p><p>&gt; The point of weighting is<br/>&gt; that you favour the most important intervals, and don&apos;t worry so<br/>much<br/>&gt; about the less important ones.  If simple intervals carry most of<br/>the<br/>&gt; harmony you care about, their tuning is more important than the<br/>more<br/>&gt; complex ones.</p><p>Yes, and the weighting is exactly what takes this into account.</p><p>&gt; And good intervals in a chord can make up for bad ones.</p><p>Possibly, though minimax is good enough for George Secor. Note that<br/>for a triad, max error and sum-absolute-errors amount to the same<br/>thing.</p><p>&gt; These considerations lead to some kind of mean error as the thing<br/>to be<br/>&gt; optimized.</p><p>OK, I&apos;m willing to delve further into your L2 variant of TOP . . . we<br/>really need to think more about what it means, though.</p><p>&gt; I really don&apos;t think that the weighted maximum error is<br/>&gt; anything we can hear.  Ideally (if we had a theory we could trust)<br/>we&apos;d<br/>&gt; do a weighted-mean optimization provided an unweighted-minimax<br/>hurdle is<br/>&gt; passed.</p><p>Why unweighted? You haven&apos;t given any arguments on that.</p><p>&gt; The best weighting schemes would target a particular piece of<br/>&gt; music, and depend on how often different intervals are used in it.<br/>You<br/>&gt; could even use it to vary the tuning over the piece.  For the<br/>general<br/>&gt; case you can guess that simple intervals are used most often, and<br/>so<br/>&gt; some Tenney-weighted mean is the best approximation.<br/>&gt;<br/>&gt; TOP doesn&apos;t solve the probem of optimum octave-stretch.  There are<br/>too<br/>&gt; many factors that aren&apos;t considered once you leave octave-<br/>equivalence.<br/>&gt; The biggest problem is the effect of the actual size of the<br/>interval.<br/>&gt; Quite small intervals sound most dissonant, and very large<br/>intervals are<br/>&gt; inaudible. Yet they have the same Tenney weight, and so are<br/>considered<br/>&gt; equally in the optimization if that&apos;s your only criterion.</p><p>That&apos;s not a fair statement. The optimization can be &quot;considered&quot; in<br/>many different ways while still yielding the same results.</p><p>&gt; I don&apos;t know<br/>&gt; the solution to this, because there isn&apos;t enough theory to rest on,<br/>but<br/>&gt; for the best solution I&apos;d want some kind of size-weighted minimax<br/>to<br/>&gt; make sure the smallest important intervals don&apos;t get out of<br/>control.</p><p>Again, you appear to be missing another important feature of TOP,<br/>which is that you&apos;re free to consider only the intervals within some<br/>range of interest for your optimization, and you still get exactly<br/>the same result!</p><p>&gt; Use the octave stretching to optimize 9:8 and 8:7, and leave 3:2<br/>and 7:4<br/>&gt; to look after themselves.</p><p>I don&apos;t understand that. Aren&apos;t they (the latter intervals) more<br/>damaged by mistuning?</p><p>&gt; Soon enough you get such a complex system<br/>&gt; that you may as well do the tuning subjectively.  So TOP is fine to<br/>give<br/>&gt; you a general idea of the ideal stretch.  But as we don&apos;t know<br/>which<br/>&gt; system is correct we may as well use the simplest vaguely correct<br/>one,<br/>&gt; and TOP isn&apos;t it.  However, it is simple enough to have some<br/>validity.<br/>&gt;<br/>&gt; The simplest weighted, prime-limit scheme is to optimize the RMS of<br/>the<br/>&gt; prime errors themselves.</p><p>I think you&apos;ve posted about that before but I don&apos;t think it&apos;s been<br/>discussed enough.</p><p>&gt; This is actually a mixture of a mean and<br/>&gt; minimax measure.</p><p>Well, for triads, mean and minimax give the same thing, while RMS<br/>gives something else. So I&apos;m not sure exactly what you mean by<br/>this . . .</p><p>&gt; It assumes errors always add up, whereas sometimes<br/>&gt; they cancel out, so 5:3 and 15:1 are always given the same error.</p><p>You lost me.</p><p>&gt; That&apos;s why you have to optimize the octave.  If the errors would<br/>tend to<br/>&gt; cancel out in an odd-limit, the RMS of primes is unrepresentative.<br/>But<br/>&gt; the octave stretching will compensate for this.  So, paradoxically,<br/>the<br/>&gt; octave-stretched optimum may be most valid after you unstretch the<br/>octaves.</p><p>Can you be a little more elaborate?</p><p>&gt; The cleverest part of TOP is that it can still be applied with pure<br/>&gt; octaves and the Kees metric.</p><p>I don&apos;t know what you mean. Though TOP, when stretched or compressed<br/>to have pure octaves, usually coincides with minimax Kees in the 5-<br/>limit, it doesn&apos;t in higher limits, according to Gene.</p><p>&gt; Is the &quot;Kees&quot; metric from &quot;Kees van<br/>&gt; Prooijen&quot; or something?</p><p>Yes. It&apos;s what we used to call log of &quot;odd limit&quot;.</p><p>&gt; In that case, you can call the<br/>&gt; octave-equivalent TOP &quot;Un-Tempered Octaves, Prooijen Ideal<br/>Accuracy&quot;.</p><p>Unfortunately this seems to have little to do with TOP (or Prooijen,<br/>depending what you mean) particularly beyond the 5-limit.</p><p>&gt; That gives the acronym &quot;UTOPIA&quot; which is better than &quot;TOP&quot; while<br/>showing<br/>&gt; the relationship.</p><p>The relationship breaks down, according to Gene.</p><p>&gt; Even if that doesn&apos;t work, there&apos;s still &quot;Un-Tempered<br/>&gt; Octaves, Please, In All Numbers&quot;.  Despite this advantage, it<br/>requires<br/>&gt; more effort to calculate than the weighted primes least squares, so<br/>you<br/>&gt; may as well do the latter and restretch.</p><p>If we can put your proposal on some good foundations . . . that has<br/>yet to be seen (by me at least). I was disappointed when Gene<br/>informed us that beyond the 5-limit, minimax Kees doesn&apos;t agree (even<br/>modulo stretching) with TOP; I&apos;m dubious that replacing minimax with<br/>sum-of-squares (assuming we can define the latter appropriately in<br/>the Kees case) will bring back the agreement . . .</p><p>&gt; I still think the easiest metric to understand is the odd-limit<br/>minimax.</p><p>Whew! Score one for minimax, then.</p><p>&gt;   I can see practical advantages in the weighted prime least<br/>squares,<br/>&gt; which I&apos;d overlooked before.  But TOP is too difficult to<br/>calculate, and<br/>&gt; rests on too dubious theory, that it doesn&apos;t fill any useful niche.</p><p>I find it odd that you&apos;d say this but of course you&apos;re entitled . . .<br/>In many cases of interest, TOP is extremely easy to calculate, as you<br/>know . . .</p></div><h3><a id=13117 href="#13117">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>10/31/2005 9:40:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; If we can put your proposal on some good foundations . . . that has<br/>&gt; yet to be seen (by me at least). I was disappointed when Gene<br/>&gt; informed us that beyond the 5-limit, minimax Kees doesn&apos;t agree (even<br/>&gt; modulo stretching) with TOP...</p><p>In practice in the 7 and 11 limits it seems it nearly always does.</p><p>&gt; In many cases of interest, TOP is extremely easy to calculate, as you<br/>&gt; know . . .</p><p>If you&apos;ve got a linear programming routine available (and they are<br/>easily found) it&apos;s quite simple to compute.</p></div><h3><a id=13123 href="#13123">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/1/2005 12:09:09 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul Erlich wrote:</p><p>&gt;&gt;Tenney weighting is like a null-weighting for primes-based measures &gt;&gt;because it means all intervals are considered equivalent in so far &gt; as<br/>&gt;&gt;that&apos;s possible.  But if you make it the only criterion it gives &gt; far too &gt;&gt;much weight to complex intervals.  I don&apos;t think you even get a &gt; stable &gt;&gt;optimization.  That&apos;s why you have to enforce a prime limit with &gt; these &gt;&gt;weighted schemes.<br/>&gt; &gt; I&apos;m dubious on this part of your claims. Can you set up an unstable &gt; optimization to show this?</p><p>For the least squares weighted prime function of the nearest prime approximation to 73-equal:</p><p>1000 0.000386288890546 1.00000525939<br/>2000 0.000342631907251 0.999999878426<br/>3000 0.000320740521709 1.00000066141<br/>4000 0.00030851462036 0.999999470906<br/>5000 0.000298744238076 1.00000050417<br/>6000 0.000291497890021 0.99999998381<br/>7000 0.000285393298895 0.999999137131<br/>8000 0.000279937136959 1.00000022932<br/>9000 0.000276009346747 0.999999628759</p><p>The left hand column is the number of primes, the middle column is the weighted RMS and the final column is the stretched octave.  You could say it&apos;s stable about 1.0, but it is at least useless as a way of getting the optimum stretch.  Here, for comparison, is 72-equal:</p><p>1000 0.000373784710919 1.0000057801<br/>2000 0.000333632022278 1.00001005312<br/>3000 0.000317872256548 1.00000028725<br/>4000 0.000306200832323 0.999998925005<br/>5000 0.000297006549111 0.999996450292<br/>6000 0.00029044672927 0.999997819137<br/>7000 0.000285145297883 0.999998100274<br/>8000 0.000280218707906 0.999997806044<br/>9000 0.000276665844039 0.999998361876</p><p>When more than 8000 primes, 73-equal is closer to JI.  In general, the error has more to do with the number of primes and steps per octave than any properties of the temperament in sensible limits.</p><p>&gt;&gt;It guarantees that intervals you don&apos;t consider are &gt;&gt;more consonant than most of those you do, by the very same &gt; complexity &gt;&gt;measure you&apos;re looking for.<br/>&gt; &gt; I&apos;m not sure exactly what you mean by this.</p><p>15:8 is part of the 5-limit, and so part of a weighted primes optimization.  7:4 is outside the 5 prime-limit, and so isn&apos;t considered at all.  18984375:16777216 is within the prime limit, and so it carries some weight, if not very much.</p><p>&gt; This is the great thing about TOP. You can place the hard limit just &gt; about anywhere you want and it won&apos;t change the results! I admit that &gt; TOP may seem pretty unjustified if you don&apos;t realize this. But I &gt; tried to point it out in my paper . . . And the results are &gt; perfectly &quot;stable&quot;, in every sense.</p><p>If that&apos;s true, it certainly is news.  But how come &quot;dimipent&quot; and &quot;dimisept&quot; are given different periods, generators and damages in your paper?  As dimipent is only dimisept without the prime 7, it certainly looks like the limit does change the result.</p><p>&gt;&gt;So we end up back with Tenney weighting and a prime limit.  It&apos;s a &gt;&gt;pragmatic approach that gives sensible results.  But, as it&apos;s still &gt; a &gt;&gt;compromise,<br/>&gt; &gt; I don&apos;t see it as a compromise. What is being compromised?</p><p>We have a weighting and a limit that isn&apos;t based on complexity.  That compromises simplicity against only having the weighting, or sensibleness against considering all primes and getting rubbish out (unless you can demonstrate otherwise).</p><p>&gt;&gt;My big, conceptual problem with TOP is that the minimax isn&apos;t &gt;&gt;appropriate for a weighted scheme.<br/>&gt; &gt; Sure it is -- just as appropriate as with equal-weighting. I don&apos;t &gt; see why one would say otherwise.</p><p>This is the thing I haven&apos;t mentioned before, because it would mean taking the time to set out all the arguments.  But I&apos;m sure it makes sense.</p><p>&gt;&gt;The point of the minimax is that you &gt;&gt;put a limit on how bad the tuning can get.<br/>&gt; &gt; Yes, and &quot;how bad&quot; is clearly something which can&apos;t be in units of &gt; cents for all the different intervals you&apos;re looking at, IMO.</p><p>I think it can, and at least this is the best guess if we don&apos;t know what the true function should be.</p><p>&gt;&gt;The point of weighting is &gt;&gt;that you favour the most important intervals, and don&apos;t worry so &gt; much &gt;&gt;about the less important ones.  If simple intervals carry most of &gt; the &gt;&gt;harmony you care about, their tuning is more important than the &gt; more &gt;&gt;complex ones.<br/>&gt; &gt; Yes, and the weighting is exactly what takes this into account.</p><p>Uh, yes.</p><p>&gt;&gt;And good intervals in a chord can make up for bad ones.<br/>&gt; &gt; Possibly, though minimax is good enough for George Secor. Note that &gt; for a triad, max error and sum-absolute-errors amount to the same &gt; thing.</p><p>I don&apos;t think major seventh chords are as dissonant as other 15-limit chords, so there must be more to it than the most dissonant interval (or odd limit/Tenney complexity).</p><p>&gt;&gt;These considerations lead to some kind of mean error as the thing &gt; to be &gt;&gt;optimized.<br/>&gt; &gt; OK, I&apos;m willing to delve further into your L2 variant of TOP . . . we &gt; really need to think more about what it means, though.</p><p>It&apos;s the average error of the primes.  That&apos;s not a difficult concept.</p><p>&gt;&gt;I really don&apos;t think that the weighted maximum error is &gt;&gt;anything we can hear.  Ideally (if we had a theory we could trust) &gt; we&apos;d &gt;&gt;do a weighted-mean optimization provided an unweighted-minimax &gt; hurdle is &gt;&gt;passed.<br/>&gt; &gt; Why unweighted? You haven&apos;t given any arguments on that.</p><p>It makes sense that this would be the case.  The worst interval is the one that&apos;s most out of tune.  If anything, I&apos;m with Partch and would expect the more complex intervals to have a smaller error.  I think the various dissonance graphs back this up.  If you draw a line for a particular worst dissonance level, the complex intervals have a narrower range than the simpler ones.</p><p>The absolute error also tells you how far a performer would have to move the pitch to get JI on a suitably flexible instrument.</p><p>&gt;&gt;Quite small intervals sound most dissonant, and very large &gt; intervals are &gt;&gt;inaudible. Yet they have the same Tenney weight, and so are &gt; considered &gt;&gt;equally in the optimization if that&apos;s your only criterion.<br/>&gt; &gt; That&apos;s not a fair statement. The optimization can be &quot;considered&quot; in &gt; many different ways while still yielding the same results.</p><p>Only if you use an algorithm that you know gives the same results, in which case you&apos;re implicitly considering all the other intervals by using that algorithm.</p><p>&gt;&gt;I don&apos;t know &gt;&gt;the solution to this, because there isn&apos;t enough theory to rest on, &gt; but &gt;&gt;for the best solution I&apos;d want some kind of size-weighted minimax &gt; to &gt;&gt;make sure the smallest important intervals don&apos;t get out of &gt; control. &gt; &gt; Again, you appear to be missing another important feature of TOP, &gt; which is that you&apos;re free to consider only the intervals within some &gt; range of interest for your optimization, and you still get exactly &gt; the same result!</p><p>No, if you&apos;re using TOP you&apos;re considering all intervals because you chose TOP which makes them the same.</p><p>&gt;&gt;Use the octave stretching to optimize 9:8 and 8:7, and leave 3:2 &gt; and 7:4 &gt;&gt;to look after themselves.<br/>&gt; &gt; I don&apos;t understand that. Aren&apos;t they (the latter intervals) more &gt; damaged by mistuning?</p><p>Yes, that&apos;s what will happen if you give them lower weight.</p><p>&gt; Well, for triads, mean and minimax give the same thing, while RMS &gt; gives something else. So I&apos;m not sure exactly what you mean by &gt; this . . .</p><p>RMS is a kind of mean.  That&apos;s what the &quot;M&quot; stands for.</p><p>&gt;&gt;It assumes errors always add up, whereas sometimes &gt;&gt;they cancel out, so 5:3 and 15:1 are always given the same error.<br/>&gt; &gt; You lost me.</p><p>An RMS of primes means the sign of the error is ignored.  The error of 5:3 is the difference between the errors of 5 and 3, and the error of 15:1 is the sum of the errors of 5 and 3.  A prime-based measure ignoring the sign will always assign the sum of the absolute errors of 5 and 3 to both ratios.</p><p>&gt;&gt;That&apos;s why you have to optimize the octave.  If the errors would &gt; tend to &gt;&gt;cancel out in an odd-limit, the RMS of primes is unrepresentative.  &gt; But &gt;&gt;the octave stretching will compensate for this.  So, paradoxically, &gt; the &gt;&gt;octave-stretched optimum may be most valid after you unstretch the &gt; octaves.<br/>&gt; &gt; Can you be a little more elaborate?</p><p>Okay, prime-weighted schemes can be parameterized to use weighted-intervals which I call w(p).  w(p) is the size of the tempered interval representing the prime p divided by the size of p:1.  So, for an equal temperament with pure octaves this is</p><p>w(p) = n(p)/d/log2(p)</p><p>where n(p) is the number of steps approximating p, and d is the number of steps to an octave.  You can think of TOP in terms of w(p) to an extent, in that it can be defined in terms of w(p) although you won&apos;t get the right weighting for complex intervals if you don&apos;t know anything else.  The simplest error measure is some average comparing these intervals to 1:</p><p>e(p) = w(p) - 1</p><p>Average error = sqrt(&lt;e2&gt;)</p><p>That is, the RMS.  &lt;..&gt; means the mean and suffix 2 is for squared.  The problem is that 19-equal is underestimated in the 5-limit if we keep pure octaves.  The errors in 3 and 5 cancel out in 6:5, but the simple RMS ignores this.  This is where we point out to newbies that they&apos;re getting it wrong, and previously we suggested odd limits.</p><p>An alternative is to take the standard deviation of the errors instead.   Then, 19-equal looks good because the two 5-prime errors are about the same.  The problem is that 19-equal is now overestimated, because it&apos;s errors cancel out but they&apos;re still finite.  Getting errors to be the same isn&apos;t enough.  We really want errors that are close to each other, and close to zero.  So, well, add zero to the standard deviation.</p><p>With pure octaves, w(2) is always 1.0 and so e(2) is always zero. Hence, the standard deviation of the errors already contains a zero if we do it octave explicitly, but with pure octaves.  So, a good bet for an octave-equivalent measure is</p><p>std(e) = sqrt(&lt;e2&gt; - &lt;e&gt;2)</p><p>where the octaves are still included in the calculation.</p><p>It happens that the least squares, prime-weighted error is</p><p>sqrt(1-&lt;w&gt;2/&lt;w2&gt;)</p><p>which can be rewritten</p><p>sqrt[(&lt;w2&gt; - &lt;w&gt;2)/&lt;w2&gt;]</p><p>or</p><p>std(w)/rms(w)</p><p>This is an interesting formula, because it&apos;s invariant with respect to the octave stretch.  If you multiply each w by a constant, the effect on the standard deviation cancels out that on the RMS.  It also happens that, because w(p) is only e(p) plus a constant, the two sets have the same standard deviation.  That means our optimal error is</p><p>std(e)/rms(w)</p><p>If the temperament is at all sensible, each w(p) will be close to 1, and so this total error is close to std(e).  Therefore it agrees with the octave-equivalent measure of standard deviation of weighted prime errors with a zero added in.  It takes account of the sizes and signs of the prime errors, and doesn&apos;t assume a particular value for the octave.</p><p>&gt;&gt;The cleverest part of TOP is that it can still be applied with pure &gt;&gt;octaves and the Kees metric.<br/>&gt; &gt; I don&apos;t know what you mean. Though TOP, when stretched or compressed &gt; to have pure octaves, usually coincides with minimax Kees in the 5-<br/>&gt; limit, it doesn&apos;t in higher limits, according to Gene.</p><p>That&apos;s not what I mean.  The RMS of primes measure is easy to find, and easy to show invalid when you take the octaves out.  The bit about the standard deviation above is something I hadn&apos;t noticed until after you showed us TOP.  The clever bit about TOP is that it&apos;s a weighted measure that has a natural octave-equivalent definition: your minimax Kees. That it&apos;s at least close to stretched TOP is nice, but a side issue.</p><p>&gt; If we can put your proposal on some good foundations . . . that has &gt; yet to be seen (by me at least). I was disappointed when Gene &gt; informed us that beyond the 5-limit, minimax Kees doesn&apos;t agree (even &gt; modulo stretching) with TOP; I&apos;m dubious that replacing minimax with &gt; sum-of-squares (assuming we can define the latter appropriately in &gt; the Kees case) will bring back the agreement . . .</p><p>I thought I proved the relationship for any case of tempering out a single comma.  That would include the 7-limit planar temperaments, and so on.</p><p>&gt;&gt;  I can see practical advantages in the weighted prime least &gt; &gt; squares, &gt; &gt;&gt;which I&apos;d overlooked before.  But TOP is too difficult to &gt; &gt; calculate, and &gt; &gt;&gt;rests on too dubious theory, that it doesn&apos;t fill any useful niche.<br/>&gt; &gt; &gt; I find it odd that you&apos;d say this but of course you&apos;re entitled . . . &gt; In many cases of interest, TOP is extremely easy to calculate, as you &gt; know . . .</p><p>It&apos;s only easy when you&apos;re tempering out a single comma.  For linear temperaments, that only covers the 5-limit.  I can do searches up to the 19 limit, and I don&apos;t know how to calculate TOP then.  It&apos;s only in the higher limits that the search gets difficult anyway.</p><p>                   Graham</p></div><h3><a id=13124 href="#13124">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/1/2005 12:12:27 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:<br/>&gt;&gt;In many cases of interest, TOP is extremely easy to calculate, as you &gt;&gt;know . . .</p><p>Gene:<br/>&gt; If you&apos;ve got a linear programming routine available (and they are<br/>&gt; easily found) it&apos;s quite simple to compute.</p><p>It can&apos;t be that simple if you need such a routine!  Can you find one for Python?  Or any free language?  How efficient is it for optimizing a billion 19-limit linear temperaments?</p><p>                Graham</p></div><h3><a id=13126 href="#13126">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/1/2005 12:27:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; The left hand column is the number of primes, the middle column is the<br/>&gt; weighted RMS and the final column is the stretched octave.  You could<br/>&gt; say it&apos;s stable about 1.0, but it is at least useless as a way of<br/>&gt; getting the optimum stretch.</p><p>I don&apos;t know what you are using for a weighting, but it probably isn&apos;t<br/>enough; you might try the p^(-1/2) as a weight for the prime p (apeing<br/>the Zeta function on the critical line.)</p></div><h3><a id=13127 href="#13127">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/1/2005 12:29:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; It can&apos;t be that simple if you need such a routine!</p><p>Why not? Routines are as common as dirt.</p><p>  Can you find one<br/>&gt; for Python?</p><p>Probably.</p><p>&gt; How efficient is it for optimizing a<br/>&gt; billion 19-limit linear temperaments?</p><p>Why in the world do you want to do that? But simplex algorithms run<br/>pretty fast for low dimensional problems like that in particular.</p></div><h3><a id=13129 href="#13129">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/1/2005 12:49:55 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; If we can put your proposal on some good foundations . . . that has<br/>&gt; &gt; yet to be seen (by me at least). I was disappointed when Gene<br/>&gt; &gt; informed us that beyond the 5-limit, minimax Kees doesn&apos;t agree<br/>(even<br/>&gt; &gt; modulo stretching) with TOP...<br/>&gt;<br/>&gt; In practice in the 7 and 11 limits it seems it nearly always does.</p><p>By &quot;in practice&quot; do you mean &quot;approximately&quot;? What&apos;s the minimax Kees<br/>pajara tuning?</p></div><h3><a id=13135 href="#13135">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/1/2005 1:48:46 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; &gt; In practice in the 7 and 11 limits it seems it nearly always does.<br/>&gt;<br/>&gt; By &quot;in practice&quot; do you mean &quot;approximately&quot;?</p><p>No, I mean &quot;most of the time&quot;, and it seems that it happens more often<br/>than one might suppose for the better 7-limit temperaments.</p><p>What&apos;s the minimax Kees<br/>&gt; pajara tuning?</p><p>It&apos;s stretched TOP. This is the Kees minimax tuning for just about any<br/>rank two 7-limit temperament worthy of mention.</p></div><h3><a id=13139 href="#13139">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/1/2005 2:30:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Paul Erlich wrote:<br/>&gt;<br/>&gt; &gt;&gt;Tenney weighting is like a null-weighting for primes-based<br/>measures<br/>&gt; &gt;&gt;because it means all intervals are considered equivalent in so<br/>far<br/>&gt; &gt; as<br/>&gt; &gt;&gt;that&apos;s possible.  But if you make it the only criterion it gives<br/>&gt; &gt; far too<br/>&gt; &gt;&gt;much weight to complex intervals.  I don&apos;t think you even get a<br/>&gt; &gt; stable<br/>&gt; &gt;&gt;optimization.  That&apos;s why you have to enforce a prime limit with<br/>&gt; &gt; these<br/>&gt; &gt;&gt;weighted schemes.<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m dubious on this part of your claims. Can you set up an<br/>unstable<br/>&gt; &gt; optimization to show this?<br/>&gt;<br/>&gt; For the least squares weighted prime function of the nearest prime<br/>&gt; approximation to 73-equal:<br/>&gt;<br/>&gt; 1000 0.000386288890546 1.00000525939<br/>&gt; 2000 0.000342631907251 0.999999878426<br/>&gt; 3000 0.000320740521709 1.00000066141<br/>&gt; 4000 0.00030851462036 0.999999470906<br/>&gt; 5000 0.000298744238076 1.00000050417<br/>&gt; 6000 0.000291497890021 0.99999998381<br/>&gt; 7000 0.000285393298895 0.999999137131<br/>&gt; 8000 0.000279937136959 1.00000022932<br/>&gt; 9000 0.000276009346747 0.999999628759<br/>&gt;<br/>&gt; The left hand column is the number of primes,</p><p>!!! You need to specify your mapping before beginning the<br/>optimization. In this case, you&apos;d need mappings with hundreds or<br/>thousands of elements.</p><p>&gt; the middle column is the<br/>&gt; weighted RMS and the final column is the stretched octave.  You<br/>could<br/>&gt; say it&apos;s stable about 1.0, but it is at least useless as a way of<br/>&gt; getting the optimum stretch.  Here, for comparison, is 72-equal:<br/>&gt;<br/>&gt; 1000 0.000373784710919 1.0000057801<br/>&gt; 2000 0.000333632022278 1.00001005312<br/>&gt; 3000 0.000317872256548 1.00000028725<br/>&gt; 4000 0.000306200832323 0.999998925005<br/>&gt; 5000 0.000297006549111 0.999996450292<br/>&gt; 6000 0.00029044672927 0.999997819137<br/>&gt; 7000 0.000285145297883 0.999998100274<br/>&gt; 8000 0.000280218707906 0.999997806044<br/>&gt; 9000 0.000276665844039 0.999998361876<br/>&gt;<br/>&gt; When more than 8000 primes, 73-equal is closer to JI.  In general,<br/>the<br/>&gt; error has more to do with the number of primes and steps per octave<br/>than<br/>&gt; any properties of the temperament in sensible limits.</p><p>I have no idea what you&apos;re doing with this many primes, nor what this<br/>is supposed to show, I&apos;m afraid.</p><p>&gt; &gt;&gt;It guarantees that intervals you don&apos;t consider are<br/>&gt; &gt;&gt;more consonant than most of those you do, by the very same<br/>&gt; &gt; complexity<br/>&gt; &gt;&gt;measure you&apos;re looking for.<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m not sure exactly what you mean by this.<br/>&gt;<br/>&gt; 15:8 is part of the 5-limit, and so part of a weighted primes<br/>&gt; optimization.</p><p>I don&apos;t need to explicity include it. I could claim that its tuning<br/>is purely a result of the tuning of simpler intervals.</p><p>&gt; 7:4 is outside the 5 prime-limit, and so isn&apos;t considered<br/>&gt; at all.</p><p>If you don&apos;t care to have it in your scales, that strikes me as the<br/>right thing to do.</p><p>&gt; 18984375:16777216 is within the prime limit, and so it carries<br/>&gt; some weight, if not very much.</p><p>I don&apos;t need to explicity include it. I could claim that its tuning<br/>is purely a result of the tuning of simpler intervals.</p><p>&gt; &gt; This is the great thing about TOP. You can place the hard limit<br/>just<br/>&gt; &gt; about anywhere you want and it won&apos;t change the results! I admit<br/>that<br/>&gt; &gt; TOP may seem pretty unjustified if you don&apos;t realize this. But I<br/>&gt; &gt; tried to point it out in my paper . . . And the results are<br/>&gt; &gt; perfectly &quot;stable&quot;, in every sense.<br/>&gt;<br/>&gt; If that&apos;s true, it certainly is news.</p><p>It says so in the paper. Perhaps I need to say it more times in there.</p><p>&gt; But how come &quot;dimipent&quot; and<br/>&gt; &quot;dimisept&quot; are given different periods, generators and damages in<br/>your<br/>&gt; paper?  As dimipent is only dimisept without the prime 7, it<br/>certainly<br/>&gt; looks like the limit does change the result.</p><p>I mean the hard limit on the complexity of the ratios included in the<br/>optimization, *given* the prime limit, can be placed just about<br/>anywhere you want, and the optimal result won&apos;t change.</p><p>&gt; &gt;&gt;So we end up back with Tenney weighting and a prime limit.  It&apos;s<br/>a<br/>&gt; &gt;&gt;pragmatic approach that gives sensible results.  But, as it&apos;s<br/>still<br/>&gt; &gt; a<br/>&gt; &gt;&gt;compromise,<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t see it as a compromise. What is being compromised?<br/>&gt;<br/>&gt; We have a weighting and a limit that isn&apos;t based on complexity.<br/>That<br/>&gt; compromises simplicity against only having the weighting, or<br/>&gt; sensibleness against considering all primes and getting rubbish out<br/>&gt; (unless you can demonstrate otherwise).</p><p>I don&apos;t understand why this argument applies to TOP and not to all<br/>the other methods, which also use a limit &quot;that isn&apos;t based on<br/>complexity&quot;. But with TOP, you can consider, say, all the intervals<br/>with an integer limit of 10, and Tenney-weight their errors. With a<br/>prime limit of 7, there are no &quot;holes&quot; -- there are no intervals<br/>missing that &quot;compromise&quot; the weighting. High primes don&apos;t need to<br/>enter the &quot;compromise&quot; scenario since you never have to explicitly<br/>weight the complex intervals anyway.</p><p>&gt; &gt;&gt;My big, conceptual problem with TOP is that the minimax isn&apos;t<br/>&gt; &gt;&gt;appropriate for a weighted scheme.<br/>&gt; &gt;<br/>&gt; &gt; Sure it is -- just as appropriate as with equal-weighting. I<br/>don&apos;t<br/>&gt; &gt; see why one would say otherwise.<br/>&gt;<br/>&gt; This is the thing I haven&apos;t mentioned before, because it would mean<br/>&gt; taking the time to set out all the arguments.  But I&apos;m sure it &gt;<br/>makes sense.</p><p>Weighted minimax is used in many fields. I look forward to your<br/>arguments :)</p><p>&gt; &gt;&gt;The point of the minimax is that you<br/>&gt; &gt;&gt;put a limit on how bad the tuning can get.<br/>&gt; &gt;<br/>&gt; &gt; Yes, and &quot;how bad&quot; is clearly something which can&apos;t be in units<br/>of<br/>&gt; &gt; cents for all the different intervals you&apos;re looking at, IMO.<br/>&gt;<br/>&gt; I think it can, and at least this is the best guess if we don&apos;t<br/>know<br/>&gt; what the true function should be.</p><p>I think it&apos;s pretty clear from the various models (such as harmonic<br/>entropy) and testimonies we have that by and large, simpler intervals<br/>are more sensitive to mistuning than slightly more complex ones. Even<br/>George Secor, who has used equal-weighted minimax, ended up agreeing<br/>with this!</p><p>&gt; &gt;&gt;The point of weighting is<br/>&gt; &gt;&gt;that you favour the most important intervals, and don&apos;t worry so<br/>&gt; &gt; much<br/>&gt; &gt;&gt;about the less important ones.  If simple intervals carry most of<br/>&gt; &gt; the<br/>&gt; &gt;&gt;harmony you care about, their tuning is more important than the<br/>&gt; &gt; more<br/>&gt; &gt;&gt;complex ones.<br/>&gt; &gt;<br/>&gt; &gt; Yes, and the weighting is exactly what takes this into account.<br/>&gt;<br/>&gt; Uh, yes.<br/>&gt;<br/>&gt; &gt;&gt;And good intervals in a chord can make up for bad ones.<br/>&gt; &gt;<br/>&gt; &gt; Possibly, though minimax is good enough for George Secor. Note<br/>that<br/>&gt; &gt; for a triad, max error and sum-absolute-errors amount to the same<br/>&gt; &gt; thing.<br/>&gt;<br/>&gt; I don&apos;t think major seventh chords are as dissonant as other 15-<br/>limit<br/>&gt; chords, so there must be more to it than the most dissonant<br/>interval (or<br/>&gt; odd limit/Tenney complexity).</p><p>Of course! But this is a completely different question. First of all,<br/>15:8 can be the most dissonant interval while being the most damaged,<br/>or while being the least damaged, interval in the chord, depending on<br/>the tuning. This is closer to what we&apos;re actually discussing here. In<br/>TOP, the damage on the most dissonant interval matters least, far<br/>from that interval mattering most as you imply above. Secondly, we&apos;re<br/>not talking about a single chord, we&apos;re talking about an entire<br/>tuning system, and minimax can mean a lot of different things in that<br/>context. Your example seems like a perfect argument for TOP, in fact,<br/>in that major seventh chords participate in a lot of the consonances<br/>in the lattice, and you don&apos;t want any of these consonances to be too<br/>far off, while the 15:8 itself can be further off while doing less<br/>damage to the chord as a whole.</p><p>&gt; &gt;&gt;These considerations lead to some kind of mean error as the thing<br/>&gt; &gt; to be<br/>&gt; &gt;&gt;optimized.<br/>&gt; &gt;<br/>&gt; &gt; OK, I&apos;m willing to delve further into your L2 variant of<br/>TOP . . . we<br/>&gt; &gt; really need to think more about what it means, though.<br/>&gt;<br/>&gt; It&apos;s the average</p><p>You mean RMS?</p><p>&gt;error of the primes.  That&apos;s not a difficult &gt;concept.</p><p>The idea is to work through the implications for *all* the intervals,<br/>as has been done with TOP (see, for example, footnote xxvi in my<br/>paper). Otherwise, there seems little justification for going along<br/>with something that just looks at the primes and nothing else.</p><p>&gt; &gt;&gt;I really don&apos;t think that the weighted maximum error is<br/>&gt; &gt;&gt;anything we can hear.  Ideally (if we had a theory we could<br/>trust)<br/>&gt; &gt; we&apos;d<br/>&gt; &gt;&gt;do a weighted-mean optimization provided an unweighted-minimax<br/>&gt; &gt; hurdle is<br/>&gt; &gt;&gt;passed.<br/>&gt; &gt;<br/>&gt; &gt; Why unweighted? You haven&apos;t given any arguments on that.<br/>&gt;<br/>&gt; It makes sense that this would be the case.  The worst interval is<br/>the<br/>&gt; one that&apos;s most out of tune.  If anything, I&apos;m with Partch and<br/>would<br/>&gt; expect the more complex intervals to have a smaller error.</p><p>I agree if you&apos;re saying that we should first check that the ratios<br/>or chords we intend to use as basic consonances aren&apos;t closer to<br/>other, equally simple ratios or chords than the ones they&apos;re supposed<br/>to approximate. But I&apos;d rather leave this to the user, who will<br/>simply chuck out some of the TOP systems as a result, and keep the<br/>rest of them -- specifying &quot;the ratios or chords we intend to use as<br/>basic consonances&quot; is more than I need or want for the purpose of<br/>setting out some TOP tunings. The hurdle can always be placed at the<br/>end without affecting the results.</p><p>&gt; I think the<br/>&gt; various dissonance graphs back this up.  If you draw a line for a<br/>&gt; particular worst dissonance level, the complex intervals have a<br/>narrower<br/>&gt; range than the simpler ones.</p><p>Right, but a given mistuning has a considerably greater impact on the<br/>consonance of the simpler interval than it does on the consonance of<br/>the complex ones. So if our target harmony is all the intervals in a<br/>big harmonic-series chord, say, the closeness of the sound to JI is<br/>best judged by weighting the errors on the simpler intervals *more*.</p><p>&gt; The absolute error also tells you how far a performer would have to<br/>move<br/>&gt; the pitch to get JI on a suitably flexible instrument.</p><p>True, and that&apos;s one reason I&apos;ve liked it in the past. It&apos;s a good<br/>thing to consider when your ultimate goal is adaptive JI.</p><p>&gt; &gt;&gt;Quite small intervals sound most dissonant, and very large<br/>&gt; &gt; intervals are<br/>&gt; &gt;&gt;inaudible. Yet they have the same Tenney weight, and so are<br/>&gt; &gt; considered<br/>&gt; &gt;&gt;equally in the optimization if that&apos;s your only criterion.<br/>&gt; &gt;<br/>&gt; &gt; That&apos;s not a fair statement. The optimization can be &quot;considered&quot;<br/>in<br/>&gt; &gt; many different ways while still yielding the same results.<br/>&gt;<br/>&gt; Only if you use an algorithm that you know gives the same results,<br/>in<br/>&gt; which case you&apos;re implicitly considering all the other intervals by<br/>&gt; using that algorithm.</p><p>You could turn the argument around and say that no matter how you<br/>look at it, you&apos;re implicitly considering the primes and only the<br/>primes. Either way, the argument is invalid. The mathematical<br/>identity of these various results doesn&apos;t mean that if one accepts<br/>one set of desiderata that lead to it, you&apos;re automatically accepting<br/>some other set of desiderata that also lead to it.</p><p>&gt; &gt;&gt;I don&apos;t know<br/>&gt; &gt;&gt;the solution to this, because there isn&apos;t enough theory to rest<br/>on,<br/>&gt; &gt; but<br/>&gt; &gt;&gt;for the best solution I&apos;d want some kind of size-weighted minimax<br/>&gt; &gt; to<br/>&gt; &gt;&gt;make sure the smallest important intervals don&apos;t get out of<br/>&gt; &gt; control.<br/>&gt; &gt;<br/>&gt; &gt; Again, you appear to be missing another important feature of TOP,<br/>&gt; &gt; which is that you&apos;re free to consider only the intervals within<br/>some<br/>&gt; &gt; range of interest for your optimization, and you still get<br/>exactly<br/>&gt; &gt; the same result!<br/>&gt;<br/>&gt; No, if you&apos;re using TOP you&apos;re considering all intervals because<br/>you<br/>&gt; chose TOP which makes them the same.</p><p>Huh? Makes what the same? Is this the same type of argument as above?<br/>You can&apos;t very well claim that I&apos;m necessarily considering the wider<br/>intervals in the optimization when ignoring them gives the results I<br/>present.</p><p>&gt; &gt;&gt;Use the octave stretching to optimize 9:8 and 8:7, and leave 3:2<br/>&gt; &gt; and 7:4<br/>&gt; &gt;&gt;to look after themselves.<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t understand that. Aren&apos;t they (the latter intervals) more<br/>&gt; &gt; damaged by mistuning?<br/>&gt;<br/>&gt; Yes, that&apos;s what will happen if you give them lower weight.</p><p>Well, it&apos;s not something my ears seem to like.</p><p>&gt; &gt; Well, for triads, mean and minimax give the same thing, while RMS<br/>&gt; &gt; gives something else. So I&apos;m not sure exactly what you mean by<br/>&gt; &gt; this . . .<br/>&gt;<br/>&gt; RMS is a kind of mean.  That&apos;s what the &quot;M&quot; stands for.</p><p>Yes I know but you didn&apos;t clarify your statement, you just snipped it.</p><p>&gt; &gt;&gt;It assumes errors always add up, whereas sometimes<br/>&gt; &gt;&gt;they cancel out, so 5:3 and 15:1 are always given the same error.<br/>&gt; &gt;<br/>&gt; &gt; You lost me.<br/>&gt;<br/>&gt; An RMS of primes means the sign of the error is ignored.  The error<br/>of<br/>&gt; 5:3 is the difference between the errors of 5 and 3, and the error<br/>of<br/>&gt; 15:1 is the sum of the errors of 5 and 3.  A prime-based measure<br/>&gt; ignoring the sign will always assign the sum of the absolute errors<br/>of 5<br/>&gt; and 3 to both ratios.</p><p>A prime-based measure always uses sum of absolute errors?</p><p>&gt; &gt;&gt;That&apos;s why you have to optimize the octave.  If the errors would<br/>&gt; &gt; tend to<br/>&gt; &gt;&gt;cancel out in an odd-limit, the RMS of primes is<br/>unrepresentative.<br/>&gt; &gt; But<br/>&gt; &gt;&gt;the octave stretching will compensate for this.  So,<br/>paradoxically,<br/>&gt; &gt; the<br/>&gt; &gt;&gt;octave-stretched optimum may be most valid after you unstretch<br/>the<br/>&gt; &gt; octaves.<br/>&gt; &gt;<br/>&gt; &gt; Can you be a little more elaborate?<br/>&gt;<br/>&gt; Okay, prime-weighted schemes can be parameterized to use<br/>&gt; weighted-intervals which I call w(p).  w(p) is the size of the<br/>tempered<br/>&gt; interval representing the prime p divided by the size of p:1.  So,<br/>for<br/>&gt; an equal temperament with pure octaves this is<br/>&gt;<br/>&gt; w(p) = n(p)/d/log2(p)<br/>&gt;<br/>&gt; where n(p) is the number of steps approximating p, and d is the<br/>number<br/>&gt; of steps to an octave.  You can think of TOP in terms of w(p) to an<br/>&gt; extent, in that it can be defined in terms of w(p) although you<br/>won&apos;t<br/>&gt; get the right weighting for complex intervals if you don&apos;t know<br/>anything<br/>&gt; else.  The simplest error measure is some average comparing these<br/>&gt; intervals to 1:<br/>&gt;<br/>&gt; e(p) = w(p) - 1<br/>&gt;<br/>&gt; Average error = sqrt(&lt;e2&gt;)<br/>&gt;<br/>&gt; That is, the RMS.  &lt;..&gt; means the mean and suffix 2 is for<br/>squared.  The<br/>&gt; problem is that 19-equal is underestimated in the 5-limit if we<br/>keep<br/>&gt; pure octaves.  The errors in 3 and 5 cancel out in 6:5, but the<br/>simple<br/>&gt; RMS ignores this.  This is where we point out to newbies that<br/>they&apos;re<br/>&gt; getting it wrong, and previously we suggested odd limits.<br/>&gt;<br/>&gt; An alternative is to take the standard deviation of the errors<br/>instead.<br/>&gt;   Then, 19-equal looks good because the two 5-prime errors are<br/>about the<br/>&gt; same.  The problem is that 19-equal is now overestimated, because<br/>it&apos;s<br/>&gt; errors cancel out but they&apos;re still finite.  Getting errors to be<br/>the<br/>&gt; same isn&apos;t enough.  We really want errors that are close to each<br/>other,<br/>&gt; and close to zero.  So, well, add zero to the standard deviation.<br/>&gt;<br/>&gt; With pure octaves, w(2) is always 1.0 and so e(2) is always zero.<br/>&gt; Hence, the standard deviation of the errors already contains a zero<br/>if<br/>&gt; we do it octave explicitly, but with pure octaves.  So, a good bet<br/>for<br/>&gt; an octave-equivalent measure is<br/>&gt;<br/>&gt; std(e) = sqrt(&lt;e2&gt; - &lt;e&gt;2)<br/>&gt;<br/>&gt; where the octaves are still included in the calculation.<br/>&gt;<br/>&gt;<br/>&gt; It happens that the least squares, prime-weighted error is<br/>&gt;<br/>&gt; sqrt(1-&lt;w&gt;2/&lt;w2&gt;)<br/>&gt;<br/>&gt; which can be rewritten<br/>&gt;<br/>&gt; sqrt[(&lt;w2&gt; - &lt;w&gt;2)/&lt;w2&gt;]<br/>&gt;<br/>&gt; or<br/>&gt;<br/>&gt; std(w)/rms(w)<br/>&gt;<br/>&gt; This is an interesting formula, because it&apos;s invariant with respect<br/>to<br/>&gt; the octave stretch.  If you multiply each w by a constant, the<br/>effect on<br/>&gt; the standard deviation cancels out that on the RMS.  It also<br/>happens<br/>&gt; that, because w(p) is only e(p) plus a constant, the two sets have<br/>the<br/>&gt; same standard deviation.  That means our optimal error is<br/>&gt;<br/>&gt; std(e)/rms(w)<br/>&gt;<br/>&gt; If the temperament is at all sensible, each w(p) will be close to<br/>1, and<br/>&gt; so this total error is close to std(e).  Therefore it agrees with<br/>the<br/>&gt; octave-equivalent measure of standard deviation of weighted prime<br/>errors<br/>&gt; with a zero added in.  It takes account of the sizes and signs of<br/>the<br/>&gt; prime errors, and doesn&apos;t assume a particular value for the octave.</p><p>I&apos;ll have to reread all this later . . . running out of time today.</p><p>&gt; &gt;&gt;The cleverest part of TOP is that it can still be applied with<br/>pure<br/>&gt; &gt;&gt;octaves and the Kees metric.<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t know what you mean. Though TOP, when stretched or<br/>compressed<br/>&gt; &gt; to have pure octaves, usually coincides with minimax Kees in the<br/>5-<br/>&gt; &gt; limit, it doesn&apos;t in higher limits, according to Gene.<br/>&gt;<br/>&gt; That&apos;s not what I mean.  The RMS of primes measure is easy to find,<br/>and<br/>&gt; easy to show invalid when you take the octaves out.  The bit about<br/>the<br/>&gt; standard deviation above is something I hadn&apos;t noticed until after<br/>you<br/>&gt; showed us TOP.  The clever bit about TOP is that it&apos;s a weighted<br/>measure<br/>&gt; that has a natural octave-equivalent definition: your minimax Kees.<br/>&gt; That it&apos;s at least close to stretched TOP is nice, but a side issue.</p><p>I&apos;ll have to consider this later.</p><p>&gt; &gt; If we can put your proposal on some good foundations . . . that<br/>has<br/>&gt; &gt; yet to be seen (by me at least). I was disappointed when Gene<br/>&gt; &gt; informed us that beyond the 5-limit, minimax Kees doesn&apos;t agree<br/>(even<br/>&gt; &gt; modulo stretching) with TOP; I&apos;m dubious that replacing minimax<br/>with<br/>&gt; &gt; sum-of-squares (assuming we can define the latter appropriately<br/>in<br/>&gt; &gt; the Kees case) will bring back the agreement . . .<br/>&gt;<br/>&gt; I thought I proved the relationship for any case of tempering out a<br/>&gt; single comma.  That would include the 7-limit planar temperaments,<br/>and<br/>&gt; so on.</p><p>Interesting. I wonder if Gene would reply. Single-comma TOP is the<br/>only kind that&apos;s motivated with the original TOP construction; I<br/>wonder if some other criterion could somehow generalize this<br/>construction to more commas.</p></div><h3><a id=13140 href="#13140">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/1/2005 2:36:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; &gt; In practice in the 7 and 11 limits it seems it nearly always does.<br/>&gt; &gt;<br/>&gt; &gt; By &quot;in practice&quot; do you mean &quot;approximately&quot;?<br/>&gt;<br/>&gt; No, I mean &quot;most of the time&quot;,</p><p>So most of the time it nearly always does??</p><p>&gt; and it seems that it happens more often<br/>&gt; than one might suppose for the better 7-limit temperaments.</p><p>I supposed it happened every time the TOP tuning didn&apos;t have pure<br/>octaves, but you said otherwise.</p><p>&gt; &gt; What&apos;s the minimax Kees<br/>&gt; &gt; pajara tuning?<br/>&gt;<br/>&gt; It&apos;s stretched TOP. This is the Kees minimax tuning for just about any<br/>&gt; rank two 7-limit temperament worthy of mention.</p><p>Wow. That&apos;s certainly not the impression you gave me before, with all<br/>the talk of corners and whatnot. But you did jump to 11-limit for the<br/>one example you gave me where they&apos;re different. Any differences for<br/>the systems in my paper? Based on TOP, I told Igliashon that 13-equal<br/>is better in the 7-limit using the Orwell approximation than any other<br/>approximation of the 7-limit in 13-equal. Is this still true based on<br/>Kees?</p></div><h3><a id=13147 href="#13147">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/1/2005 4:51:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; Any differences for<br/>&gt; the systems in my paper?</p><p>I&apos;ll check, but if you have handy a table of wedgies you could post<br/>here it would be nice.</p><p>Based on TOP, I told Igliashon that 13-equal<br/>&gt; is better in the 7-limit using the Orwell approximation than any other<br/>&gt; approximation of the 7-limit in 13-equal. Is this still true based on<br/>&gt; Kees?</p><p>I don&apos;t even know what you mean.</p></div><h3><a id=13156 href="#13156">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/2/2005 7:18:10 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul Erlich wrote:</p><p>&gt;&gt;For the least squares weighted prime function of the nearest prime &gt;&gt;approximation to 73-equal:<br/>&gt;&gt;<br/>&gt;&gt;1000 0.000386288890546 1.00000525939<br/>&gt;&gt;2000 0.000342631907251 0.999999878426<br/>&gt;&gt;3000 0.000320740521709 1.00000066141<br/>&gt;&gt;4000 0.00030851462036 0.999999470906<br/>&gt;&gt;5000 0.000298744238076 1.00000050417<br/>&gt;&gt;6000 0.000291497890021 0.99999998381<br/>&gt;&gt;7000 0.000285393298895 0.999999137131<br/>&gt;&gt;8000 0.000279937136959 1.00000022932<br/>&gt;&gt;9000 0.000276009346747 0.999999628759<br/>&gt;&gt;<br/>&gt;&gt;The left hand column is the number of primes,<br/>&gt; &gt; !!! You need to specify your mapping before beginning the &gt; optimization. In this case, you&apos;d need mappings with hundreds or &gt; thousands of elements. I did specify it: &quot;nearest prime approximation&quot;.  I could try listing all the elements, but I hardly think it would be an appropriate use of bandwidth.</p><p>&gt; I have no idea what you&apos;re doing with this many primes, nor what this &gt; is supposed to show, I&apos;m afraid.</p><p>It shows that the weighted error depends too much on ridiculously complex intervals.</p><p>&gt;&gt;15:8 is part of the 5-limit, and so part of a weighted primes &gt;&gt;optimization.<br/>&gt; &gt; I don&apos;t need to explicity include it. I could claim that its tuning &gt; is purely a result of the tuning of simpler intervals.</p><p>If you choose a prime limit of 5, you include it unless you explicitly exclude it.</p><p>&gt;&gt;7:4 is outside the 5 prime-limit, and so isn&apos;t considered &gt;&gt;at all.<br/>&gt; &gt; If you don&apos;t care to have it in your scales, that strikes me as the &gt; right thing to do.</p><p>But why does every interval involving a prime of 7 or larger have to come with it?</p><p>&gt; I mean the hard limit on the complexity of the ratios included in the &gt; optimization, *given* the prime limit, can be placed just about &gt; anywhere you want, and the optimal result won&apos;t change. Yes, I made it perfectly clear in my original message that the &quot;hard limit&quot; *is* the prime limit.  So any other limit you may happen to think of is irrelevant in response to it.</p><p>&gt;&gt;We have a weighting and a limit that isn&apos;t based on complexity.  &gt; That &gt;&gt;compromises simplicity against only having the weighting, or &gt;&gt;sensibleness against considering all primes and getting rubbish out &gt;&gt;(unless you can demonstrate otherwise).<br/>&gt; &gt; I don&apos;t understand why this argument applies to TOP and not to all &gt; the other methods, which also use a limit &quot;that isn&apos;t based on &gt; complexity&quot;. But with TOP, you can consider, say, all the intervals &gt; with an integer limit of 10, and Tenney-weight their errors. With a &gt; prime limit of 7, there are no &quot;holes&quot; -- there are no intervals &gt; missing that &quot;compromise&quot; the weighting. High primes don&apos;t need to &gt; enter the &quot;compromise&quot; scenario since you never have to explicitly &gt; weight the complex intervals anyway.</p><p>It applies to any method that uses Tenney weighting with a prime limit.   Where do I say otherwise?</p><p>&gt; I think it&apos;s pretty clear from the various models (such as harmonic &gt; entropy) and testimonies we have that by and large, simpler intervals &gt; are more sensitive to mistuning than slightly more complex ones. Even &gt; George Secor, who has used equal-weighted minimax, ended up agreeing &gt; with this!</p><p>It doesn&apos;t matter how sensitive they are to mistuning unless they&apos;re the most painful intervals -- if you use a minimax.  With a weighted mean, you can give most weight to the most sensitive intervals.</p><p>&gt; Of course! But this is a completely different question. First of all, &gt; 15:8 can be the most dissonant interval while being the most damaged, &gt; or while being the least damaged, interval in the chord, depending on &gt; the tuning. This is closer to what we&apos;re actually discussing here. In &gt; TOP, the damage on the most dissonant interval matters least, far &gt; from that interval mattering most as you imply above. Secondly, we&apos;re &gt; not talking about a single chord, we&apos;re talking about an entire &gt; tuning system, and minimax can mean a lot of different things in that &gt; context. Your example seems like a perfect argument for TOP, in fact, &gt; in that major seventh chords participate in a lot of the consonances &gt; in the lattice, and you don&apos;t want any of these consonances to be too &gt; far off, while the 15:8 itself can be further off while doing less &gt; damage to the chord as a whole.</p><p>To me, it&apos;s the same question that leads to an unbounded, weighted average.  You can&apos;t be sure what supplementary intervals you might end up using, so you try to get them as good as possible but don&apos;t make them as important as the primary consonances.</p><p>I disagree with what you say about TOP.  Every interval you consider (and you get the same result whether you consider them or not, so this is a fairly nebulous consideration) is exactly as important as every other interval.  The only difference is that some intervals are allowed more scope for mistuning than others.  A more complex interval can take more mistuning for the same amount of damage.</p><p>The point of this is &quot;good intervals in a chord make up for the bad ones.&quot;  That was my original statement that these comments stem from. The minimax is inappropriate in that context.  By enforcing the minimax, you assume that the worst interval (however you define it) carries all the badness of the chord.  If the goodness of the chord is a trade-off between the good and bad intervals, then it should also be a trade-off between well and poorly tuned intervals -- hence a mean and not a minimax.</p><p>&gt;&gt;It&apos;s the average<br/>&gt; &gt; You mean RMS?</p><p>You can do any average of absolute errors you like.  The RMS happens to be the simplest to optimize.</p><p>&gt;&gt;error of the primes.  That&apos;s not a difficult &gt;concept.<br/>&gt; &gt; The idea is to work through the implications for *all* the intervals, &gt; as has been done with TOP (see, for example, footnote xxvi in my &gt; paper). Otherwise, there seems little justification for going along &gt; with something that just looks at the primes and nothing else.</p><p>That&apos;s exactly the attitude I tried to argue against, and you keep avoiding by saying &quot;it doesn&apos;t matter which intervals you choose&quot;.</p><p>&gt; I agree if you&apos;re saying that we should first check that the ratios &gt; or chords we intend to use as basic consonances aren&apos;t closer to &gt; other, equally simple ratios or chords than the ones they&apos;re supposed &gt; to approximate. But I&apos;d rather leave this to the user, who will &gt; simply chuck out some of the TOP systems as a result, and keep the &gt; rest of them -- specifying &quot;the ratios or chords we intend to use as &gt; basic consonances&quot; is more than I need or want for the purpose of &gt; setting out some TOP tunings. The hurdle can always be placed at the &gt; end without affecting the results.</p><p>It could be placed either end.  What I&apos;m saying is that the hurdle of worst-error is different in nature to the consideration of average mistuning.</p><p>&gt;&gt;I think the &gt;&gt;various dissonance graphs back this up.  If you draw a line for a &gt;&gt;particular worst dissonance level, the complex intervals have a &gt; narrower &gt;&gt;range than the simpler ones.<br/>&gt; &gt; Right, but a given mistuning has a considerably greater impact on the &gt; consonance of the simpler interval than it does on the consonance of &gt; the complex ones. So if our target harmony is all the intervals in a &gt; big harmonic-series chord, say, the closeness of the sound to JI is &gt; best judged by weighting the errors on the simpler intervals *more*.</p><p>The point of minimax, as I said before, is that all intervals have *equal* weight.  I find it so obvious that your argument there applies to a mean, and not a minimax, that I&apos;m not sure how to make it explicit.</p><p>&gt; You could turn the argument around and say that no matter how you &gt; look at it, you&apos;re implicitly considering the primes and only the &gt; primes. Either way, the argument is invalid. The mathematical &gt; identity of these various results doesn&apos;t mean that if one accepts &gt; one set of desiderata that lead to it, you&apos;re automatically accepting &gt; some other set of desiderata that also lead to it.</p><p>So what other conditions lead to a Tenney weighting?</p><p>&gt;&gt;&gt;Again, you appear to be missing another important feature of TOP, &gt;&gt;&gt;which is that you&apos;re free to consider only the intervals within &gt; some &gt;&gt;&gt;range of interest for your optimization, and you still get &gt; exactly &gt;&gt;&gt;the same result!<br/>&gt;&gt;<br/>&gt;&gt;No, if you&apos;re using TOP you&apos;re considering all intervals because &gt; you &gt;&gt;chose TOP which makes them the same.<br/>&gt; &gt; Huh? Makes what the same? Is this the same type of argument as above? &gt; You can&apos;t very well claim that I&apos;m necessarily considering the wider &gt; intervals in the optimization when ignoring them gives the results I &gt; present.</p><p>As you keep making the same argument, I keep giving the same reply.  Or am I supposed to think up innovative new responses?</p><p>&gt;&gt;&gt;I don&apos;t understand that. Aren&apos;t they (the latter intervals) more &gt;&gt;&gt;damaged by mistuning?<br/>&gt;&gt;<br/>&gt;&gt;Yes, that&apos;s what will happen if you give them lower weight.<br/>&gt; &gt; Well, it&apos;s not something my ears seem to like.</p><p>So you accept that a different weighting can give a different emphasis to the small intervals?</p><p>&gt;&gt;&gt;Well, for triads, mean and minimax give the same thing, while RMS &gt;&gt;&gt;gives something else. So I&apos;m not sure exactly what you mean by &gt;&gt;&gt;this . . .<br/>&gt;&gt;<br/>&gt;&gt;RMS is a kind of mean.  That&apos;s what the &quot;M&quot; stands for.<br/>&gt; &gt; Yes I know but you didn&apos;t clarify your statement, you just snipped it.</p><p>The mean (absolute) error of 2, 3 and 5 is going to be different to the minimax error.  Why do you say otherwise?  What do triads have to do with it anyway?</p><p>&gt; A prime-based measure always uses sum of absolute errors?</p><p>An RMS of prime errors always ignores the sign.  That comes from the square of a real number always being positive.  Summing is a part of calculating the mean.</p><p>                      Graham</p></div><h3><a id=13157 href="#13157">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/2/2005 7:18:27 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; I don&apos;t know what you are using for a weighting, but it probably isn&apos;t<br/>&gt; enough; you might try the p^(-1/2) as a weight for the prime p (apeing<br/>&gt; the Zeta function on the critical line.)</p><p>I&apos;m using Tenney weighting: 1/log(p).  Yes, it isn&apos;t enough for stability.  But what I originally said is that any weighting that is stable over all primes will be too biased towards the very simple ratios.</p><p>              Graham</p></div><h3><a id=13158 href="#13158">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/2/2005 7:17:56 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; Why not? Routines are as common as dirt.</p><p>I can do the weighted-prime least squares optimization with 14 lines of Python.  No libraries and mostly simple arithmetic.  If the TOP requires a specialist library routine, it can&apos;t be as simple.</p><p>&gt;   Can you find one &gt;&gt;for Python?  &gt; &gt; Probably.</p><p>Then where is it?</p><p>&gt;&gt;How efficient is it for optimizing a &gt;&gt;billion 19-limit linear temperaments?<br/>&gt; &gt; Why in the world do you want to do that? But simplex algorithms run<br/>&gt; pretty fast for low dimensional problems like that in particular.</p><p>All combinations of 100 commas would do it comfortably.  The least squares error of weighted primes is really snot-drenchingly fast.  A single pass over the primes, and no unpredictable branches to break the pipeline.  It is possible to get simpler (say, set the weighted, signed prime error to zero) but at least this is an optimization of some sensible kind of average.</p><p>                             Graham</p></div><h3><a id=13160 href="#13160">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/2/2005 12:22:06 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; Then where is it?</p><p>Did you try googling for it? Python simplex returns 84000 hits, the<br/>first of which already gives you code.</p></div><h3><a id=13161 href="#13161">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/2/2005 12:59:09 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; Did you try googling for it? Python simplex returns 84000 hits, the<br/>&gt; first of which already gives you code.</p><p>How am I supposed to Google unless I know what I&apos;m Googling for?  In this case, the first few hits give dead links.  But I want a Nedler-Mead Simplex, do I?  It looks a bit drastic for an apparently simple optimization.  How do I specify the TOP function?  Can I assume the worst weighted prime error is also the minimax?</p><p>                Graham</p></div><h3><a id=13162 href="#13162">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/2/2005 2:36:03 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; How am I supposed to Google unless I know what I&apos;m Googling for?  In<br/>&gt; this case, the first few hits give dead links.  But I want a<br/>Nedler-Mead<br/>&gt; Simplex, do I?</p><p>No, stay away from that. You want a linear programming routine, and<br/>simplex would be fine for the low-dimension problems of music. Other<br/>methods are the projective method and the predictor-corrector method.<br/>Googling on &quot;linear programming python&quot; brings up 743000 hits, the<br/>first of which is a linear programming routine called PuLP. I have no<br/>idea what the best routine would be, but clearly they are available.</p><p>It looks a bit drastic for an apparently simple<br/>&gt; optimization.  How do I specify the TOP function?</p><p>You want a tuning &lt;x2, x3, ..., xp|, so you can consider the x&apos;s to be<br/>variables to be solved for. You have a certain number of commas in the<br/>comma basis for the temperament, and you add these to start out with,<br/>as equations: c2x2 + c3x3 + ... + cpxp = 0. Then you add inequalities,<br/>for each prime q you add r &gt;= 1-xq/log2(q) and r &gt;= xq/log2(q)-1,<br/>which allows the routine to deal with r &gt;= |xq/log2(q)-1|. Then you<br/>minimize r subject to these constraints, to which you may add<br/>nonnegativity; the values xq where r reaches the minimum are your tuning.</p></div><h3><a id=13170 href="#13170">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/3/2005 12:27:26 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; Any differences for<br/>&gt; &gt; the systems in my paper?<br/>&gt;<br/>&gt; I&apos;ll check, but if you have handy a table of wedgies you could post<br/>&gt; here it would be nice.</p><p>I know you insist that wedgies are wedge products of vals and not<br/>wedge products of commas, but since taking the dual is so easy, I&apos;ll<br/>give you the latter anyway :)</p><p>Commas&apos; bivector        Horagram name<br/>[[-14 0 8 0 -5 0&gt;&gt;&#x9;Blacksmith<br/>[[2 -5 3 -4 4 -4&gt;&gt;&#x9;Dimisept<br/>[[16 -6 -4 2 4 -1&gt;&gt;&#x9;Dominant<br/>[[-14 1 7 -6 0 -3&gt;&gt;&#x9;August<br/>[[-2 -12 11 4 -4 -2&gt;&gt;&#x9;Pajara<br/>[[20 -4 -8 -1 8 -2&gt;&gt;&#x9;Semaphore<br/>[[-12 13 -4 -10 4 -1&gt;&gt;&#x9;Meantone<br/>[[4 7 -8 -8 8 -2&gt;&gt;&#x9;Injera<br/>[[13 8 -14 2 3 4&gt;&gt;&#x9;Negrisept<br/>[[14 -18 7 6 0 -3&gt;&gt;&#x9;Augene<br/>[[-7 12 -6 3 -5 6&gt;&gt;&#x9;Keemun<br/>[[28 -19 0 12 0 0&gt;&gt;&#x9;Catler<br/>[[-5 1 2 10 -10 6&gt;&gt;&#x9;Hedgehog<br/>[[-30 6 12 -2 -9 1&gt;&gt;&#x9;Superpyth<br/>[[-5 1 2 -13 9 -7&gt;&gt;&#x9;Sensisept<br/>[[1 20 -17 -2 2 6&gt;&gt;&#x9;Lemba<br/>[[-28 18 1 -6 -5 3&gt;&gt;&#x9;Porcupine<br/>[[32 -17 -4 9 4 -1&gt;&gt;&#x9;Flattone<br/>[[25 -5 -10 12 -1 5&gt;&gt;&#x9;Magic<br/>[[-3 13 -9 6 -6 8&gt;&gt;&#x9;Doublewide<br/>[[-21 12 2 3 -10 6&gt;&gt;&#x9;Nautilus<br/>[[-16 -12 19 4 -9 -2&gt;&gt;&#x9;Beatles<br/>[[8 9 -12 -11 12 -3&gt;&gt;&#x9;Liese<br/>[[36 -10 -12 1 12 -3&gt;&gt;&#x9;Cynder<br/>[[27 7 -21 8 3 7&gt;&gt;&#x9;Orwell<br/>[[-10 25 -15 -14 8 1&gt;&gt;&#x9;Garibaldi<br/>[[9 -17 9 -7 9 -10&gt;&gt;&#x9;Myna<br/>[[15 20 -25 -2 7 6&gt;&gt;&#x9;Miracle</p><p>bonus:</p><p>[[-34 22 1 18 -27 18&gt;&gt;&#x9;Ennealimmal</p><p>I&apos;d also like to know, when the TOP tuning is not unique (such as for<br/>5-limit Blackwood), whether a stretched Kees tuning *could* be a (non-<br/>canonical) TOP tuning.</p><p>&gt; &gt; Based on TOP, I told Igliashon that 13-equal<br/>&gt; &gt; is better in the 7-limit using the Orwell approximation than any<br/>other<br/>&gt; &gt; approximation of the 7-limit in 13-equal. Is this still true<br/>based on<br/>&gt; &gt; Kees?<br/>&gt;<br/>&gt; I don&apos;t even know what you mean.</p><p>I think I could state this as: Is the best 13-equal val for the 7-<br/>limit the one which is an Orwell val?</p></div><h3><a id=13176 href="#13176">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/3/2005 2:21:31 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Paul Erlich wrote:<br/>&gt;<br/>&gt; &gt;&gt;For the least squares weighted prime function of the nearest<br/>prime<br/>&gt; &gt;&gt;approximation to 73-equal:<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt;1000 0.000386288890546 1.00000525939<br/>&gt; &gt;&gt;2000 0.000342631907251 0.999999878426<br/>&gt; &gt;&gt;3000 0.000320740521709 1.00000066141<br/>&gt; &gt;&gt;4000 0.00030851462036 0.999999470906<br/>&gt; &gt;&gt;5000 0.000298744238076 1.00000050417<br/>&gt; &gt;&gt;6000 0.000291497890021 0.99999998381<br/>&gt; &gt;&gt;7000 0.000285393298895 0.999999137131<br/>&gt; &gt;&gt;8000 0.000279937136959 1.00000022932<br/>&gt; &gt;&gt;9000 0.000276009346747 0.999999628759<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt;The left hand column is the number of primes,<br/>&gt; &gt;<br/>&gt; &gt; !!! You need to specify your mapping before beginning the<br/>&gt; &gt; optimization. In this case, you&apos;d need mappings with hundreds or<br/>&gt; &gt; thousands of elements.<br/>&gt;<br/>&gt; I did specify it: &quot;nearest prime approximation&quot;.  I could try<br/>listing<br/>&gt; all the elements, but I hardly think it would be an appropriate use<br/>of<br/>&gt; bandwidth.</p><p>As you know, I don&apos;t endorse &quot;nearest prime approximation&quot;, so I see<br/>no reason to take these kinds of considerations into account. Do<br/>you?</p><p>&gt; &gt; I have no idea what you&apos;re doing with this many primes, nor what<br/>this<br/>&gt; &gt; is supposed to show, I&apos;m afraid.<br/>&gt;<br/>&gt; It shows that the weighted error depends too much on ridiculously<br/>&gt; complex intervals.</p><p>It doesn&apos;t show that to me. You have to hold the mapping constant if<br/>you really want to show something like that.</p><p>&gt; &gt;&gt;15:8 is part of the 5-limit, and so part of a weighted primes<br/>&gt; &gt;&gt;optimization.<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t need to explicity include it. I could claim that its<br/>tuning<br/>&gt; &gt; is purely a result of the tuning of simpler intervals.<br/>&gt;<br/>&gt; If you choose a prime limit of 5, you include it unless you<br/>explicitly<br/>&gt; exclude it.</p><p>No I don&apos;t. I get the same optimal tuning either way.</p><p>&gt; &gt;&gt;7:4 is outside the 5 prime-limit, and so isn&apos;t considered<br/>&gt; &gt;&gt;at all.<br/>&gt; &gt;<br/>&gt; &gt; If you don&apos;t care to have it in your scales, that strikes me as<br/>the<br/>&gt; &gt; right thing to do.<br/>&gt;<br/>&gt; But why does every interval involving a prime of 7 or larger have<br/>to<br/>&gt; come with it?</p><p>You mean in the optimization? It doesn&apos;t -- you get the same answer<br/>with virtually any cutoff on the complexity of these intervals.<br/>Otherwise, they come with it simply because the lattice extends<br/>infinitely in all directions; every traceable move in the lattice<br/>corresponds to some rational interval.</p><p>&gt; &gt;&gt;We have a weighting and a limit that isn&apos;t based on complexity.<br/>&gt; &gt; That<br/>&gt; &gt;&gt;compromises simplicity against only having the weighting, or<br/>&gt; &gt;&gt;sensibleness against considering all primes and getting rubbish<br/>out<br/>&gt; &gt;&gt;(unless you can demonstrate otherwise).<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t understand why this argument applies to TOP and not to<br/>all<br/>&gt; &gt; the other methods, which also use a limit &quot;that isn&apos;t based on<br/>&gt; &gt; complexity&quot;. But with TOP, you can consider, say, all the<br/>intervals<br/>&gt; &gt; with an integer limit of 10, and Tenney-weight their errors. With<br/>a<br/>&gt; &gt; prime limit of 7, there are no &quot;holes&quot; -- there are no intervals<br/>&gt; &gt; missing that &quot;compromise&quot; the weighting. High primes don&apos;t need<br/>to<br/>&gt; &gt; enter the &quot;compromise&quot; scenario since you never have to<br/>explicitly<br/>&gt; &gt; weight the complex intervals anyway.<br/>&gt;<br/>&gt; It applies to any method that uses Tenney weighting with a prime<br/>&gt;limit.</p><p>But what about all the other methods (which is what I was asking<br/>about)?</p><p>&gt;   Where do I say otherwise?</p><p>Huh. If we replace &quot;TOP&quot; above with &quot;any method that uses Tenney<br/>weighting with a prime limit&quot;, can we proceed with this conversation?</p><p>&gt; &gt; I think it&apos;s pretty clear from the various models (such as<br/>harmonic<br/>&gt; &gt; entropy) and testimonies we have that by and large, simpler<br/>intervals<br/>&gt; &gt; are more sensitive to mistuning than slightly more complex ones.<br/>Even<br/>&gt; &gt; George Secor, who has used equal-weighted minimax, ended up<br/>agreeing<br/>&gt; &gt; with this!<br/>&gt;<br/>&gt; It doesn&apos;t matter how sensitive they are to mistuning unless<br/>&gt;they&apos;re the<br/>&gt; most painful intervals</p><p>I completely disagree! If a mistuning makes a pleasant interval<br/>painful, it matters a lot more than if a mistuning makes an already<br/>painful interval stay at about the same level of pain.</p><p>&gt; -- if you use a minimax.</p><p>Don&apos;t follow.</p><p>&gt; With a weighted mean,<br/>&gt; you can give most weight to the most sensitive intervals.</p><p>You can do that with a weighted mean of any kind, including the<br/>weighted minimax kind (an L_infinity weighted mean).</p><p>&gt; &gt; Of course! But this is a completely different question. First of<br/>all,<br/>&gt; &gt; 15:8 can be the most dissonant interval while being the most<br/>damaged,<br/>&gt; &gt; or while being the least damaged, interval in the chord,<br/>depending on<br/>&gt; &gt; the tuning. This is closer to what we&apos;re actually discussing<br/>here. In<br/>&gt; &gt; TOP, the damage on the most dissonant interval matters least, far<br/>&gt; &gt; from that interval mattering most as you imply above. Secondly,<br/>we&apos;re<br/>&gt; &gt; not talking about a single chord, we&apos;re talking about an entire<br/>&gt; &gt; tuning system, and minimax can mean a lot of different things in<br/>that<br/>&gt; &gt; context. Your example seems like a perfect argument for TOP, in<br/>fact,<br/>&gt; &gt; in that major seventh chords participate in a lot of the<br/>consonances<br/>&gt; &gt; in the lattice, and you don&apos;t want any of these consonances to be<br/>too<br/>&gt; &gt; far off, while the 15:8 itself can be further off while doing<br/>less<br/>&gt; &gt; damage to the chord as a whole.<br/>&gt;<br/>&gt; To me, it&apos;s the same question that leads to an unbounded, weighted<br/>&gt; average.</p><p>How so?</p><p>&gt; You can&apos;t be sure what supplementary intervals you might end<br/>&gt; up using, so you try to get them as good as possible but don&apos;t make<br/>them<br/>&gt; as important as the primary consonances.</p><p>OK, so what&apos;s the problem?</p><p>&gt; I disagree with what you say about TOP.  Every interval you<br/>consider<br/>&gt; (and you get the same result whether you consider them or not, so<br/>this<br/>&gt; is a fairly nebulous consideration) is exactly as important as<br/>every<br/>&gt; other interval.  The only difference is that some intervals are<br/>allowed<br/>&gt; more scope for mistuning than others.  A more complex interval can<br/>take<br/>&gt; more mistuning for the same amount of damage.<br/>&gt;<br/>&gt; The point of this is &quot;good intervals in a chord make up for the bad<br/>&gt; ones.&quot;</p><p>I&apos;m not sure about this leap of yours from &quot;tuning&quot; to &quot;chord&quot;. How<br/>do you justify it?</p><p>&gt; That was my original statement that these comments stem from.<br/>&gt; The minimax is inappropriate in that context.  By enforcing the<br/>minimax,<br/>&gt; you assume that the worst interval (however you define it) carries<br/>all<br/>&gt; the badness of the chord.</p><p>Not applicable to the tuning as a whole. You may or may not even be<br/>using more than 2-voice chord in the music. But in the tuning as a<br/>whole, an infinite number of intervals will be &quot;worst&quot;. Even ignoring<br/>that, I don&apos;t see how this is at all like your major seventh chord<br/>example.</p><p>&gt; If the goodness of the chord is a trade-off<br/>&gt; between the good and bad intervals, then it should also be a trade-<br/>off<br/>&gt; between well and poorly tuned intervals -- hence a mean and not a<br/>&gt;minimax.</p><p>Perhaps. But since we&apos;re dealing with a set of non-independent<br/>intervals, things aren&apos;t quite as they seem here. For example, for<br/>triads, mean and minimax exactly the same (or proportional)<br/>quantities! Either way, there are arguments to be made for minimax,<br/>by George Secor for example.</p><p>&gt; &gt;&gt;It&apos;s the average<br/>&gt; &gt;<br/>&gt; &gt; You mean RMS?<br/>&gt;<br/>&gt; You can do any average of absolute errors you like.  The RMS<br/>happens to<br/>&gt; be the simplest to optimize.</p><p>RMS means you&apos;re squaring the absolute errors before taking the<br/>average. If that counts as one way of doing &quot;any average of absolute<br/>errors you like&quot;, so should minimax, which replaces squaring with<br/>taking a very high power.</p><p>&gt; &gt;&gt;error of the primes.  That&apos;s not a difficult &gt;concept.<br/>&gt; &gt;<br/>&gt; &gt; The idea is to work through the implications for *all* the<br/>intervals,<br/>&gt; &gt; as has been done with TOP (see, for example, footnote xxvi in my<br/>&gt; &gt; paper). Otherwise, there seems little justification for going<br/>along<br/>&gt; &gt; with something that just looks at the primes and nothing else.<br/>&gt;<br/>&gt; That&apos;s exactly the attitude I tried to argue against, and you keep<br/>&gt; avoiding by saying &quot;it doesn&apos;t matter which intervals you choose&quot;.</p><p>Huh? I don&apos;t get it. Is the problem that I said *all* intervals<br/>rather than just some reasonable set? Clearly the former implies that<br/>you&apos;ve taken care of the latter, so that shouldn&apos;t be a problem.</p><p>If many, many different optimizations lead to the very same tuning,<br/>that only serves as a still stronger justification for the tuning --<br/>far from weakening it below what a single optimization would yield.<br/>Do you disagree?</p><p>&gt; &gt; I agree if you&apos;re saying that we should first check that the<br/>ratios<br/>&gt; &gt; or chords we intend to use as basic consonances aren&apos;t closer to<br/>&gt; &gt; other, equally simple ratios or chords than the ones they&apos;re<br/>supposed<br/>&gt; &gt; to approximate. But I&apos;d rather leave this to the user, who will<br/>&gt; &gt; simply chuck out some of the TOP systems as a result, and keep<br/>the<br/>&gt; &gt; rest of them -- specifying &quot;the ratios or chords we intend to use<br/>as<br/>&gt; &gt; basic consonances&quot; is more than I need or want for the purpose of<br/>&gt; &gt; setting out some TOP tunings. The hurdle can always be placed at<br/>the<br/>&gt; &gt; end without affecting the results.<br/>&gt;<br/>&gt; It could be placed either end.  What I&apos;m saying is that the hurdle<br/>of<br/>&gt; worst-error is different in nature to the consideration of average<br/>&gt; mistuning.</p><p>Which is different from worst mistuning which is different from<br/>average error. Yes, they&apos;re all different.</p><p>&gt; &gt;&gt;I think the<br/>&gt; &gt;&gt;various dissonance graphs back this up.  If you draw a line for a<br/>&gt; &gt;&gt;particular worst dissonance level, the complex intervals have a<br/>&gt; &gt; narrower<br/>&gt; &gt;&gt;range than the simpler ones.<br/>&gt; &gt;<br/>&gt; &gt; Right, but a given mistuning has a considerably greater impact on<br/>the<br/>&gt; &gt; consonance of the simpler interval than it does on the consonance<br/>of<br/>&gt; &gt; the complex ones. So if our target harmony is all the intervals<br/>in a<br/>&gt; &gt; big harmonic-series chord, say, the closeness of the sound to JI<br/>is<br/>&gt; &gt; best judged by weighting the errors on the simpler intervals<br/>*more*.<br/>&gt;<br/>&gt; The point of minimax, as I said before, is that all intervals have<br/>&gt; *equal* weight.</p><p>Of course that&apos;s not true, since one can do weighted minimax. So I<br/>don&apos;t know what you mean.</p><p>&gt; I find it so obvious that your argument there</p><p>Where?</p><p>&gt; applies<br/>&gt; to a mean, and not a minimax, that I&apos;m not sure how to make it<br/>&gt;explicit.</p><p>Well, don&apos;t give up! And I&apos;m still very much open to the idea of an<br/>L_2 version of TOP, which you came up with an acronym for like a year<br/>and a half ago, and little has been said about since just recently. I<br/>just want to understand better what it *means* (what its implications<br/>are for all the intervals we might care about) in some fairly<br/>finitary, comprehensible way.</p><p>&gt; &gt; You could turn the argument around and say that no matter how you<br/>&gt; &gt; look at it, you&apos;re implicitly considering the primes and only the<br/>&gt; &gt; primes. Either way, the argument is invalid. The mathematical<br/>&gt; &gt; identity of these various results doesn&apos;t mean that if one<br/>accepts<br/>&gt; &gt; one set of desiderata that lead to it, you&apos;re automatically<br/>accepting<br/>&gt; &gt; some other set of desiderata that also lead to it.<br/>&gt;<br/>&gt; So what other conditions lead to a Tenney weighting?</p><p>The conditions we&apos;re talking about all lead to the same tuning *if*<br/>you&apos;re using Tenney weighting to begin with -- I don&apos;t see how they<br/>could possibly *lead* to a weighting themselves. Given that the<br/>inverse of Tenney Harmonic Distance is the appropriate coefficient<br/>for converting mistuning to damage, you can include just the primes<br/>in the optimization; you can include just the intervals within one<br/>octave; you can include just the intervals below any limit on n*d (as<br/>long as this limit is not lower than the lowest prime); you can<br/>choose just the intervals within one octave that are below almost any<br/>limit on n*d; etc., and you get the same minimax-damage tuning.</p><p>&gt; &gt;&gt;&gt;Again, you appear to be missing another important feature of<br/>TOP,<br/>&gt; &gt;&gt;&gt;which is that you&apos;re free to consider only the intervals within<br/>&gt; &gt; some<br/>&gt; &gt;&gt;&gt;range of interest for your optimization, and you still get<br/>&gt; &gt; exactly<br/>&gt; &gt;&gt;&gt;the same result!<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt;No, if you&apos;re using TOP you&apos;re considering all intervals because<br/>&gt; &gt; you<br/>&gt; &gt;&gt;chose TOP which makes them the same.<br/>&gt; &gt;<br/>&gt; &gt; Huh? Makes what the same? Is this the same type of argument as<br/>above?<br/>&gt; &gt; You can&apos;t very well claim that I&apos;m necessarily considering the<br/>wider<br/>&gt; &gt; intervals in the optimization when ignoring them gives the<br/>results I<br/>&gt; &gt; present.<br/>&gt;<br/>&gt; As you keep making the same argument, I keep giving the same<br/>reply.  Or<br/>&gt; am I supposed to think up innovative new responses?</p><p>I don&apos;t know, because I really think my argument is valid! If<br/>ignoring wider intervals could give a different answer, that answer<br/>would sure be interesting and important. Since it doesn&apos;t, the<br/>original answer must have wider applicability than we originally<br/>supposed.</p><p>&gt; &gt;&gt;&gt;I don&apos;t understand that. Aren&apos;t they (the latter intervals) more<br/>&gt; &gt;&gt;&gt;damaged by mistuning?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt;Yes, that&apos;s what will happen if you give them lower weight.<br/>&gt; &gt;<br/>&gt; &gt; Well, it&apos;s not something my ears seem to like.<br/>&gt;<br/>&gt; So you accept that a different weighting can give a different<br/>emphasis<br/>&gt; to the small intervals?</p><p>In this case, I was accepting that a different weighting can give a<br/>different emphasis to the *simple* intervals.</p><p>&gt; &gt;&gt;&gt;Well, for triads, mean and minimax give the same thing, while<br/>RMS<br/>&gt; &gt;&gt;&gt;gives something else. So I&apos;m not sure exactly what you mean by<br/>&gt; &gt;&gt;&gt;this . . .<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt;RMS is a kind of mean.  That&apos;s what the &quot;M&quot; stands for.<br/>&gt; &gt;<br/>&gt; &gt; Yes I know but you didn&apos;t clarify your statement, you just<br/>snipped it.<br/>&gt;<br/>&gt; The mean (absolute) error of 2, 3 and 5 is going to be different to<br/>the<br/>&gt; minimax error.  Why do you say otherwise?</p><p>I did? Where?</p><p>&gt; What do triads have to do<br/>&gt; with it anyway?</p><p>Triads are just a simple example of how our reasoning can lead us<br/>astray if we forget that the terms in our optimization are not all<br/>independent.</p><p>&gt; &gt; A prime-based measure always uses sum of absolute errors?<br/>&gt;<br/>&gt; An RMS of prime errors always ignores the sign.  That comes from<br/>the<br/>&gt; square of a real number always being positive.  Summing is a part<br/>of<br/>&gt; calculating the mean.</p><p>All that is quite clear so I suspect my query referred to something<br/>wider than just the RMS case. Perhaps I had misunderstood something<br/>you wrote earlier.</p><p>Oh, and I still need to read part of your post from last week --<br/>don&apos;t let me forget! (That is, remind me!!)</p></div><h3><a id=13177 href="#13177">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/3/2005 2:26:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Gene Ward Smith wrote:<br/>&gt;<br/>&gt; &gt; I don&apos;t know what you are using for a weighting, but it probably<br/>isn&apos;t<br/>&gt; &gt; enough; you might try the p^(-1/2) as a weight for the prime p<br/>(apeing<br/>&gt; &gt; the Zeta function on the critical line.)<br/>&gt;<br/>&gt; I&apos;m using Tenney weighting: 1/log(p).  Yes, it isn&apos;t enough for<br/>&gt; stability.  But what I originally said is that any weighting that<br/>is<br/>&gt; stable over all primes will be too biased towards the very simple<br/>ratios.<br/>&gt;<br/>&gt;<br/>&gt;                Graham</p><p>I don&apos;t buy into this notion of &quot;stability&quot;. We&apos;re looking at more<br/>than just ETs here. Nearest prime approximations in an ET don&apos;t mean<br/>very much to me, you&apos;ve agreed they&apos;re don&apos;t lead to the &quot;best&quot; val.<br/>And regardless of which prime approximations you&apos;re talking about, I<br/>don&apos;t know why anyone would expect stability when appending more and<br/>more of these onto the val (given that the optimizations we&apos;re<br/>talking about require you to specify a val). It doesn&apos;t seem like a<br/>reasonable expectation.</p></div><h3><a id=13197 href="#13197">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/4/2005 2:03:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; No, stay away from that. You want a linear programming routine, and<br/>&gt; simplex would be fine for the low-dimension problems of music. Other<br/>&gt; methods are the projective method and the predictor-corrector method.<br/>&gt; Googling on &quot;linear programming python&quot; brings up 743000 hits, the<br/>&gt; first of which is a linear programming routine called PuLP. I have no<br/>&gt; idea what the best routine would be, but clearly they are available.</p><p>I found the Nedler-Mead Simplex on the Way Back Machine, anyway.  I got it working for a normal (1-D) minimax so it will presumably work for TOP.  It won&apos;t be especially efficient.  Fine for one temperament at a time, but even slower than my current minimax algorithm.  The problem is that it&apos;s easy to get the gradient for a minimax, but this function only uses the value.</p><p>PuLP looks interesting -- symbolic algebra with Python syntax.  I&apos;ll have to look into it.  It depends on C code as well.  It looks like it&apos;d work for least squares optimizations too :P</p><p>&gt; You want a tuning &lt;x2, x3, ..., xp|, so you can consider the x&apos;s to be<br/>&gt; variables to be solved for. You have a certain number of commas in the<br/>&gt; comma basis for the temperament, and you add these to start out with,<br/>&gt; as equations: c2x2 + c3x3 + ... + cpxp = 0. Then you add inequalities,<br/>&gt; for each prime q you add r &gt;= 1-xq/log2(q) and r &gt;= xq/log2(q)-1,<br/>&gt; which allows the routine to deal with r &gt;= |xq/log2(q)-1|. Then you<br/>&gt; minimize r subject to these constraints, to which you may add<br/>&gt; nonnegativity; the values xq where r reaches the minimum are your tuning.</p><p>I only need to look at the primes, then?  I don&apos;t have the commas at this stage, but writing a formula for the mistuning of each prime in terms of the generator and period is easy.</p><p>                Graham</p></div><h3><a id=13198 href="#13198">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/4/2005 2:03:38 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul Erlich wrote:</p><p>&gt;&gt;It applies to any method that uses Tenney weighting with a prime &gt;&gt;limit.<br/>&gt; &gt; But what about all the other methods (which is what I was asking &gt; about)?</p><p>What other methods are there?  I only know Tenney weighted prime limits, the odd limit (which is an octave-equivalent guess to complexity) and integer-limits (which are complexity).</p><p>&gt; Huh. If we replace &quot;TOP&quot; above with &quot;any method that uses Tenney &gt; weighting with a prime limit&quot;, can we proceed with this conversation?</p><p>It wouldn&apos;t make much sense because we&apos;re comparing two differnet Tenney-weighted measures.  I&apos;m not sure where this is leading (or where it&apos;s gone!)</p><p>&gt; I completely disagree! If a mistuning makes a pleasant interval &gt; painful, it matters a lot more than if a mistuning makes an already &gt; painful interval stay at about the same level of pain.</p><p>What&apos;s &quot;pain&quot;?  Are you suggesting &quot;tuning relative to JI&quot; is a directly observable measure?  I&apos;m looking at dissonance.</p><p>&gt; You can do that with a weighted mean of any kind, including the &gt; weighted minimax kind (an L_infinity weighted mean).</p><p>Oh, then I bow to you superior knowledge of statistics.  The RMS will give a better average because all intervals have some weight.</p><p>TOP will do fine as an approximation to the RMS, or an extreme kind of mean.  The only problem is that it&apos;s more complicated.  If it were simpler, I&apos;d argue for it.  As it is, I still have to spend time arguing   that the RMS is simpler.  I also happen to think that the prime-weighted RMS is better anyway but that&apos;s not so important.</p><p>&gt;&gt;To me, it&apos;s the same question that leads to an unbounded, weighted &gt;&gt;average.<br/>&gt; &gt; How so?</p><p>If you allow dissonant but not completely random intervals in chords, you care a bit how close they are to JI.  You don&apos;t care very much because the good intervals carry most of the consonance.  The weighting tells you how much you care.  This is only valid for chords that are musically dissonant, in particular where the dissonance is for coloration so that consonance still matters.</p><p>&gt;&gt;You can&apos;t be sure what supplementary intervals you might end &gt;&gt;up using, so you try to get them as good as possible but don&apos;t make &gt; them &gt;&gt;as important as the primary consonances.<br/>&gt; &gt; OK, so what&apos;s the problem?</p><p>No problem.</p><p>&gt; I&apos;m not sure about this leap of yours from &quot;tuning&quot; to &quot;chord&quot;. How &gt; do you justify it?</p><p>I&apos;m starting with chords and trying to justify the tuning.</p><p>&gt;&gt;That was my original statement that these comments stem from. &gt;&gt;The minimax is inappropriate in that context.  By enforcing the &gt; minimax, &gt;&gt;you assume that the worst interval (however you define it) carries &gt; all &gt;&gt;the badness of the chord.<br/>&gt; &gt; Not applicable to the tuning as a whole. You may or may not even be &gt; using more than 2-voice chord in the music. But in the tuning as a &gt; whole, an infinite number of intervals will be &quot;worst&quot;. Even ignoring &gt; that, I don&apos;t see how this is at all like your major seventh chord &gt; example.</p><p>It isn&apos;t like the major seventh example.  It&apos;s supposed to be the alternative idea.  For chords that are musically consonant, and so every interval should be heard as a consonance.  You assume that the simplicity of the interval and closeness to JI tell you how dissonant an interval will be.  Then you draw a line of maximum allowable dissonance for an interval to be counted as a consonance.  You check that all the intervals you want to be consonances are below the line.  The minimax error of an odd limit is a way of guessing if intervals are below the line.  There&apos;s all kinds of arbitrariness and approximation but we don&apos;t have a solid enough theory to avoid that anyway.</p><p>&gt;&gt;If the goodness of the chord is a trade-off &gt;&gt;between the good and bad intervals, then it should also be a trade-<br/>&gt; off &gt;&gt;between well and poorly tuned intervals -- hence a mean and not a &gt;&gt;minimax.<br/>&gt; &gt; Perhaps. But since we&apos;re dealing with a set of non-independent &gt; intervals, things aren&apos;t quite as they seem here. For example, for &gt; triads, mean and minimax exactly the same (or proportional) &gt; quantities! Either way, there are arguments to be made for minimax, &gt; by George Secor for example.</p><p>I don&apos;t understand the triads example.  What are George&apos;s arguments? You&apos;ve mentioned him twice now.</p><p>&gt; Huh? I don&apos;t get it. Is the problem that I said *all* intervals &gt; rather than just some reasonable set? Clearly the former implies that &gt; you&apos;ve taken care of the latter, so that shouldn&apos;t be a problem.</p><p>The problem is that you&apos;re looking at consistency rather than validity.</p><p>&gt; If many, many different optimizations lead to the very same tuning, &gt; that only serves as a still stronger justification for the tuning -- &gt; far from weakening it below what a single optimization would yield. &gt; Do you disagree?</p><p>It&apos;s a little stronger, yes.  But it isn&apos;t that important.  The main advantage is that it simplifies the definition and calculation, but any prime based measure will do that.</p><p>&gt;&gt;The point of minimax, as I said before, is that all intervals have &gt;&gt;*equal* weight.<br/>&gt; &gt; Of course that&apos;s not true, since one can do weighted minimax. So I &gt; don&apos;t know what you mean.</p><p>I don&apos;t see how it makes sense to weight the intervals and call it a minimax.  Any weighting has to be on the errors.</p><p>&gt; Well, don&apos;t give up! And I&apos;m still very much open to the idea of an &gt; L_2 version of TOP, which you came up with an acronym for like a year &gt; and a half ago, and little has been said about since just recently. I &gt; just want to understand better what it *means* (what its implications &gt; are for all the intervals we might care about) in some fairly &gt; finitary, comprehensible way.</p><p>It was PORMSWE for Prime Optimum RMS Weighted Error.  But it&apos;s not a good acronym because you lose the O when it isn&apos;t optimized.</p><p>The implication is that it&apos;s a guess as to what the Tenney weighted average mistuning of any given set of intervals (within the prime limit) will be.  If I trusted any way of selecting octave-specific intervals, I could do a comparison.</p><p>&gt; Triads are just a simple example of how our reasoning can lead us &gt; astray if we forget that the terms in our optimization are not all &gt; independent.</p><p>Can you explain this one?</p><p>&gt; &gt; &gt;&gt;&gt;A prime-based measure always uses sum of absolute errors?</p><p>...</p><p>&gt; All that is quite clear so I suspect my query referred to something &gt; wider than just the RMS case. Perhaps I had misunderstood something &gt; you wrote earlier.</p><p>You chop my paragraphs up so much in the replies I don&apos;t know what the original context was.</p><p>                  Graham</p></div><h3><a id=13199 href="#13199">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/4/2005 2:03:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul Erlich wrote:</p><p>&gt; I don&apos;t buy into this notion of &quot;stability&quot;. We&apos;re looking at more &gt; than just ETs here. Nearest prime approximations in an ET don&apos;t mean &gt; very much to me, you&apos;ve agreed they&apos;re don&apos;t lead to the &quot;best&quot; val. &gt; And regardless of which prime approximations you&apos;re talking about, I &gt; don&apos;t know why anyone would expect stability when appending more and &gt; more of these onto the val (given that the optimizations we&apos;re &gt; talking about require you to specify a val). It doesn&apos;t seem like a &gt; reasonable expectation.</p><p>It doesn&apos;t matter if it&apos;s the best val or not.  However badly the 1000th prime is approximated, it makes no audible difference to the temperament.  Any badness measure that it does make a difference to is wrong.</p><p>                Graham</p></div><h3><a id=13207 href="#13207">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/4/2005 4:02:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Paul Erlich wrote:<br/>&gt;<br/>&gt; &gt; I don&apos;t buy into this notion of &quot;stability&quot;. We&apos;re looking at<br/>more<br/>&gt; &gt; than just ETs here. Nearest prime approximations in an ET don&apos;t<br/>mean<br/>&gt; &gt; very much to me, you&apos;ve agreed they&apos;re don&apos;t lead to the &quot;best&quot;<br/>val.<br/>&gt; &gt; And regardless of which prime approximations you&apos;re talking<br/>about, I<br/>&gt; &gt; don&apos;t know why anyone would expect stability when appending more<br/>and<br/>&gt; &gt; more of these onto the val (given that the optimizations we&apos;re<br/>&gt; &gt; talking about require you to specify a val). It doesn&apos;t seem like<br/>a<br/>&gt; &gt; reasonable expectation.<br/>&gt;<br/>&gt; It doesn&apos;t matter if it&apos;s the best val or not.</p><p>That&apos;s not my main point. You&apos;re still appending more approximations<br/>to the system either way.</p><p>&gt; However badly the 1000th<br/>&gt; prime is approximated, it makes no audible difference to the<br/>&gt; temperament.  Any badness measure that it does make a difference to<br/>is<br/>&gt; wrong.</p><p>I don&apos;t see how this is more damaging to TOP than to any other<br/>method, once the idea of approximating intervals involving the 1000th<br/>prime enter the picture in the first place.</p></div><h3><a id=13208 href="#13208">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/4/2005 4:00:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Paul Erlich wrote:<br/>&gt;<br/>&gt; &gt;&gt;It applies to any method that uses Tenney weighting with a prime<br/>&gt; &gt;&gt;limit.<br/>&gt; &gt;<br/>&gt; &gt; But what about all the other methods (which is what I was asking<br/>&gt; &gt; about)?<br/>&gt;<br/>&gt; What other methods are there?  I only know Tenney weighted prime<br/>limits,<br/>&gt; the odd limit (which is an octave-equivalent guess to complexity)<br/>and<br/>&gt; integer-limits (which are complexity).</p><p>&quot;Odd limit&quot; can be interpreted in several ways, including equal-<br/>weighted Hahn, weighted Hahn, and Kees . . . but yes, these all serve<br/>as examples of &quot;other methods&quot;.</p><p>&gt; &gt; Huh. If we replace &quot;TOP&quot; above with &quot;any method that uses Tenney<br/>&gt; &gt; weighting with a prime limit&quot;, can we proceed with this<br/>conversation?<br/>&gt;<br/>&gt; It wouldn&apos;t make much sense because we&apos;re comparing two differnet<br/>&gt; Tenney-weighted measures.  I&apos;m not sure where this is leading (or<br/>where<br/>&gt; it&apos;s gone!)</p><p>You said, &quot;It applies to any method that uses Tenney weighting with a<br/>prime limit.&quot; So how can it serve for a comparison between two<br/>different such methods?</p><p>&gt; &gt; I completely disagree! If a mistuning makes a pleasant interval<br/>&gt; &gt; painful, it matters a lot more than if a mistuning makes an<br/>already<br/>&gt; &gt; painful interval stay at about the same level of pain.<br/>&gt;<br/>&gt; What&apos;s &quot;pain&quot;?  Are you suggesting &quot;tuning relative to JI&quot; is a<br/>directly<br/>&gt; observable measure?</p><p>You can observe that, but more importantly, you can observe the<br/>differences between different tunings of the same temperament. And<br/>the differences sound qualitively greater, for a given cents<br/>difference in an interval, for the simpler ratios than for the more<br/>complex ratios.</p><p>&gt; I&apos;m looking at dissonance.</p><p>In terms of the relative dissonance of two tunings of a given<br/>temperament, I stand by my statement above -- a simpler interval<br/>differing by a given amount in cents between the two tunings will<br/>have more impact on the relative dissonance of the two than a more<br/>complex interval differing by the same amount (ceteris parebis).</p><p>&gt; &gt; You can do that with a weighted mean of any kind, including the<br/>&gt; &gt; weighted minimax kind (an L_infinity weighted mean).<br/>&gt;<br/>&gt; Oh, then I bow to you superior knowledge of statistics.  The RMS<br/>will<br/>&gt; give a better average because all intervals have some weight.</p><p>Is the logical conclusion of this that sum-absolute error gives a<br/>still better average?</p><p>&gt; TOP will do fine as an approximation to the RMS, or an extreme kind<br/>of<br/>&gt; mean.  The only problem is that it&apos;s more complicated.  If it were<br/>&gt; simpler, I&apos;d argue for it.</p><p>To me, it&apos;s simpler. Simpler definition, simpler motivation, and I<br/>don&apos;t think the calculation is significantly more complicated.</p><p>&gt; As it is, I still have to spend time arguing<br/>&gt;   that the RMS is simpler.  I also happen to think that the<br/>&gt; prime-weighted RMS is better anyway but that&apos;s not so important.</p><p>I, on the other hand, would like to see that pursued further.</p><p>&gt; &gt;&gt;To me, it&apos;s the same question that leads to an unbounded,<br/>weighted<br/>&gt; &gt;&gt;average.<br/>&gt; &gt;<br/>&gt; &gt; How so?<br/>&gt;<br/>&gt; If you allow dissonant but not completely random intervals in<br/>chords,<br/>&gt; you care a bit how close they are to JI.  You don&apos;t care very much<br/>&gt; because the good intervals carry most of the consonance.  The<br/>weighting<br/>&gt; tells you how much you care.  This is only valid for chords that<br/>are<br/>&gt; musically dissonant, in particular where the dissonance is for<br/>&gt; coloration so that consonance still matters.</p><p>OK; now how does this fit into our conversation?</p><p>&gt; &gt;&gt;You can&apos;t be sure what supplementary intervals you might end<br/>&gt; &gt;&gt;up using, so you try to get them as good as possible but don&apos;t<br/>make<br/>&gt; &gt; them<br/>&gt; &gt;&gt;as important as the primary consonances.<br/>&gt; &gt;<br/>&gt; &gt; OK, so what&apos;s the problem?<br/>&gt;<br/>&gt; No problem.</p><p>:)</p><p>&gt; &gt; I&apos;m not sure about this leap of yours from &quot;tuning&quot; to &quot;chord&quot;.<br/>How<br/>&gt; &gt; do you justify it?<br/>&gt;<br/>&gt; I&apos;m starting with chords and trying to justify the tuning.</p><p>I tend to think of chords as smallish blobs in the lattice. This is<br/>very different from thinking of them as similar to the tuning, or the<br/>whole lattice. So I&apos;m still not seeing the justification for the<br/>leap from one to the other in your reasoning.</p><p>&gt; &gt;&gt;That was my original statement that these comments stem from.<br/>&gt; &gt;&gt;The minimax is inappropriate in that context.  By enforcing the<br/>&gt; &gt; minimax,<br/>&gt; &gt;&gt;you assume that the worst interval (however you define it)<br/>carries<br/>&gt; &gt; all<br/>&gt; &gt;&gt;the badness of the chord.<br/>&gt; &gt;<br/>&gt; &gt; Not applicable to the tuning as a whole. You may or may not even<br/>be<br/>&gt; &gt; using more than 2-voice chord in the music. But in the tuning as<br/>a<br/>&gt; &gt; whole, an infinite number of intervals will be &quot;worst&quot;. Even<br/>ignoring<br/>&gt; &gt; that, I don&apos;t see how this is at all like your major seventh<br/>chord<br/>&gt; &gt; example.<br/>&gt;<br/>&gt; It isn&apos;t like the major seventh example.  It&apos;s supposed to be the<br/>&gt; alternative idea.  For chords that are musically consonant, and so<br/>every<br/>&gt; interval should be heard as a consonance.  You assume that the<br/>&gt; simplicity of the interval and closeness to JI tell you how<br/>dissonant an<br/>&gt; interval will be.  Then you draw a line of maximum allowable<br/>dissonance<br/>&gt; for an interval to be counted as a consonance.  You check that all<br/>the<br/>&gt; intervals you want to be consonances are below the line.  The<br/>minimax<br/>&gt; error of an odd limit is a way of guessing if intervals are below<br/>the<br/>&gt; line.  There&apos;s all kinds of arbitrariness and approximation but we<br/>don&apos;t<br/>&gt; have a solid enough theory to avoid that anyway.</p><p>OK (in general terms).</p><p>&gt; &gt;&gt;If the goodness of the chord is a trade-off<br/>&gt; &gt;&gt;between the good and bad intervals, then it should also be a<br/>trade-<br/>&gt; &gt; off<br/>&gt; &gt;&gt;between well and poorly tuned intervals -- hence a mean and not a<br/>&gt; &gt;&gt;minimax.<br/>&gt; &gt;<br/>&gt; &gt; Perhaps. But since we&apos;re dealing with a set of non-independent<br/>&gt; &gt; intervals, things aren&apos;t quite as they seem here. For example,<br/>for<br/>&gt; &gt; triads, mean and minimax exactly the same (or proportional)<br/>&gt; &gt; quantities! Either way, there are arguments to be made for<br/>minimax,<br/>&gt; &gt; by George Secor for example.<br/>&gt;<br/>&gt; I don&apos;t understand the triads example.</p><p>Try evaluating (ranking or rating) a set of tempered triads using<br/>both equal-weighted minimax error and equal-weighted sum-absolute<br/>error (I assume the latter is what you mean by &quot;mean&quot;). Do the two<br/>criteria lead to different conclusions?</p><p>&gt; What are George&apos;s arguments?<br/>&gt; You&apos;ve mentioned him twice now.</p><p>I want him to chime in himself!</p><p>&gt; &gt; Huh? I don&apos;t get it. Is the problem that I said *all* intervals<br/>&gt; &gt; rather than just some reasonable set? Clearly the former implies<br/>that<br/>&gt; &gt; you&apos;ve taken care of the latter, so that shouldn&apos;t be a problem.<br/>&gt;<br/>&gt; The problem is that you&apos;re looking at consistency rather than<br/>&gt;validity.</p><p>If *any* of the equivalent criteria that lead to the optimal result<br/>are valid (and I believe some of them are quite valid, it&apos;s just that<br/>I don&apos;t know *exactly* where to cut off the list of ratios for<br/>complexity), then the optimal result carries validity, regardless of<br/>how consistent it may be with the results of other &quot;less-valid&quot;<br/>optimizations. You seemed to be implying that this consistency is<br/>somehow a bad thing, as if it somehow forced you to claim that all<br/>the &quot;less-valid&quot; optimizations that lead to that same result are<br/>therefore themselves valid. This doesn&apos;t follow.</p><p>&gt; &gt; If many, many different optimizations lead to the very same<br/>tuning,<br/>&gt; &gt; that only serves as a still stronger justification for the<br/>tuning --<br/>&gt; &gt; far from weakening it below what a single optimization would<br/>yield.<br/>&gt; &gt; Do you disagree?<br/>&gt;<br/>&gt; It&apos;s a little stronger, yes.  But it isn&apos;t that important.  The<br/>main<br/>&gt; advantage is that it simplifies the definition and calculation, but<br/>any<br/>&gt; prime based measure will do that.</p><p>I&apos;d like to see that worked out, especially as regards definition,<br/>for the case of weighted prime RMS. But I believe there&apos;s part of a<br/>recent message from you that I still need to really read and respond<br/>to -- please don&apos;t let me forget!</p><p>&gt; &gt;&gt;The point of minimax, as I said before, is that all intervals<br/>have<br/>&gt; &gt;&gt;*equal* weight.<br/>&gt; &gt;<br/>&gt; &gt; Of course that&apos;s not true, since one can do weighted minimax. So<br/>I<br/>&gt; &gt; don&apos;t know what you mean.<br/>&gt;<br/>&gt; I don&apos;t see how it makes sense to weight the intervals and call it<br/>a<br/>&gt; minimax.  Any weighting has to be on the errors.</p><p>OK, I think I see what you mean. But this doesn&apos;t argue against<br/>weighting the errors in any way I can see.</p><p>&gt; &gt; Well, don&apos;t give up! And I&apos;m still very much open to the idea of<br/>an<br/>&gt; &gt; L_2 version of TOP, which you came up with an acronym for like a<br/>year<br/>&gt; &gt; and a half ago, and little has been said about since just<br/>recently. I<br/>&gt; &gt; just want to understand better what it *means* (what its<br/>implications<br/>&gt; &gt; are for all the intervals we might care about) in some fairly<br/>&gt; &gt; finitary, comprehensible way.<br/>&gt;<br/>&gt; It was PORMSWE for Prime Optimum RMS Weighted Error.  But it&apos;s not<br/>a<br/>&gt; good acronym because you lose the O when it isn&apos;t optimized.<br/>&gt;<br/>&gt; The implication is that it&apos;s a guess as to what the Tenney weighted<br/>&gt; average</p><p>A straight average? Or RMS? Or . . . (?)</p><p>&gt; mistuning of any given set of intervals (within the prime limit)<br/>&gt; will be.  If I trusted any way of selecting octave-specific<br/>intervals, I<br/>&gt; could do a comparison.</p><p>Two ways are to use some Tenney limit, and some integer limit.</p><p>&gt; &gt; Triads are just a simple example of how our reasoning can lead us<br/>&gt; &gt; astray if we forget that the terms in our optimization are not<br/>all<br/>&gt; &gt; independent.<br/>&gt;<br/>&gt; Can you explain this one?</p><p>Hopefully your comparison of eq-wtd-sum-abs error vs. eq-wtd-minimax<br/>error will shed some light on this for you.</p><p>&gt; &gt;&gt;&gt;A prime-based measure always uses sum of absolute errors?<br/>&gt;<br/>&gt; ...<br/>&gt;<br/>&gt; &gt; All that is quite clear so I suspect my query referred to<br/>something<br/>&gt; &gt; wider than just the RMS case. Perhaps I had misunderstood<br/>something<br/>&gt; &gt; you wrote earlier.<br/>&gt;<br/>&gt; You chop my paragraphs up so much in the replies I don&apos;t know what<br/>the<br/>&gt; original context was.</p><p>Sorry! Try looking back over the posts, then, if you feel so<br/>inclined . . .</p></div><h3><a id=13211 href="#13211">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/5/2005 5:32:00 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul Erlich wrote:</p><p>&gt; &quot;Odd limit&quot; can be interpreted in several ways, including equal-<br/>&gt; weighted Hahn, weighted Hahn, and Kees . . . but yes, these all serve &gt; as examples of &quot;other methods&quot;.</p><p>I take &quot;odd limit&quot; to be a set of intervals.  It&apos;s a good approximation of complexity, but fails in comparison to the Tenney limit.</p><p>&gt; You said, &quot;It applies to any method that uses Tenney weighting with a &gt; prime limit.&quot; So how can it serve for a comparison between two &gt; different such methods?</p><p>It doesn&apos;t.  This thread is about TOP, not comparing TOP with one other method.</p><p>&gt; You can observe that, but more importantly, you can observe the &gt; differences between different tunings of the same temperament. And &gt; the differences sound qualitively greater, for a given cents &gt; difference in an interval, for the simpler ratios than for the more &gt; complex ratios.</p><p>I don&apos;t think the listeners can hear these differences.  Even relative to JI, how many piano players know what a 5:4 sound like?</p><p>&gt; In terms of the relative dissonance of two tunings of a given &gt; temperament, I stand by my statement above -- a simpler interval &gt; differing by a given amount in cents between the two tunings will &gt; have more impact on the relative dissonance of the two than a more &gt; complex interval differing by the same amount (ceteris parebis).</p><p>You&apos;re talking about &quot;impact&quot; there, which suggests averaging.  A minimax isn&apos;t about measuring the total impact, but whether one interval has enough force to break the chord.</p><p>&gt;&gt;Oh, then I bow to you superior knowledge of statistics.  The RMS &gt; will &gt;&gt;give a better average because all intervals have some weight.<br/>&gt; &gt; &gt; Is the logical conclusion of this that sum-absolute error gives a &gt; still better average?</p><p>No, but that some power will give the best approximation to how the ear does the averaging, however that might be.</p><p>&gt;&gt;TOP will do fine as an approximation to the RMS, or an extreme kind &gt; of &gt;&gt;mean.  The only problem is that it&apos;s more complicated.  If it were &gt;&gt;simpler, I&apos;d argue for it. &gt; &gt; To me, it&apos;s simpler. Simpler definition, simpler motivation, and I &gt; don&apos;t think the calculation is significantly more complicated.</p><p>The definitions are much the same either way.  (Well, you can take the &quot;R&quot; out of &quot;RMS&quot;).  I don&apos;t now about motivation.  But the calculation is surely more complicated for TOP.  How are you calculating it? Everything I&apos;ve seen so far is numerical optimization, but the least squares is algebraic.  And only 14 lines of simple code with no libraries.</p><p>&gt;&gt;As it is, I still have to spend time arguing &gt;&gt;  that the RMS is simpler.  I also happen to think that the &gt;&gt;prime-weighted RMS is better anyway but that&apos;s not so important.<br/>&gt; &gt; I, on the other hand, would like to see that pursued further.</p><p>Do you agree that weighting is more appropriate for the octave-equivalent case?  Or, at least, that an unweighted, octave-equivalent measure doesn&apos;t make sense?  This is all part of the same revelation to me.</p><p>&gt;&gt;If you allow dissonant but not completely random intervals in &gt; chords, &gt;&gt;you care a bit how close they are to JI.  You don&apos;t care very much &gt;&gt;because the good intervals carry most of the consonance.  The &gt; weighting &gt;&gt;tells you how much you care.  This is only valid for chords that &gt; are &gt;&gt;musically dissonant, in particular where the dissonance is for &gt;&gt;coloration so that consonance still matters.<br/>&gt; &gt; OK; now how does this fit into our conversation?</p><p>For these kind of chords, we want to know the average dissonance.  Or at least the average relative to the ideal tuning.  An average of weighted errors relative to JI is a good guess for that.</p><p>&gt; I tend to think of chords as smallish blobs in the lattice. This is &gt; very different from thinking of them as similar to the tuning, or the &gt; whole lattice. So I&apos;m still not seeing the justification for the  &gt; leap from one to the other in your reasoning.</p><p>I think of chords as more than one note sounding together.</p><p>&gt; I&apos;d like to see that worked out, especially as regards definition, &gt; for the case of weighted prime RMS. But I believe there&apos;s part of a &gt; recent message from you that I still need to really read and respond &gt; to -- please don&apos;t let me forget!</p><p>I don&apos;t have any correlation between the WPRMS and any other measure.  I had a look, but the math isn&apos;t as clean as the TOP :(</p><p>&gt; OK, I think I see what you mean. But this doesn&apos;t argue against &gt; weighting the errors in any way I can see.</p><p>You can weight the errors as long as the weighted error itself has a meaning.  That isn&apos;t something you can argue over.  But once you apply a minimax, only the worst interval (however weighted the error) matters.</p><p>&gt;&gt;The implication is that it&apos;s a guess as to what the Tenney weighted &gt;&gt;average<br/>&gt; &gt; A straight average? Or RMS? Or . . . (?)</p><p>Any average.  I use the word deliberately vaguely.  Ideally it will be the one closest to what we hear.  In practice, I expect an RMS will approximate an RMS best.  You can think of TOP as an average, and that&apos;s exact.</p><p>&gt;&gt;mistuning of any given set of intervals (within the prime limit) &gt;&gt;will be.  If I trusted any way of selecting octave-specific &gt; intervals, I &gt;&gt;could do a comparison.<br/>&gt; &gt; Two ways are to use some Tenney limit, and some integer limit.</p><p>Simple, but as I said, I don&apos;t trust them.  They don&apos;t take account of the interval size.  They have some validity, but so does the prime RMS, so neither would be dignified by comparison with the other.</p><p>                    Graham</p></div><h3><a id=13212 href="#13212">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/6/2005 2:07:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; I only need to look at the primes, then?  I don&apos;t have the commas at<br/>&gt; this stage, but writing a formula for the mistuning of each prime in<br/>&gt; terms of the generator and period is easy.</p><p>It&apos;s as I said--TOP tuning is not that difficult to compute.</p></div><h3><a id=13223 href="#13223">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/8/2005 12:19:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Paul Erlich wrote:<br/>&gt;<br/>&gt; &gt; &quot;Odd limit&quot; can be interpreted in several ways, including equal-<br/>&gt; &gt; weighted Hahn, weighted Hahn, and Kees . . . but yes, these all<br/>serve<br/>&gt; &gt; as examples of &quot;other methods&quot;.<br/>&gt;<br/>&gt; I take &quot;odd limit&quot; to be a set of intervals.  It&apos;s a good<br/>approximation<br/>&gt; of complexity, but fails in comparison to the Tenney limit.</p><p>It seems like you&apos;re using both definitions of odd limit above, one<br/>right after the other. How can a set of intervals be an approximation<br/>of complexity?</p><p>&gt; &gt; You said, &quot;It applies to any method that uses Tenney weighting<br/>with a<br/>&gt; &gt; prime limit.&quot; So how can it serve for a comparison between two<br/>&gt; &gt; different such methods?<br/>&gt;<br/>&gt; It doesn&apos;t.  This thread is about TOP, not comparing TOP with one<br/>other<br/>&gt; method.</p><p>OK . . . (I forgot what that was about anyway).</p><p>&gt; &gt; You can observe that, but more importantly, you can observe the<br/>&gt; &gt; differences between different tunings of the same temperament.<br/>And<br/>&gt; &gt; the differences sound qualitively greater, for a given cents<br/>&gt; &gt; difference in an interval, for the simpler ratios than for the<br/>more<br/>&gt; &gt; complex ratios.<br/>&gt;<br/>&gt; I don&apos;t think the listeners can hear these differences.  Even<br/>relative<br/>&gt; to JI, how many piano players know what a 5:4 sound like?</p><p>The point is that a given cents difference in the major third will<br/>have less of an aural effect than a given cents difference in the<br/>perfect fifth. Are you saying that no differences will be heard in<br/>any case, regardless of how many cents and which intervals? Or . . .<br/>(?)</p><p>&gt; &gt; In terms of the relative dissonance of two tunings of a given<br/>&gt; &gt; temperament, I stand by my statement above -- a simpler interval<br/>&gt; &gt; differing by a given amount in cents between the two tunings will<br/>&gt; &gt; have more impact on the relative dissonance of the two than a<br/>more<br/>&gt; &gt; complex interval differing by the same amount (ceteris parebis).<br/>&gt;<br/>&gt; You&apos;re talking about &quot;impact&quot; there, which suggests averaging.</p><p>Not necessarily. We could be considering each of these intervals in<br/>isolation.</p><p>&gt; A<br/>&gt; minimax isn&apos;t about measuring the total impact,</p><p>No, it&apos;s about measuring the maximum impact.</p><p>And if &quot;total impact&quot; is supposed to be a linear thing that one can<br/>apply with respect to JI as well as with respect to some other<br/>temperament, neither is any average of errors as far as I can see.</p><p>&gt; but whether one interval<br/>&gt; has enough force to break the chord.</p><p>You shouldn&apos;t assume that there are necessarily any many-interval<br/>chords in the music.</p><p>&gt; &gt;&gt;Oh, then I bow to you superior knowledge of statistics.  The RMS<br/>&gt; &gt; will<br/>&gt; &gt;&gt;give a better average because all intervals have some weight.<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; Is the logical conclusion of this that sum-absolute error gives a<br/>&gt; &gt; still better average?<br/>&gt;<br/>&gt; No, but that some power will give the best approximation to how the<br/>ear<br/>&gt; does the averaging, however that might be.</p><p>Might this not depend entirely or almost entirely on context and<br/>other &quot;accidents&quot;?</p><p>&gt; &gt;&gt;TOP will do fine as an approximation to the RMS, or an extreme<br/>kind<br/>&gt; &gt; of<br/>&gt; &gt;&gt;mean.  The only problem is that it&apos;s more complicated.  If it<br/>were<br/>&gt; &gt;&gt;simpler, I&apos;d argue for it.<br/>&gt; &gt;<br/>&gt; &gt; To me, it&apos;s simpler. Simpler definition, simpler motivation, and<br/>I<br/>&gt; &gt; don&apos;t think the calculation is significantly more complicated.<br/>&gt;<br/>&gt; The definitions are much the same either way.  (Well, you can take<br/>the<br/>&gt; &quot;R&quot; out of &quot;RMS&quot;).  I don&apos;t now about motivation.  But the<br/>calculation<br/>&gt; is surely more complicated for TOP.  How are you calculating it?<br/>&gt; Everything I&apos;ve seen so far is numerical optimization, but the<br/>least<br/>&gt; squares is algebraic.  And only 14 lines of simple code with no<br/>libraries.</p><p>I bet the linear programming involved here can be optimized down to a<br/>similarly short piece of code.</p><p>&gt; &gt;&gt;As it is, I still have to spend time arguing<br/>&gt; &gt;&gt;  that the RMS is simpler.  I also happen to think that the<br/>&gt; &gt;&gt;prime-weighted RMS is better anyway but that&apos;s not so important.<br/>&gt; &gt;<br/>&gt; &gt; I, on the other hand, would like to see that pursued further.<br/>&gt;<br/>&gt; Do you agree that weighting is more appropriate for the<br/>&gt; octave-equivalent case?</p><p>More appropriate than what? What kind of weighting? The Kees or<br/>inverse-log-of-minimum-odd-limit weighting seems most appropriate to<br/>me . . .</p><p>&gt; Or, at least, that an unweighted,<br/>&gt; octave-equivalent measure doesn&apos;t make sense?</p><p>I thought you were arguing that it *does* make sense in contexts<br/>like, for example, adaptive JI where a given retuning motion will be<br/>equally &quot;painful&quot; regardless of which interval it &quot;corrects&quot;. So have<br/>you now changed your mind?</p><p>&gt; This is all part of the<br/>&gt; same revelation to me.</p><p>Please elaborate!</p><p>&gt; &gt;&gt;If you allow dissonant but not completely random intervals in<br/>&gt; &gt; chords,<br/>&gt; &gt;&gt;you care a bit how close they are to JI.  You don&apos;t care very<br/>much<br/>&gt; &gt;&gt;because the good intervals carry most of the consonance.  The<br/>&gt; &gt; weighting<br/>&gt; &gt;&gt;tells you how much you care.  This is only valid for chords that<br/>&gt; &gt; are<br/>&gt; &gt;&gt;musically dissonant, in particular where the dissonance is for<br/>&gt; &gt;&gt;coloration so that consonance still matters.<br/>&gt; &gt;<br/>&gt; &gt; OK; now how does this fit into our conversation?<br/>&gt;<br/>&gt; For these kind of chords, we want to know the average dissonance.<br/>Or at<br/>&gt; least the average relative to the ideal tuning.  An average of<br/>weighted<br/>&gt; errors relative to JI is a good guess for that.</p><p>How wrong can you go with maximum weighted error? Because with Tenney<br/>weighting, there&apos;s no way to create a chord where a complex interval<br/>has greater weighted error than any of the simple intervals of which<br/>it is constructed.</p><p>&gt; &gt; I tend to think of chords as smallish blobs in the lattice. This<br/>is<br/>&gt; &gt; very different from thinking of them as similar to the tuning, or<br/>the<br/>&gt; &gt; whole lattice. So I&apos;m still not seeing the justification for the<br/>&gt; &gt; leap from one to the other in your reasoning.<br/>&gt;<br/>&gt; I think of chords as more than one note sounding together.</p><p>Still.</p><p>&gt; &gt; I&apos;d like to see that worked out, especially as regards<br/>definition,<br/>&gt; &gt; for the case of weighted prime RMS. But I believe there&apos;s part of<br/>a<br/>&gt; &gt; recent message from you that I still need to really read and<br/>respond<br/>&gt; &gt; to -- please don&apos;t let me forget!<br/>&gt;<br/>&gt; I don&apos;t have any correlation between the WPRMS and any other<br/>measure.  I<br/>&gt; had a look, but the math isn&apos;t as clean as the TOP :(</p><p>That&apos;s what I suspected. WPRMS (I thought you had a different,<br/>slightly longer acronym for it) may be wonderful, but its<br/>specification with respect to some list of intervals we care about<br/>ends up being so abstract that I don&apos;t think I could go with it for<br/>an expository paper.</p><p>&gt; &gt; OK, I think I see what you mean. But this doesn&apos;t argue against<br/>&gt; &gt; weighting the errors in any way I can see.<br/>&gt;<br/>&gt; You can weight the errors as long as the weighted error itself has<br/>a<br/>&gt; meaning.  That isn&apos;t something you can argue over.  But once you<br/>apply a<br/>&gt; minimax, only the worst interval (however weighted the error)<br/>matters.</p><p>Right, where &quot;worst&quot; means &quot;biggest weight*error&quot; and not &quot;biggest<br/>error in cents&quot;.</p><p>&gt; &gt;&gt;The implication is that it&apos;s a guess as to what the Tenney<br/>weighted<br/>&gt; &gt;&gt;average<br/>&gt; &gt;<br/>&gt; &gt; A straight average? Or RMS? Or . . . (?)<br/>&gt;<br/>&gt; Any average.  I use the word deliberately vaguely.  Ideally it will<br/>be<br/>&gt; the one closest to what we hear.  In practice, I expect an RMS will<br/>&gt; approximate an RMS best.  You can think of TOP as an average, and<br/>that&apos;s<br/>&gt; exact.</p><p>Whew!</p><p>&gt; &gt;&gt;mistuning of any given set of intervals (within the prime limit)<br/>&gt; &gt;&gt;will be.  If I trusted any way of selecting octave-specific<br/>&gt; &gt; intervals, I<br/>&gt; &gt;&gt;could do a comparison.<br/>&gt; &gt;<br/>&gt; &gt; Two ways are to use some Tenney limit, and some integer limit.<br/>&gt;<br/>&gt; Simple, but as I said, I don&apos;t trust them.  They don&apos;t take account<br/>of<br/>&gt; the interval size.</p><p>Integer limit does -- it favors smaller intervals.</p><p>&gt; They have some validity, but so does the prime RMS,</p><p>Huh? How does that give you a different way of selecting octave-<br/>specific intervals than integer or product limit?</p><p>&gt; so neither would be dignified by comparison with the other.</p><p>It seems like there are at least two different things here -- the<br/>optimization criterion (which includes weights (such as Tenney) and<br/>an exponent (such as 2)), and the way of selecting which intervals to<br/>include. You can&apos;t compare these things, though you can certainly<br/>combine them in different ways, and compare the combinations . . .</p></div><h3><a id=13226 href="#13226">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/8/2005 3:33:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13228 href="#13228">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/8/2005 5:37:30 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; I bet the linear programming involved here can be optimized down to a<br/>&gt; similarly short piece of code.</p><p>For small problems like this, linear programming can be solved using<br/>brute force.</p></div><h3><a id=13239 href="#13239">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/9/2005 5:04:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;In terms of the relative dissonance of two tunings of a given<br/>&gt;temperament, I stand by my statement above -- a simpler interval<br/>&gt;differing by a given amount in cents between the two tunings will<br/>&gt;have more impact on the relative dissonance of the two than a more<br/>&gt;complex interval differing by the same amount (ceteris parebis).</p><p>I believe that&apos;s ceteris paribus.</p><p>-Carl</p></div><h3><a id=13240 href="#13240">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/9/2005 5:28:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; You can observe that, but more importantly, you can observe the<br/>&gt;&gt; differences between different tunings of the same temperament. And<br/>&gt;&gt; the differences sound qualitively greater, for a given cents<br/>&gt;&gt; difference in an interval, for the simpler ratios than for the more<br/>&gt;&gt; complex ratios.<br/>&gt;<br/>&gt;I don&apos;t think the listeners can hear these differences.  Even relative<br/>&gt;to JI, how many piano players know what a 5:4 sound like?</p><p>I certainly find mistuning of the octave more objectionable than<br/>mistuning of the 5:4.</p><p>But it&apos;s an age-old question... mistuning simpler intervals is<br/>more painful, but complex intervals require more accuracy of<br/>tuning to be &apos;evoked&apos;, since you&apos;re more likely to run into<br/>other consonances.  Though it amazes me how good 1000 cents is<br/>at doing 7:4.....</p><p>&gt;&gt;&gt;As it is, I still have to spend time arguing<br/>&gt;&gt;&gt;  that the RMS is simpler.  I also happen to think that the<br/>&gt;&gt;&gt;prime-weighted RMS is better anyway but that&apos;s not so important.<br/>&gt;&gt;<br/>&gt;&gt; I, on the other hand, would like to see that pursued further.<br/>&gt;<br/>&gt;Do you agree that weighting is more appropriate for the<br/>&gt;octave-equivalent case?</p><p>I don&apos;t.  Weighting is a great way to allow octaves to be<br/>tempered.</p><p>&gt;Or, at least, that an unweighted, octave-equivalent measure<br/>&gt;doesn&apos;t make sense?</p><p>Sorry for barging in, but make sense for what?  Harmonic<br/>complexity?  I don&apos;t think any octave-equivalent measure is<br/>very good.  But certainly an unweighted-factors approach<br/>will fail.</p><p>-Carl</p></div><h3><a id=13242 href="#13242">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/9/2005 6:22:12 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;The point is that a given cents difference in the major third will<br/>&gt;&gt;have less of an aural effect than a given cents difference in the<br/>&gt;&gt;perfect fifth. Are you saying that no differences will be heard in<br/>&gt;&gt;any case, regardless of how many cents and which intervals? Or . . .<br/>&gt;&gt;(?)<br/>&gt;<br/>&gt;I&apos;m saying no differences will be heard in most cases.  Octaves will<br/>&gt;probably be heard as different from a pure octave because most people<br/>&gt;will know what a pure octave sounds like.</p><p>That goes for all intervals of the normal diatonic scale, and<br/>not necessarily their 12-tET versions.  But even when bizzare<br/>things like a Partch hexad appears, most people can tell it&apos;s<br/>smooth (or not).</p><p>&gt;No, what I did is get my &quot;octave-specific&quot; and &quot;octave-equivalent&quot;<br/>&gt;mixed up.  I knew I&apos;d do it eventually.  Weighting makes sense with<br/>&gt;octave specificity because 2 is such a small number it has to be<br/>&gt;treated differently from 7 and 11 (or however high you&apos;re going).</p><p>Ah!</p><p>&gt;&gt;It seems like there are at least two different things here -- the<br/>&gt;&gt;optimization criterion (which includes weights (such as Tenney) and<br/>&gt;&gt;an exponent (such as 2)), and the way of selecting which intervals to<br/>&gt;&gt;include. You can&apos;t compare these things, though you can certainly<br/>&gt;&gt;combine them in different ways, and compare the combinations . . .<br/>&gt;<br/>&gt;Yes, two things.  But the point is that the average of primes is some<br/>&gt;approximation to the average of real intervals.  You could compare<br/>&gt;the prime-limit RMS to the RMS of some specific set of intervals.  But<br/>&gt;if there&apos;s a correlation, it would only show the prime-limit RMS is<br/>&gt;valid if the RMS of a set of consonances has some validity in the first<br/>&gt;place, and that hasn&apos;t been established.</p><p>In my recent thread, I consider the prime factor basis of a comma<br/>the consonant chord of the tuning, and find ms deviation of them.</p><p>-Carl</p></div><h3><a id=13246 href="#13246">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/9/2005 8:52:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p>&gt; But it&apos;s an age-old question... mistuning simpler intervals is<br/>&gt; more painful, but complex intervals require more accuracy of<br/>&gt; tuning to be &apos;evoked&apos;, since you&apos;re more likely to run into<br/>&gt; other consonances.  Though it amazes me how good 1000 cents is<br/>&gt; at doing 7:4.....</p><p>It sounds fairly putrid, but it evokes pretty well. The two things<br/>seem to be different.</p></div><h3><a id=13250 href="#13250">ðŸ”—</a>oyarman@ozanyarman.com</h3><span>11/10/2005 4:41:01 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>And putrid is the word. Insipid also comes to mind.</p><p>----- Original Message -----<br/>From: &quot;Gene Ward Smith&quot; &lt;<a href="mailto:gwsmith@svpal.org">gwsmith@svpal.org</a>&gt;<br/>To: &lt;<a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>&gt;<br/>Sent: 10 Kas&iuml;&iquest;&half;m 2005 Per&iuml;&iquest;&half;embe 6:52<br/>Subject: [tuning-math] Re: TOP arguments</p><p>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt; But it&apos;s an age-old question... mistuning simpler intervals is<br/>&gt; &gt; more painful, but complex intervals require more accuracy of<br/>&gt; &gt; tuning to be &apos;evoked&apos;, since you&apos;re more likely to run into<br/>&gt; &gt; other consonances.  Though it amazes me how good 1000 cents is<br/>&gt; &gt; at doing 7:4.....<br/>&gt;<br/>&gt; It sounds fairly putrid, but it evokes pretty well. The two things<br/>&gt; seem to be different.<br/>&gt;<br/>&gt;<br/>&gt;</p></div><h3><a id=13252 href="#13252">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/10/2005 9:51:34 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; But it&apos;s an age-old question... mistuning simpler intervals is<br/>&gt;&gt; more painful, but complex intervals require more accuracy of<br/>&gt;&gt; tuning to be &apos;evoked&apos;, since you&apos;re more likely to run into<br/>&gt;&gt; other consonances.  Though it amazes me how good 1000 cents is<br/>&gt;&gt; at doing 7:4.....<br/>&gt;<br/>&gt;It sounds fairly putrid, but it evokes pretty well. The two things<br/>&gt;seem to be different.</p><p>Agree.  -Carl</p></div><h3><a id=13256 href="#13256">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/10/2005 1:54:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/9/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; &gt; It seems like you&apos;re using both definitions of odd limit above,<br/>one<br/>&gt; &gt; right after the other. How can a set of intervals be an<br/>approximation<br/>&gt; &gt; of complexity?<br/>&gt;<br/>&gt;  Choosing an odd limit gives you an octave-equivalent set of<br/>intervals</p><p>OK.</p><p>&gt; bounded by complexity.</p><p>which is a function with only two values in this case?</p><p>&gt; &gt;The point is that a given cents difference in the major third will<br/>&gt; &gt;have less of an aural effect than a given cents difference in the<br/>&gt; &gt;perfect fifth. Are you saying that no differences will be heard in<br/>&gt; &gt;any case, regardless of how many cents and which intervals?<br/>Or . . .<br/>&gt; &gt;(?)<br/>&gt;  I&apos;m saying no differences will be heard in most cases.</p><p>Hmm . . .</p><p>&gt; Octaves will<br/>&gt; probably be heard as different from a pure octave because most<br/>people will<br/>&gt; know what a pure octave sounds like.</p><p>:)</p><p>&gt; &gt;&gt; but whether one interval<br/>&gt; &gt;&gt; has enough force to break the chord.<br/>&gt;<br/>&gt; &gt;You shouldn&apos;t assume that there are necessarily any many-interval<br/>&gt; &gt;chords in the music.<br/>&gt;  Why not? You certainly shouldn&apos;t assume there won&apos;t be.</p><p>OK, so should we develop a sum-abs-error version of TOP? Could we<br/>work out its implications the way we can for (minimax) TOP?</p><p>&gt; An all-purpose<br/>&gt; measure should take account of usual practice, which is many-<br/>interval<br/>&gt; chords.</p><p>How is that &quot;usual practice&quot;?</p><p>&gt; You could assume only one note at a time, in which case anything<br/>&gt; goes.<br/>&gt;<br/>&gt; &gt;&gt; No, but that some power will give the best approximation to how<br/>the<br/>&gt; ear<br/>&gt; &gt;&gt; does the averaging, however that might be.<br/>&gt;<br/>&gt; &gt;Might this not depend entirely or almost entirely on context and<br/>&gt; &gt;other &quot;accidents&quot;?<br/>&gt;  It could well do, and if we understood how we could do special-case<br/>&gt; optimizations. But as we don&apos;t know we have to make some good<br/>guesses that<br/>&gt; will work in a range of situations.</p><p>OK.</p><p>&gt; &gt; I bet the linear programming involved here can be optimized down<br/>to a<br/>&gt; &gt; similarly short piece of code.<br/>&gt;  Maybe, if it were horribly slow.</p><p>?</p><p>&gt; But least squares can be done numerically<br/>&gt; as well -- should be easier as the function&apos;s continually<br/>differentiable.<br/>&gt; Are you arguing that an algebraic solution isn&apos;t superior to a<br/>numerical<br/>&gt; one?</p><p>I could see both types of solutions as being *geometric*.</p><p>&gt; &gt;&gt; Do you agree that weighting is more appropriate for the<br/>&gt; &gt;&gt; octave-equivalent case?<br/>&gt;<br/>&gt; &gt;More appropriate than what? What kind of weighting? The Kees or<br/>&gt; &gt;inverse-log-of-minimum-odd-limit weighting seems most appropriate<br/>to<br/>&gt; &gt;me . . .<br/>&gt;<br/>&gt; &gt;&gt; Or, at least, that an unweighted,<br/>&gt; &gt;&gt; octave-equivalent measure doesn&apos;t make sense?<br/>&gt;<br/>&gt; &gt;I thought you were arguing that it *does* make sense in contexts<br/>&gt; &gt;like, for example, adaptive JI where a given retuning motion will<br/>be<br/>&gt; &gt;equally &quot;painful&quot; regardless of which interval it &quot;corrects&quot;. So<br/>have<br/>&gt; &gt;you now changed your mind?<br/>&gt;  No, what I did is get my &quot;octave-specific&quot; and &quot;octave-equivalent&quot;<br/>mixed<br/>&gt; up. I knew I&apos;d do it eventually. Weighting makes sense with octave<br/>&gt; specificity because 2 is such a small number it has to be treated<br/>&gt; differently from 7 and 11 (or however high you&apos;re going).</p><p>OK.</p><p>&gt; &gt;How wrong can you go with maximum weighted error? Because with<br/>Tenney<br/>&gt; &gt;weighting, there&apos;s no way to create a chord where a complex<br/>interval<br/>&gt; &gt;has greater weighted error than any of the simple intervals of<br/>which<br/>&gt; &gt;it is constructed.<br/>&gt;  Not far wrong, because it&apos;s going to be pretty close to the least<br/>squares.</p><p>:)</p><p>&gt; &gt;That&apos;s what I suspected. WPRMS (I thought you had a different,<br/>&gt; &gt;slightly longer acronym for it) may be wonderful, but its<br/>&gt; &gt;specification with respect to some list of intervals we care about<br/>&gt; &gt;ends up being so abstract that I don&apos;t think I could go with it for<br/>&gt; &gt;an expository paper.</p><p>&gt;  This is where I have a problem with your TOP propaganda. You&apos;re<br/>suggesting<br/>&gt; by implication that it isn&apos;t abstract in some way.</p><p>I think it&apos;s easier when we talk about a set of intervals we care<br/>about, the exact boundaries of what is and isn&apos;t in the set being<br/>flexible, and specify some function of the error in these intervals<br/>that is minimized in the tuning you care about. And yes, minimax is<br/>less abstract than RMS. But I don&apos;t suggest what you imply.</p><p>&gt;  &gt;&gt; You can weight the errors as long as the weighted error itself<br/>has<br/>&gt; a<br/>&gt; &gt;&gt; meaning. That isn&apos;t something you can argue over. But once you<br/>&gt; apply a<br/>&gt; &gt;&gt; minimax, only the worst interval (however weighted the error)<br/>&gt; matters.<br/>&gt;<br/>&gt; &gt;Right, where &quot;worst&quot; means &quot;biggest weight*error&quot; and not &quot;biggest<br/>&gt; &gt;error in cents&quot;.<br/>&gt;  I&apos;d say it should include the inherent dissonance of the JI<br/>interval being<br/>&gt; approximated in some way. But perhaps that&apos;s the &quot;weight&quot;.<br/>&gt;  &gt;Integer limit does -- it favors smaller intervals.<br/>&gt;  Oh, maybe. In comparison to the product limit at least. But to get<br/>11:6 you<br/>&gt; need 11:1 as well.</p><p>This is why I like things like TOP where you can justify it all using<br/>just the intervals within two octaves or one octave or whatever if<br/>you like . . .</p><p>&gt; &gt;&gt; They have some validity, but so does the prime RMS,<br/>&gt; &gt;<br/>&gt; &gt;Huh? How does that give you a different way of selecting octave-<br/>&gt; &gt;specific intervals than integer or product limit?<br/>&gt;  It means you don&apos;t have to (or only a prime limit).</p><p>I thought you didn&apos;t buy this idea of &quot;you don&apos;t have to&quot; . . .</p><p>&gt; &gt;&gt; so neither would be dignified by comparison with the other.<br/>&gt;<br/>&gt; &gt;It seems like there are at least two different things here -- the<br/>&gt; &gt;optimization criterion (which includes weights (such as Tenney) and<br/>&gt; &gt;an exponent (such as 2)), and the way of selecting which intervals<br/>to<br/>&gt; &gt;include. You can&apos;t compare these things, though you can certainly<br/>&gt; &gt;combine them in different ways, and compare the combinations . . .<br/>&gt;  Yes, two things. But the point is that the average of primes is<br/>some<br/>&gt; approximation to the average of real intervals. You could compare<br/>the<br/>&gt; prime-limit RMS to the RMS of some specific set of intervals. But<br/>if there&apos;s<br/>&gt; a correlation, it would only show the prime-limit RMS is valid if<br/>the RMS of<br/>&gt; a set of consonances has some validity in the first place, and that<br/>hasn&apos;t<br/>&gt; been established.</p><p>Hmm . . . but that&apos;s your favorite assumption, isn&apos;t it?</p></div><h3><a id=13261 href="#13261">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/10/2005 3:07:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13262 href="#13262">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/10/2005 3:29:41 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13266 href="#13266">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/10/2005 10:13:13 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;That goes for all intervals of the normal diatonic scale, and<br/>&gt;&gt;not necessarily their 12-tET versions.  But even when bizzare<br/>&gt;&gt;things like a Partch hexad appears, most people can tell it&apos;s<br/>&gt;&gt;smooth (or not).<br/>&gt;<br/>&gt;Yes, I&apos;m suggesting people can hear &quot;smooth&quot; or &quot;rough&quot; but not<br/>&gt;the deviation (weighted or otherwise) from an interval they don&apos;t<br/>&gt;know.</p><p>But doesn&apos;t roughness correspond to deviation fairly well?</p><p>-Carl</p></div><h3><a id=13267 href="#13267">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/10/2005 10:17:03 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; OK, so should we develop a sum-abs-error version of TOP? Could we<br/>&gt;&gt; work out its implications the way we can for (minimax) TOP?<br/>&gt;<br/>&gt;I don&apos;t think it will work.  The problem is that m:n and mn:1 will have<br/>&gt;different errors.  All you can predict from the errors in m and n is the<br/>&gt;maximum error in m:n and mn:1.  Hence a minimax has properties that a<br/>&gt;sum doesn&apos;t.</p><p>Wouldn&apos;t knowing the signs of the errors fix this?</p><p>-Carl</p></div><h3><a id=13276 href="#13276">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/12/2005 6:12:45 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13277 href="#13277">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/12/2005 6:19:37 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13278 href="#13278">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/12/2005 7:42:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;&gt; OK, so should we develop a sum-abs-error version of TOP? Could we<br/>&gt;&gt;&gt; work out its implications the way we can for (minimax) TOP?<br/>&gt;&gt;<br/>&gt;&gt;I don&apos;t think it will work.  The problem is that m:n and mn:1 will have<br/>&gt;&gt;different errors.  All you can predict from the errors in m and n is the<br/>&gt;&gt;maximum error in m:n and mn:1.  Hence a minimax has properties that a<br/>&gt;&gt;sum doesn&apos;t.<br/>&gt;<br/>&gt;Wouldn&apos;t knowing the signs of the errors fix this?<br/>&gt;<br/>&gt;Yes, but what do you do with them?</p><p>I may not have understood the context.</p><p>-Carl</p></div><h3><a id=13279 href="#13279">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/12/2005 7:46:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;But doesn&apos;t roughness correspond to deviation fairly well?<br/>&gt;<br/>&gt;With what weighting?  I&apos;m happy with absolute deviation relative<br/>&gt;to a small odd limit as a rough guess.</p><p>I don&apos;t think any weighting is necessary at all if you start<br/>with a consonant chord.  If you want to compare chords, each<br/>with a deviation, you need weighting.  I like the &apos;simple<br/>intervals get rougher faster&apos; approximation, though I&apos;m not<br/>sure Tenney weighting is ideal.</p><p>&gt;(Deviation from arbitrary<br/>&gt;complex intervals is meaningless.)  But the Tenney weighting<br/>&gt;would give a 1 cent mistuning of a simple interval as rougher<br/>&gt;than a 1 cent mistuning of a complex interval.  I don&apos;t like that<br/>&gt;at all.</p><p>It seems like the first and last sentences here are<br/>contradictory.</p><p>&gt;Whatever Tenney weighted deviation measures, it ain&apos;t<br/>&gt;roughness, and I don&apos;t think it&apos;s directly perceptible (unless<br/>&gt;perhaps you&apos;ve spent a long time on JI ear training).</p><p>Hm!  Tenney seems to work for JI dyads (many tests) and<br/>tetrads (&quot;tuning lab&quot; test).  I can&apos;t say I&apos;ve ever tried<br/>weighted deviations in the real world.</p><p>-Carl</p></div><h3><a id=13280 href="#13280">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/13/2005 12:10:20 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13281 href="#13281">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/13/2005 10:26:05 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;Hm!  Tenney seems to work for JI dyads (many tests) and<br/>&gt;&gt;tetrads (&quot;tuning lab&quot; test).  I can&apos;t say I&apos;ve ever tried<br/>&gt;&gt;weighted deviations in the real world.<br/>&gt;<br/>&gt;Seems to work for what property of JI dyads?</p><p>Concordance?</p><p>&gt;It&apos;s still what I go for when I want weighting.</p><p>Have you tried Gene&apos;s sqrt(p) suggestion?</p><p>-Carl</p></div><h3><a id=13282 href="#13282">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 12:26:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;In terms of the relative dissonance of two tunings of a given<br/>&gt; &gt;temperament, I stand by my statement above -- a simpler interval<br/>&gt; &gt;differing by a given amount in cents between the two tunings will<br/>&gt; &gt;have more impact on the relative dissonance of the two than a more<br/>&gt; &gt;complex interval differing by the same amount (ceteris parebis).<br/>&gt;<br/>&gt; I believe that&apos;s ceteris paribus.</p><p>Huh? I wrote &quot;ceteris paribus&quot; because in reality, other intervals<br/>would have to be different as well, but for this simplified statement,<br/>we assume that somehow, they&apos;re the same.</p></div><h3><a id=13283 href="#13283">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 12:28:59 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p>&gt; In my recent thread, I consider the prime factor basis of a comma<br/>&gt; the consonant chord of the tuning</p><p>I don&apos;t understand this and I certainly don&apos;t recall reading anything<br/>like this. How do you justify this? It seems dead wrong to me.</p><p>&gt;, and find ms deviation of them.</p><p>Them?</p></div><h3><a id=13284 href="#13284">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 12:35:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 12:26 PM 11/14/2005, you wrote:<br/>&gt;--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; &gt;In terms of the relative dissonance of two tunings of a given<br/>&gt;&gt; &gt;temperament, I stand by my statement above -- a simpler interval<br/>&gt;&gt; &gt;differing by a given amount in cents between the two tunings will<br/>&gt;&gt; &gt;have more impact on the relative dissonance of the two than a more<br/>&gt;&gt; &gt;complex interval differing by the same amount (ceteris parebis).<br/>&gt;&gt;<br/>&gt;&gt; I believe that&apos;s ceteris paribus.<br/>&gt;<br/>&gt;Huh? I wrote &quot;ceteris paribus&quot; because in reality, other intervals<br/>&gt;would have to be different as well, but for this simplified statement,<br/>&gt;we assume that somehow, they&apos;re the same.</p><p>You wrote &quot;parebis&quot;.  :)</p><p>-Carl</p></div><h3><a id=13285 href="#13285">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 12:36:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; Carl wrote:<br/>&gt; &gt;That goes for all intervals of the normal diatonic scale, and<br/>&gt; &gt;not necessarily their 12-tET versions. But even when bizzare<br/>&gt; &gt;things like a Partch hexad appears, most people can tell it&apos;s<br/>&gt; &gt;smooth (or not).<br/>&gt;  Yes, I&apos;m suggesting people can hear &quot;smooth&quot; or &quot;rough&quot; but not the<br/>&gt; deviation (weighted or otherwise) from an interval they don&apos;t know.<br/>&gt;    Graham</p><p>How about loud, intense beating?</p></div><h3><a id=13286 href="#13286">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 12:44:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/11/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Choosing an odd limit gives you an octave-equivalent set of<br/>&gt; &gt; intervals<br/>&gt; &gt;<br/>&gt; &gt; OK.<br/>&gt; &gt;<br/>&gt; &gt; &gt; bounded by complexity.<br/>&gt; &gt;<br/>&gt; &gt; which is a function with only two values in this case?<br/>&gt;<br/>&gt;  It&apos;s a circular definition really, in that there&apos;s no independent<br/>measure<br/>&gt; of octave-equivalent complexity to compare it with.</p><p>Why not? You can use expressibility, in which case any odd limit is<br/>just a certain bound on it.</p><p>&gt; But odd limits give<br/>&gt; roughly the same hierarchy as a Tenney metric once you accept octave<br/>&gt; equivalence. At least it&apos;s a better indication of complexity than a<br/>prime<br/>&gt; limit.</p><p>Prime limit is not an indication of complexity, since any prime limit<br/>always allows for intervals whose complexity approaches infinity.<br/>However, for characterizing infinite JI lattices, prime limit is<br/>preferable to odd limit for several reasons.</p><p>&gt; &gt; &gt;&gt; but whether one interval<br/>&gt; &gt; &gt;&gt; has enough force to break the chord.<br/>&gt; &gt;<br/>&gt; &gt; &gt;You shouldn&apos;t assume that there are necessarily any many-interval<br/>&gt; &gt; &gt;chords in the music.<br/>&gt; &gt; Why not? You certainly shouldn&apos;t assume there won&apos;t be.<br/>&gt;<br/>&gt; &gt; OK, so should we develop a sum-abs-error version of TOP? Could we<br/>&gt; &gt; work out its implications the way we can for (minimax) TOP?</p><p>&gt;  I don&apos;t think it will work. The problem is that m:n and mn:1 will<br/>have<br/>&gt; different errors. All you can predict from the errors in m and n is<br/>the<br/>&gt; maximum error in m:n and mn:1. Hence a minimax has properties that<br/>a sum<br/>&gt; doesn&apos;t. Perhaps if you know enough statistics you can prove a<br/>correlation.<br/>&gt;<br/>&gt; &gt;&gt; An all-purpose<br/>&gt; &gt;&gt; measure should take account of usual practice, which is many-<br/>&gt; interval<br/>&gt; &gt;&gt; chords.<br/>&gt;<br/>&gt; &gt; How is that &quot;usual practice&quot;?<br/>&gt;  Most music I hear has more than two instruments at once if it has<br/>more than<br/>&gt; one instrument at once.<br/>&gt;<br/>&gt;<br/>&gt; &gt; &gt; &gt; I bet the linear programming involved here can be optimized<br/>down<br/>&gt; to a<br/>&gt; &gt; &gt; &gt; similarly short piece of code.<br/>&gt; &gt; &gt; Maybe, if it were horribly slow.<br/>&gt;<br/>&gt; &gt; ?<br/>&gt;  You can do for loops over all possible values for the period and<br/>generator,<br/>&gt; to within some resolution, and you&apos;ll find the right answer but<br/>it&apos;ll take a<br/>&gt; long time.</p><p>Of course that&apos;s not what I had in mond.</p><p>&gt; Any more complicated numerical approach will use more code than<br/>&gt; the least squares and still be slower to run.<br/>&gt;  Anyway, I have code that uses the simplex library now.</p><p>Good! This is far simpler and far faster than what you outlined above.</p><p>&gt; I&apos;m wondering if<br/>&gt; there are any cases where the minimax doesn&apos;t give a unique<br/>solution and how<br/>&gt; I&apos;d deal with them.</p><p>Yes, there&apos;s no unique TOP tuning for Blackwood, for example, as you<br/>know. The convention in my paper is to leave pure the primes that<br/>have some flexibility in their tuning under the TOP criterion.</p><p>&gt; &gt;&gt; But least squares can be done numerically<br/>&gt; &gt;&gt; as well -- should be easier as the function&apos;s continually<br/>&gt; differentiable.<br/>&gt; &gt;&gt; Are you arguing that an algebraic solution isn&apos;t superior to a<br/>&gt; numerical<br/>&gt; &gt;&gt; one?<br/>&gt;<br/>&gt; &gt;I could see both types of solutions as being *geometric*.<br/>&gt;  Meaning what?</p><p>They can be pictured in a geometric diagram.</p><p>&gt;I&apos;m using the definition that algebraic solutions give exact<br/>&gt; answers but numeric solutions give successive approximations.</p><p>Well, there&apos;s no need for successive approximations when doing a<br/>simplex algorithm.</p><p>&gt; At least, I<br/>&gt; thought I was, but you can get an exact solution for a piecewise<br/>linear<br/>&gt; graph I suppose. But you need to iterate to find it</p><p>Iterate what? Just search, not iterate, I&apos;d say.</p><p>&gt; whereas you can write<br/>&gt; down the least squares solution in all cases.<br/>&gt;<br/>&gt; &gt;I think it&apos;s easier when we talk about a set of intervals we care<br/>&gt; &gt;about, the exact boundaries of what is and isn&apos;t in the set being<br/>&gt; &gt;flexible, and specify some function of the error in these intervals<br/>&gt; &gt;that is minimized in the tuning you care about. And yes, minimax is<br/>&gt; &gt;less abstract than RMS. But I don&apos;t suggest what you imply.</p><p>&gt;  A fuzzy set, then? In many cases, there should be intervals with a<br/>higher<br/>&gt; prime limit in there as well.</p><p>There should?</p><p>&gt; &gt;&gt; &gt;&gt; They have some validity, but so does the prime RMS,<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Huh? How does that give you a different way of selecting octave-<br/>&gt; &gt;&gt; &gt;specific intervals than integer or product limit?<br/>&gt; &gt;&gt; It means you don&apos;t have to (or only a prime limit).<br/>&gt; &gt;<br/>&gt; &gt; I thought you didn&apos;t buy this idea of &quot;you don&apos;t have to&quot; . . .<br/>&gt;  Choosing a prime limit is more flexible than choosing the exact<br/>intervals.<br/>&gt; It&apos;d be nice if we didn&apos;t have to choose the prime limit either,<br/>but I don&apos;t<br/>&gt; think I&apos;d like the results. So I choose the prime limit as a<br/>compromise.</p><p>:)</p><p>&gt; &gt;&gt; a correlation, it would only show the prime-limit RMS is valid if<br/>&gt; the RMS of<br/>&gt; &gt;&gt; a set of consonances has some validity in the first place, and<br/>that<br/>&gt; hasn&apos;t<br/>&gt; &gt;&gt; been established.<br/>&gt;<br/>&gt; &gt; Hmm . . . but that&apos;s your favorite assumption, isn&apos;t it?<br/>&gt;<br/>&gt; Yes, none of this has been established, that&apos;s why I look for<br/>simplicity<br/>&gt; first.</p><p>OK -- I guess there are different possible notions of simplicity!</p><p>-P</p></div><h3><a id=13288 href="#13288">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 1:00:27 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; In my recent thread, I consider the prime factor basis of a comma<br/>&gt;&gt; the consonant chord of the tuning<br/>&gt;<br/>&gt; I don&apos;t understand this and I certainly don&apos;t recall reading anything<br/>&gt; like this.</p><p>You just called it a fair assumption!</p><p>&gt; How do you justify this? It seems dead wrong to me.<br/>&gt;<br/>&gt;&gt; and find ms deviation of them.<br/>&gt;<br/>&gt;Them?</p><p>Example<br/>81:80 - 2, 3, 5 are consonant.</p><p>-Carl</p></div><h3><a id=13294 href="#13294">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:26:01 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt;That goes for all intervals of the normal diatonic scale, and<br/>&gt; &gt;&gt;not necessarily their 12-tET versions.  But even when bizzare<br/>&gt; &gt;&gt;things like a Partch hexad appears, most people can tell it&apos;s<br/>&gt; &gt;&gt;smooth (or not).<br/>&gt; &gt;<br/>&gt; &gt;Yes, I&apos;m suggesting people can hear &quot;smooth&quot; or &quot;rough&quot; but not<br/>&gt; &gt;the deviation (weighted or otherwise) from an interval they don&apos;t<br/>&gt; &gt;know.<br/>&gt;<br/>&gt; But doesn&apos;t roughness correspond to deviation fairly well?</p><p>I think Graham&apos;s point is that since the starting intervals have<br/>different roughnesses to begin with, deviation can&apos;t capture roughness.</p></div><h3><a id=13296 href="#13296">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 1:33:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt;That goes for all intervals of the normal diatonic scale, and<br/>&gt;&gt; &gt;&gt;not necessarily their 12-tET versions.  But even when bizzare<br/>&gt;&gt; &gt;&gt;things like a Partch hexad appears, most people can tell it&apos;s<br/>&gt;&gt; &gt;&gt;smooth (or not).<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Yes, I&apos;m suggesting people can hear &quot;smooth&quot; or &quot;rough&quot; but not<br/>&gt;&gt; &gt;the deviation (weighted or otherwise) from an interval they don&apos;t<br/>&gt;&gt; &gt;know.<br/>&gt;&gt;<br/>&gt;&gt; But doesn&apos;t roughness correspond to deviation fairly well?<br/>&gt;<br/>&gt;I think Graham&apos;s point is that since the starting intervals have<br/>&gt;different roughnesses to begin with, deviation can&apos;t capture<br/>&gt;roughness.</p><p>I&apos;ve always assumed the starting thing is a chord, and the<br/>rms deviation of its intervals is what should be minimized.<br/>With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>what you&apos;ve been discussing.</p><p>-Carl</p></div><h3><a id=13299 href="#13299">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:39:48 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/11/05, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; &gt;&gt; OK, so should we develop a sum-abs-error version of TOP? Could<br/>we<br/>&gt; &gt; &gt;&gt; work out its implications the way we can for (minimax) TOP?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt;I don&apos;t think it will work. The problem is that m:n and mn:1<br/>will have<br/>&gt; &gt; &gt;different errors. All you can predict from the errors in m and n<br/>is the<br/>&gt; &gt; &gt;maximum error in m:n and mn:1. Hence a minimax has properties<br/>that a<br/>&gt; &gt; &gt;sum doesn&apos;t.<br/>&gt; &gt;<br/>&gt; &gt; Wouldn&apos;t knowing the signs of the errors fix this?<br/>&gt;<br/>&gt;  Yes, but what do you do with them? The RMS has the advantage of<br/>being<br/>&gt; simple. The optimum happens to be the same as the standard deviation<br/>&gt; ignoring the stretch.</p><p>What does that mean? The optimum the same as the standard deviation??</p></div><h3><a id=13300 href="#13300">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:45:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/11/05, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; But doesn&apos;t roughness correspond to deviation fairly well?<br/>&gt;<br/>&gt;  With what weighting? I&apos;m happy with absolute deviation relative to<br/>a small<br/>&gt; odd limit as a rough guess.</p><p>As a rough guess for roughness? You&apos;d say that an interval 1 cent<br/>from 2:1 is just as rough as an interval 1 cent from 8:5??</p><p>&gt; (Deviation from arbitrary complex intervals is<br/>&gt; meaningless.) But the Tenney weighting would give a 1 cent<br/>mistuning of a<br/>&gt; simple interval as rougher than a 1 cent mistuning of a complex<br/>interval. I<br/>&gt; don&apos;t like that at all.</p><p>Good, because it doesn&apos;t say that. But which of the measures we&apos;ve<br/>considered would work better when forced into this interpretation?</p><p>&gt;Whatever Tenney weighted deviation measures, it<br/>&gt; ain&apos;t roughness, and I don&apos;t think it&apos;s directly perceptible<br/>(unless perhaps<br/>&gt; you&apos;ve spent a long time on JI ear training).<br/>&gt;    Graham</p><p>The same seems to go for any of the deviation measures.</p><p>What you&apos;re failing to consider is that when comparing tunings, the<br/>*absolute* roughnesses of JI intervals (which are different) can fall<br/>out in the wash, and you can end up with the deviation comparisons<br/>being equivalent to roughness comparisons.</p></div><h3><a id=13301 href="#13301">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:48:11 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p>&gt; &gt;Whatever Tenney weighted deviation measures, it ain&apos;t<br/>&gt; &gt;roughness, and I don&apos;t think it&apos;s directly perceptible (unless<br/>&gt; &gt;perhaps you&apos;ve spent a long time on JI ear training).<br/>&gt;<br/>&gt; Hm!  Tenney seems to work for JI dyads (many tests) and<br/>&gt; tetrads (&quot;tuning lab&quot; test).</p><p>I think you&apos;re talking about Tenney complexity, while the relevant<br/>quantity here would be Tenney-weighted error. Completely different<br/>animals/scenarios.</p><p>&gt; I can&apos;t say I&apos;ve ever tried<br/>&gt; weighted deviations in the real world.</p><p>How about *any* deviations?</p></div><h3><a id=13302 href="#13302">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:49:12 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/13/05, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;<br/>&gt; &gt; &gt;(Deviation from arbitrary<br/>&gt; &gt; &gt;complex intervals is meaningless.) But the Tenney weighting<br/>&gt; &gt; &gt;would give a 1 cent mistuning of a simple interval as rougher<br/>&gt; &gt; &gt;than a 1 cent mistuning of a complex interval. I don&apos;t like that<br/>&gt; &gt; &gt;at all.<br/>&gt; &gt;<br/>&gt; &gt; It seems like the first and last sentences here are<br/>&gt; &gt; contradictory.<br/>&gt;<br/>&gt;  For the sake of simplicity, you can set a rule for all intervals<br/>and not<br/>&gt; worry if it also includes some meaningless ones. The weight on the<br/>overly<br/>&gt; complex intervals should work out as being negligible. That isn&apos;t<br/>quite true<br/>&gt; with Tenney weighting,</p><p>Why not?</p></div><h3><a id=13303 href="#13303">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:50:02 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt;Hm!  Tenney seems to work for JI dyads (many tests) and<br/>&gt; &gt;&gt;tetrads (&quot;tuning lab&quot; test).  I can&apos;t say I&apos;ve ever tried<br/>&gt; &gt;&gt;weighted deviations in the real world.<br/>&gt; &gt;<br/>&gt; &gt;Seems to work for what property of JI dyads?<br/>&gt;<br/>&gt; Concordance?</p><p>What was at issue here was how to measure/weight *errors* or mistunings.</p></div><h3><a id=13304 href="#13304">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 1:51:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; In my recent thread, I consider the prime factor basis of a comma<br/>&gt; &gt;&gt; the consonant chord of the tuning<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t understand this and I certainly don&apos;t recall reading<br/>anything<br/>&gt; &gt; like this.<br/>&gt;<br/>&gt; You just called it a fair assumption!</p><p>Absolutely not. I said it&apos;s a fair assumption that each of the prime<br/>factors of the comma will be consonances. That&apos;s all.</p><p>&gt; &gt; How do you justify this? It seems dead wrong to me.<br/>&gt; &gt;<br/>&gt; &gt;&gt; and find ms deviation of them.<br/>&gt; &gt;<br/>&gt; &gt;Them?<br/>&gt;<br/>&gt; Example<br/>&gt; 81:80 - 2, 3, 5 are consonant.</p><p>But not the only consonances, right? For example, 4 is consonant too,<br/>right?</p></div><h3><a id=13308 href="#13308">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 2:00:14 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt;That goes for all intervals of the normal diatonic scale, and<br/>&gt; &gt;&gt; &gt;&gt;not necessarily their 12-tET versions.  But even when bizzare<br/>&gt; &gt;&gt; &gt;&gt;things like a Partch hexad appears, most people can tell it&apos;s<br/>&gt; &gt;&gt; &gt;&gt;smooth (or not).<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Yes, I&apos;m suggesting people can hear &quot;smooth&quot; or &quot;rough&quot; but not<br/>&gt; &gt;&gt; &gt;the deviation (weighted or otherwise) from an interval they<br/>don&apos;t<br/>&gt; &gt;&gt; &gt;know.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; But doesn&apos;t roughness correspond to deviation fairly well?<br/>&gt; &gt;<br/>&gt; &gt;I think Graham&apos;s point is that since the starting intervals have<br/>&gt; &gt;different roughnesses to begin with, deviation can&apos;t capture<br/>&gt; &gt;roughness.<br/>&gt;<br/>&gt; I&apos;ve always assumed the starting thing is a chord, and the<br/>&gt; rms deviation of its intervals is what should be minimized.<br/>&gt; With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>&gt; what you&apos;ve been discussing.</p><p>In some of the other schemes it&apos;s also not the case. For his 9-limit<br/>and higher-odd-limit optimizations, Gene counts intervals like 3:1<br/>only once even though they occur multiple times in the &quot;complete&quot;<br/>chord -- it&apos;s impossible to construct a chord with each interval<br/>occuring only once. I&apos;d rather avoid assuming this (chord thing) for<br/>*any* of the schemes -- even in the simplest equal-weighted 5-limit<br/>case, is it a major or minor chord? -- and then one can show later<br/>for some of the schemes that starting with a chord would in a sense<br/>be equivalent.</p></div><h3><a id=13309 href="#13309">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 2:02:11 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;Whatever Tenney weighted deviation measures, it ain&apos;t<br/>&gt;&gt; &gt;roughness, and I don&apos;t think it&apos;s directly perceptible (unless<br/>&gt;&gt; &gt;perhaps you&apos;ve spent a long time on JI ear training).<br/>&gt;&gt;<br/>&gt;&gt; Hm!  Tenney seems to work for JI dyads (many tests) and<br/>&gt;&gt; tetrads (&quot;tuning lab&quot; test).<br/>&gt;<br/>&gt;I think you&apos;re talking about Tenney complexity, while the relevant<br/>&gt;quantity here would be Tenney-weighted error. Completely different<br/>&gt;animals/scenarios.</p><p>Funny that, since I seemed to recognize this below...</p><p>&gt;&gt; I can&apos;t say I&apos;ve ever tried weighted deviations in the real world.<br/>&gt;<br/>&gt;How about *any* deviations?</p><p>Sure.  Sum and RMS quite a bit.</p><p>-Carl</p></div><h3><a id=13311 href="#13311">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 2:06:30 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; In my recent thread, I consider the prime factor basis of a comma<br/>&gt;&gt; &gt;&gt; the consonant chord of the tuning<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt; I don&apos;t understand this and I certainly don&apos;t recall reading<br/>&gt;&gt; &gt; anything like this.<br/>&gt;&gt;<br/>&gt;&gt; You just called it a fair assumption!<br/>&gt;<br/>&gt;Absolutely not. I said it&apos;s a fair assumption that each of the prime<br/>&gt;factors of the comma will be consonances. That&apos;s all.</p><p>Ok, but the &quot;chord&quot; part doesn&apos;t make any difference in anything<br/>I was doing.  I understand it can make a big difference in<br/>optimizing tunings...</p><p>&gt;&gt; &gt; How do you justify this? It seems dead wrong to me.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;&gt; and find ms deviation of them.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Them?<br/>&gt;&gt;<br/>&gt;&gt; Example<br/>&gt;&gt; 81:80 - 2, 3, 5 are consonant.<br/>&gt;<br/>&gt;But not the only consonances, right? For example, 4 is consonant too,<br/>&gt;right?</p><p>Nope.  You have to use the active distance measure to get to it.</p><p>-Carl</p></div><h3><a id=13314 href="#13314">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 2:10:24 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; In my recent thread, I consider the prime factor basis of a<br/>comma<br/>&gt; &gt;&gt; &gt;&gt; the consonant chord of the tuning<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt; I don&apos;t understand this and I certainly don&apos;t recall reading<br/>&gt; &gt;&gt; &gt; anything like this.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; You just called it a fair assumption!<br/>&gt; &gt;<br/>&gt; &gt;Absolutely not. I said it&apos;s a fair assumption that each of the<br/>prime<br/>&gt; &gt;factors of the comma will be consonances. That&apos;s all.<br/>&gt;<br/>&gt; Ok, but the &quot;chord&quot; part doesn&apos;t make any difference in anything<br/>&gt; I was doing.  I understand it can make a big difference in<br/>&gt; optimizing tunings...</p><p>The &quot;chord&quot; part can? How so?</p><p>&gt; &gt;&gt; &gt; How do you justify this? It seems dead wrong to me.<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;&gt; and find ms deviation of them.<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Them?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Example<br/>&gt; &gt;&gt; 81:80 - 2, 3, 5 are consonant.<br/>&gt; &gt;<br/>&gt; &gt;But not the only consonances, right? For example, 4 is consonant<br/>too,<br/>&gt; &gt;right?<br/>&gt;<br/>&gt; Nope.  You have to use the active distance measure to get to it.</p><p>I don&apos;t get it. What active distance measure, how does it get to 4,<br/>and how does it not get to 2, 3, or 5?</p></div><h3><a id=13315 href="#13315">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 2:12:00 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; I&apos;ve always assumed the starting thing is a chord, and the<br/>&gt;&gt; rms deviation of its intervals is what should be minimized.<br/>&gt;&gt; With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>&gt;&gt; what you&apos;ve been discussing.<br/>&gt;<br/>&gt;In some of the other schemes it&apos;s also not the case. For his 9-limit<br/>&gt;and higher-odd-limit optimizations, Gene counts intervals like 3:1<br/>&gt;only once even though they occur multiple times in the &quot;complete&quot;<br/>&gt;chord -- it&apos;s impossible to construct a chord with each interval<br/>&gt;occuring only once. I&apos;d rather avoid assuming this (chord thing) for<br/>&gt;*any* of the schemes -- even in the simplest equal-weighted 5-limit<br/>&gt;case, is it a major or minor chord? -- and then one can show later<br/>&gt;for some of the schemes that starting with a chord would in a sense<br/>&gt;be equivalent.</p><p>Yes, that was a great appeal of TOP.  Bravo, I say!</p><p>-Carl</p></div><h3><a id=13316 href="#13316">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 2:16:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;&gt; &gt;&gt; In my recent thread, I consider the prime factor basis of<br/>&gt;&gt; &gt;&gt; &gt;&gt; a comma the consonant chord of the tuning<br/>&gt;&gt; &gt;&gt; &gt;<br/>&gt;&gt; &gt;&gt; &gt; I don&apos;t understand this and I certainly don&apos;t recall reading<br/>&gt;&gt; &gt;&gt; &gt; anything like this.<br/>&gt;&gt; &gt;&gt;<br/>&gt;&gt; &gt;&gt; You just called it a fair assumption!<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Absolutely not. I said it&apos;s a fair assumption that each of the<br/>&gt;&gt; &gt;prime factors of the comma will be consonances. That&apos;s all.<br/>&gt;&gt;<br/>&gt;&gt; Ok, but the &quot;chord&quot; part doesn&apos;t make any difference in anything<br/>&gt;&gt; I was doing.  I understand it can make a big difference in<br/>&gt;&gt; optimizing tunings...<br/>&gt;<br/>&gt;The &quot;chord&quot; part can? How so?</p><p>If the numbers are only expected to be individually consonant,<br/>you can measure their deviations.  If they&apos;re expected to be<br/>consonant with eachother, you have to consider &quot;all the intervals&quot;<br/>or use something like chordadic harmonic entropy.</p><p>&gt;&gt; &gt;&gt; Example<br/>&gt;&gt; &gt;&gt; 81:80 - 2, 3, 5 are consonant.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;But not the only consonances, right? For example, 4 is<br/>&gt;&gt; &gt;consonant too, right?<br/>&gt;&gt;<br/>&gt;&gt; Nope.  You have to use the active distance measure to get to it.<br/>&gt;<br/>&gt;I don&apos;t get it. What active distance measure, how does it get to 4,<br/>&gt;and how does it not get to 2, 3, or 5?</p><p>I assumed you were asking in the exploring badness thread context.<br/>There, I tried several different distance measures.  Since<br/>2 is in the comma, it&apos;s length 1, and 4 would be, say, length 2.</p><p>-Carl</p></div><h3><a id=13318 href="#13318">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 2:17:55 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; I&apos;ve always assumed the starting thing is a chord, and the<br/>&gt; &gt;&gt; rms deviation of its intervals is what should be minimized.<br/>&gt; &gt;&gt; With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>&gt; &gt;&gt; what you&apos;ve been discussing.<br/>&gt; &gt;<br/>&gt; &gt;In some of the other schemes it&apos;s also not the case. For his 9-<br/>limit<br/>&gt; &gt;and higher-odd-limit optimizations, Gene counts intervals like 3:1<br/>&gt; &gt;only once even though they occur multiple times in the &quot;complete&quot;<br/>&gt; &gt;chord -- it&apos;s impossible to construct a chord with each interval<br/>&gt; &gt;occuring only once. I&apos;d rather avoid assuming this (chord thing)<br/>for<br/>&gt; &gt;*any* of the schemes -- even in the simplest equal-weighted 5-<br/>limit<br/>&gt; &gt;case, is it a major or minor chord? -- and then one can show later<br/>&gt; &gt;for some of the schemes that starting with a chord would in a<br/>sense<br/>&gt; &gt;be equivalent.<br/>&gt;<br/>&gt; Yes, that was a great appeal of TOP.</p><p>Not at all what I was thinking.</p><p>&gt; Bravo, I say!</p><p>Thanks anyway!</p></div><h3><a id=13319 href="#13319">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 2:19:57 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; &gt;&gt; &gt;&gt; &gt;&gt; In my recent thread, I consider the prime factor basis of<br/>&gt; &gt;&gt; &gt;&gt; &gt;&gt; a comma the consonant chord of the tuning<br/>&gt; &gt;&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;&gt; &gt; I don&apos;t understand this and I certainly don&apos;t recall reading<br/>&gt; &gt;&gt; &gt;&gt; &gt; anything like this.<br/>&gt; &gt;&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; You just called it a fair assumption!<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Absolutely not. I said it&apos;s a fair assumption that each of the<br/>&gt; &gt;&gt; &gt;prime factors of the comma will be consonances. That&apos;s all.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Ok, but the &quot;chord&quot; part doesn&apos;t make any difference in anything<br/>&gt; &gt;&gt; I was doing.  I understand it can make a big difference in<br/>&gt; &gt;&gt; optimizing tunings...<br/>&gt; &gt;<br/>&gt; &gt;The &quot;chord&quot; part can? How so?<br/>&gt;<br/>&gt; If the numbers are only expected to be individually consonant,<br/>&gt; you can measure their deviations.  If they&apos;re expected to be<br/>&gt; consonant with eachother, you have to consider &quot;all the intervals&quot;<br/>&gt; or use something like chordadic harmonic entropy.</p><p>How can you do that? For optimizing tunings, there&apos;s no way to tune<br/>the intervals in a minor n-ad vs. a major n-ad differently, but<br/>clearly they&apos;re very different as regards chordal harmonic entropy.</p><p>&gt; &gt;&gt; &gt;&gt; Example<br/>&gt; &gt;&gt; &gt;&gt; 81:80 - 2, 3, 5 are consonant.<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;But not the only consonances, right? For example, 4 is<br/>&gt; &gt;&gt; &gt;consonant too, right?<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Nope.  You have to use the active distance measure to get to it.<br/>&gt; &gt;<br/>&gt; &gt;I don&apos;t get it. What active distance measure, how does it get to<br/>4,<br/>&gt; &gt;and how does it not get to 2, 3, or 5?<br/>&gt;<br/>&gt; I assumed you were asking in the exploring badness thread context.<br/>&gt; There, I tried several different distance measures.  Since<br/>&gt; 2 is in the comma, it&apos;s length 1, and 4 would be, say, length 2.</p><p>Oof.</p></div><h3><a id=13321 href="#13321">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/14/2005 2:33:33 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 02:17 PM 11/14/2005, you wrote:<br/>&gt;--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;&gt;<br/>&gt;&gt; &gt;&gt; I&apos;ve always assumed the starting thing is a chord, and the<br/>&gt;&gt; &gt;&gt; rms deviation of its intervals is what should be minimized.<br/>&gt;&gt; &gt;&gt; With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>&gt;&gt; &gt;&gt; what you&apos;ve been discussing.<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;In some of the other schemes it&apos;s also not the case. For his 9-<br/>&gt;limit<br/>&gt;&gt; &gt;and higher-odd-limit optimizations, Gene counts intervals like 3:1<br/>&gt;&gt; &gt;only once even though they occur multiple times in the &quot;complete&quot;<br/>&gt;&gt; &gt;chord -- it&apos;s impossible to construct a chord with each interval<br/>&gt;&gt; &gt;occuring only once. I&apos;d rather avoid assuming this (chord thing)<br/>&gt;&gt; &gt;for *any* of the schemes -- even in the simplest equal-weighted<br/>&gt;&gt; &gt;5-limit case, is it a major or minor chord? -- and then one can<br/>&gt;&gt; &gt;show later for some of the schemes that starting with a chord<br/>&gt;&gt; &gt;would in a sense be equivalent.<br/>&gt;&gt;<br/>&gt;&gt; Yes, that was a great appeal of TOP.<br/>&gt;<br/>&gt;Not at all what I was thinking.</p><p>What were you thinking?</p><p>-Carl</p></div><h3><a id=13330 href="#13330">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/14/2005 3:46:07 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt;<br/>&gt; At 02:17 PM 11/14/2005, you wrote:<br/>&gt; &gt;--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; &gt;&gt; I&apos;ve always assumed the starting thing is a chord, and the<br/>&gt; &gt;&gt; &gt;&gt; rms deviation of its intervals is what should be minimized.<br/>&gt; &gt;&gt; &gt;&gt; With TOP that&apos;s not the case, and I guess that&apos;s some of<br/>&gt; &gt;&gt; &gt;&gt; what you&apos;ve been discussing.<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;In some of the other schemes it&apos;s also not the case. For his 9-<br/>&gt; &gt;limit<br/>&gt; &gt;&gt; &gt;and higher-odd-limit optimizations, Gene counts intervals like<br/>3:1<br/>&gt; &gt;&gt; &gt;only once even though they occur multiple times in<br/>the &quot;complete&quot;<br/>&gt; &gt;&gt; &gt;chord -- it&apos;s impossible to construct a chord with each<br/>interval<br/>&gt; &gt;&gt; &gt;occuring only once. I&apos;d rather avoid assuming this (chord thing)<br/>&gt; &gt;&gt; &gt;for *any* of the schemes -- even in the simplest equal-weighted<br/>&gt; &gt;&gt; &gt;5-limit case, is it a major or minor chord? -- and then one can<br/>&gt; &gt;&gt; &gt;show later for some of the schemes that starting with a chord<br/>&gt; &gt;&gt; &gt;would in a sense be equivalent.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Yes, that was a great appeal of TOP.<br/>&gt; &gt;<br/>&gt; &gt;Not at all what I was thinking.<br/>&gt;<br/>&gt; What were you thinking?</p><p>Woolhouse, for example.</p></div><h3><a id=13344 href="#13344">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/14/2005 7:25:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13345 href="#13345">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/14/2005 7:28:12 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13346 href="#13346">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/14/2005 7:41:55 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/15/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;  It&apos;s a circular definition really, in that there&apos;s no independent<br/>&gt; measure<br/>&gt; &gt; of octave-equivalent complexity to compare it with.<br/>&gt;<br/>&gt; Why not? You can use expressibility, in which case any odd limit is<br/>&gt; just a certain bound on it.</p><p>I don&apos;t count that as &quot;independent&quot;.</p><p>&gt; Prime limit is not an indication of complexity, since any prime limit<br/>&gt; always allows for intervals whose complexity approaches infinity.<br/>&gt; However, for characterizing infinite JI lattices, prime limit is<br/>&gt; preferable to odd limit for several reasons.</p><p>Yes.  But we aren&apos;t talking about infinite JI lattices.  At least, I&apos;m not.</p><p>&gt; &gt; Any more complicated numerical approach will use more code than<br/>&gt; &gt; the least squares and still be slower to run.<br/>&gt; &gt;  Anyway, I have code that uses the simplex library now.<br/>&gt;<br/>&gt; Good! This is far simpler and far faster than what you outlined above.</p><p>Than what I outlined where?  It&apos;s got about 200 lines of code behind<br/>it.  It calls the function to be optimized several times and doesn&apos;t<br/>get to the exact solution.</p><p>&gt; Yes, there&apos;s no unique TOP tuning for Blackwood, for example, as you<br/>&gt; know. The convention in my paper is to leave pure the primes that<br/>&gt; have some flexibility in their tuning under the TOP criterion.</p><p>I didn&apos;t know that, but I&apos;m not surprised.  Anyway, the simple code<br/>using the simplex won&apos;t take any account of this.</p><p>&gt; Well, there&apos;s no need for successive approximations when doing a<br/>&gt; simplex algorithm.</p><p>There is for the one I&apos;ve been pointed towards.  If you know of a<br/>better one, show me!</p><p>&gt; &gt; At least, I<br/>&gt; &gt; thought I was, but you can get an exact solution for a piecewise<br/>&gt; linear<br/>&gt; &gt; graph I suppose. But you need to iterate to find it<br/>&gt;<br/>&gt; Iterate what? Just search, not iterate, I&apos;d say.</p><p>The way I do it for 1-D minimax, I start with two points and work out<br/>the point where the lines they&apos;re on join.  Then replace the one of<br/>the old points with the new one such that the solution lies in the<br/>middle.  And loop round until I get to the bottom.</p><p>How can you search without iterating?</p><p>&gt; &gt;  A fuzzy set, then? In many cases, there should be intervals with a<br/>&gt; higher<br/>&gt; &gt; prime limit in there as well.<br/>&gt;<br/>&gt; There should?</p><p>7-limit music in miracle is likely to hit neutral thirds, which will<br/>be heard as 11:9.  7-limit music in Orwell will use a load of 11-limit<br/>intervals as dissonances.</p><p>                           Graham</p></div><h3><a id=13348 href="#13348">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/14/2005 7:54:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[ Attachment content not displayed ]</p></div><h3><a id=13358 href="#13358">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/15/2005 11:34:24 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/15/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; As a rough guess for roughness? You&apos;d say that an interval 1 cent<br/>&gt; &gt; from 2:1 is just as rough as an interval 1 cent from 8:5??<br/>&gt;<br/>&gt;  No, but the question would never arise with an odd limit because<br/>octaves<br/>&gt; are taken out of the equation.</p><p>So substitute 3:2 for 2:1.</p><p>&gt; I said that octave equivalence, odd limits,<br/>&gt; minimax and absolute error all go together.</p><p>?</p><p>&gt; &gt; (Deviation from arbitrary complex intervals is<br/>&gt; &gt; &gt; meaningless.) But the Tenney weighting would give a 1 cent<br/>&gt; &gt; mistuning of a<br/>&gt; &gt; &gt; simple interval as rougher than a 1 cent mistuning of a complex<br/>&gt; &gt; interval. I<br/>&gt; &gt; &gt; don&apos;t like that at all.<br/>&gt; &gt;<br/>&gt; &gt; Good, because it doesn&apos;t say that. But which of the measures we&apos;ve<br/>&gt; &gt; considered would work better when forced into this interpretation?<br/>&gt;<br/>&gt;  Error as roughness works better.</p><p>Then JI has some error already, a contradiction in terms.</p><p>&gt; We haven&apos;t considered measures that do<br/>&gt; better then that, but I&apos;m sure they exist.<br/>&gt;<br/>&gt; &gt;Whatever Tenney weighted deviation measures, it<br/>&gt; &gt; &gt; ain&apos;t roughness, and I don&apos;t think it&apos;s directly perceptible<br/>&gt; &gt; (unless perhaps<br/>&gt; &gt; &gt; you&apos;ve spent a long time on JI ear training).<br/>&gt; &gt; &gt; Graham<br/>&gt; &gt;<br/>&gt; &gt; The same seems to go for any of the deviation measures.<br/>&gt;<br/>&gt;  The minimax absolute error relative to an odd limit at least puts<br/>some cap<br/>&gt; on the roughness.</p><p>Huh? How so? I don&apos;t see this at all.</p><p>&gt; &gt; What you&apos;re failing to consider is that when comparing tunings,<br/>the<br/>&gt; &gt; *absolute* roughnesses of JI intervals (which are different) can<br/>fall<br/>&gt; &gt; out in the wash, and you can end up with the deviation comparisons<br/>&gt; &gt; being equivalent to roughness comparisons.<br/>&gt;<br/>&gt;  You can say I fail to consider it. I still say it&apos;s exactly what I<br/>consider<br/>&gt; when I say that a minimax should be unweighted, and a mean weighted.</p><p>And a sum-abs should be ____? Sorry, I don&apos;t follow this at all. Am I<br/>trying your patience to ask you to explain this again?</p></div><h3><a id=13359 href="#13359">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/15/2005 11:35:51 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/15/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; What does that mean? The optimum the same as the standard<br/>deviation??<br/>&gt;<br/>&gt;  The optimum RMS error in the primes is approximately the same as the<br/>&gt; (optimum) standard deviation of the errors in the primes.</p><p>Wow -- so if all the primes had the same error, the standard deviation<br/>would be zero, and the RMS error would therefore have to be<br/>approximately zero?</p><p> It&apos;s explained in<br/>&gt; the bit you told me to remind you to look at.<br/>&gt;    Graham</p><p>Excellent. What post was that again?</p></div><h3><a id=13360 href="#13360">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/15/2005 11:46:54 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/15/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt;  It&apos;s a circular definition really, in that there&apos;s no<br/>independent<br/>&gt; &gt; measure<br/>&gt; &gt; &gt; of octave-equivalent complexity to compare it with.<br/>&gt; &gt;<br/>&gt; &gt; Why not? You can use expressibility, in which case any odd limit<br/>is<br/>&gt; &gt; just a certain bound on it.<br/>&gt;<br/>&gt; I don&apos;t count that as &quot;independent&quot;.</p><p>What would count?</p><p>&gt; &gt; Prime limit is not an indication of complexity, since any prime<br/>limit<br/>&gt; &gt; always allows for intervals whose complexity approaches infinity.<br/>&gt; &gt; However, for characterizing infinite JI lattices, prime limit is<br/>&gt; &gt; preferable to odd limit for several reasons.<br/>&gt;<br/>&gt; Yes.  But we aren&apos;t talking about infinite JI lattices.  At least,<br/>&gt;I&apos;m not.</p><p>I am. I see all tempered tunings as imposing some periodicity on, or<br/>rolling up, an infinite JI lattice.</p><p>&gt; &gt; &gt; Any more complicated numerical approach will use more code than<br/>&gt; &gt; &gt; the least squares and still be slower to run.<br/>&gt; &gt; &gt;  Anyway, I have code that uses the simplex library now.<br/>&gt; &gt;<br/>&gt; &gt; Good! This is far simpler and far faster than what you outlined<br/>above.<br/>&gt;<br/>&gt; Than what I outlined where?</p><p>You snipped it. You mentioned numerically searching all possible<br/>generators.</p><p>&gt; It&apos;s got about 200 lines of code behind<br/>&gt; it.  It calls the function to be optimized several times and doesn&apos;t<br/>&gt; get to the exact solution.</p><p>The simplex algorithm doesn&apos;t get to the exact solution? How could<br/>that be?? All it does is search the small number of corners, one of<br/>which is exactly at the exact solution.</p><p>&gt; &gt; Yes, there&apos;s no unique TOP tuning for Blackwood, for example, as<br/>you<br/>&gt; &gt; know. The convention in my paper is to leave pure the primes that<br/>&gt; &gt; have some flexibility in their tuning under the TOP criterion.<br/>&gt;<br/>&gt; I didn&apos;t know that, but I&apos;m not surprised.  Anyway, the simple code<br/>&gt; using the simplex won&apos;t take any account of this.<br/>&gt;<br/>&gt; &gt; Well, there&apos;s no need for successive approximations when doing a<br/>&gt; &gt; simplex algorithm.<br/>&gt;<br/>&gt; There is for the one I&apos;ve been pointed towards.</p><p>How bizarre. I don&apos;t see how that could be a simplex algorithm then.<br/>We&apos;re talking about simplex as in linear programming, right?</p><p>&gt; If you know of a<br/>&gt; better one, show me!</p><p>Gene has a method on his TOP page.</p><p>&gt; &gt; &gt; At least, I<br/>&gt; &gt; &gt; thought I was, but you can get an exact solution for a piecewise<br/>&gt; &gt; linear<br/>&gt; &gt; &gt; graph I suppose. But you need to iterate to find it<br/>&gt; &gt;<br/>&gt; &gt; Iterate what? Just search, not iterate, I&apos;d say.<br/>&gt;<br/>&gt; The way I do it for 1-D minimax, I start with two points and work<br/>out<br/>&gt; the point where the lines they&apos;re on join.  Then replace the one of<br/>&gt; the old points with the new one such that the solution lies in the<br/>&gt; middle.  And loop round until I get to the bottom.<br/>&gt;<br/>&gt; How can you search without iterating?</p><p>Just look at each of the corners. One of the corners is guaranteed to<br/>be exactly at the minimax solution.</p><p>&gt; &gt; &gt;  A fuzzy set, then? In many cases, there should be intervals<br/>with a<br/>&gt; &gt; higher<br/>&gt; &gt; &gt; prime limit in there as well.<br/>&gt; &gt;<br/>&gt; &gt; There should?<br/>&gt;<br/>&gt; 7-limit music in miracle is likely to hit neutral thirds, which will<br/>&gt; be heard as 11:9.</p><p>I don&apos;t know about that.</p><p>&gt; 7-limit music in Orwell will use a load of 11-limit<br/>&gt; intervals as dissonances.</p><p>If they&apos;re indeed being heard that way . . .</p></div><h3><a id=13382 href="#13382">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/15/2005 5:09:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/16/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; Wow -- so if all the primes had the same error, the standard deviation<br/>&gt; would be zero, and the RMS error would therefore have to be<br/>&gt; approximately zero?</p><p>If all the primes have the same error before you optimize, you can<br/>stretch the scale so they all have a very small error.  If all the<br/>primes have the same error after you optimize, that error must be<br/>zero.  This is taking account of the sign of the error (which the<br/>standard deviation uses but the RMS doesn&apos;t).</p><p>&gt;  It&apos;s explained in<br/>&gt; &gt; the bit you told me to remind you to look at.<br/>&gt; &gt;    Graham<br/>&gt;<br/>&gt; Excellent. What post was that again?</p><p>November 2nd</p><p>                     Graham</p></div><h3><a id=13383 href="#13383">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/15/2005 5:24:50 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/16/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt; &gt; &gt; Why not? You can use expressibility, in which case any odd limit<br/>&gt; is<br/>&gt; &gt; &gt; just a certain bound on it.<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t count that as &quot;independent&quot;.<br/>&gt;<br/>&gt; What would count?</p><p>An octave-equivalent harmonic entropy would count, I suppose.</p><p>&gt; I am. I see all tempered tunings as imposing some periodicity on, or<br/>&gt; rolling up, an infinite JI lattice.</p><p>Right.  The prime limit&apos;s only relevant before you roll up the lattice.</p><p>&gt; &gt; &gt; Good! This is far simpler and far faster than what you outlined<br/>&gt; above.<br/>&gt; &gt;<br/>&gt; &gt; Than what I outlined where?<br/>&gt;<br/>&gt; You snipped it. You mentioned numerically searching all possible<br/>&gt; generators.</p><p>Well, that&apos;s simple but slow.  I don&apos;t know what criteria you say any<br/>other method is simpler.  I&apos;ll go by lines of code for now.  Faster,<br/>yes, of course.</p><p>&gt; The simplex algorithm doesn&apos;t get to the exact solution? How could<br/>&gt; that be?? All it does is search the small number of corners, one of<br/>&gt; which is exactly at the exact solution.</p><p>This simplex algorithm has a parameter for how close you want the<br/>match to be.  And it works for arbitrary functions, so there needn&apos;t<br/>be corners.</p><p>&gt; How bizarre. I don&apos;t see how that could be a simplex algorithm then.<br/>&gt; We&apos;re talking about simplex as in linear programming, right?</p><p>We&apos;re talking about Nedler-Mead, which is what you get by searching<br/>for &quot;Python simplex&quot; on Google.  According to Google, it isn&apos;t linear<br/>programming, in that linear programming requires linear functions but<br/>this library doesn&apos;t.</p><p>&gt; &gt; If you know of a<br/>&gt; &gt; better one, show me!<br/>&gt;<br/>&gt; Gene has a method on his TOP page.</p><p>And where&apos;s that?</p><p>&gt; &gt; How can you search without iterating?<br/>&gt;<br/>&gt; Just look at each of the corners. One of the corners is guaranteed to<br/>&gt; be exactly at the minimax solution.</p><p>So you have to iterate over the corners!  Which makes it slower than a<br/>least squares optimization because you have to evaluate the function<br/>at all corners.</p><p>&gt; &gt; 7-limit music in Orwell will use a load of 11-limit<br/>&gt; &gt; intervals as dissonances.<br/>&gt;<br/>&gt; If they&apos;re indeed being heard that way . . .</p><p>You don&apos;t know the 9-limit intervals will be either, but a prime limit<br/>includes them.</p><p>                         Graham</p></div><h3><a id=13384 href="#13384">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/15/2005 5:38:09 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/16/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; On 11/15/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; As a rough guess for roughness? You&apos;d say that an interval 1 cent<br/>&gt; &gt; &gt; from 2:1 is just as rough as an interval 1 cent from 8:5??<br/>&gt; &gt;<br/>&gt; &gt;  No, but the question would never arise with an odd limit because<br/>&gt; octaves<br/>&gt; &gt; are taken out of the equation.<br/>&gt;<br/>&gt; So substitute 3:2 for 2:1.</p><p>It&apos;s a rough bet.  Quarter comma meantone gives equal error to 3:2 and<br/>6:5, but that works with the weighted minmax anyway.  Really, absolute<br/>error&apos;s already a compromise between pure roughness and pure weighted<br/>error for small errors.</p><p>&gt; &gt; I said that octave equivalence, odd limits,<br/>&gt; &gt; minimax and absolute error all go together.<br/>&gt;<br/>&gt; ?</p><p>The odd limit only works for octave equivalence.  Without octave<br/>equivalence, you have to consider 2 alongside other primes, when it<br/>should naturally be given some different weighting (although integer<br/>limits would sort this out anyway, but it breaks the spirit of a<br/>minimax).  The minimax makes more sense for a small set, which the odd<br/>limit gives you.</p><p>&gt; &gt;  Error as roughness works better.<br/>&gt;<br/>&gt; Then JI has some error already, a contradiction in terms.</p><p>That&apos;s true however you weight the errors.  No weighting gets closer<br/>than Tenney weighting.</p><p>&gt; &gt;  The minimax absolute error relative to an odd limit at least puts<br/>&gt; some cap<br/>&gt; &gt; on the roughness.<br/>&gt;<br/>&gt; Huh? How so? I don&apos;t see this at all.</p><p>More complex intervals have more natural roughness.  Simpler intervals<br/>get rougher more quickly as the error gets larger.  So for some given<br/>error, a simpler and more complex interval will have the same<br/>roughness.  For some given odd limit and worst error, you know how<br/>rough any of those consonances can get.</p><p>&gt; And a sum-abs should be ____? Sorry, I don&apos;t follow this at all. Am I<br/>&gt; trying your patience to ask you to explain this again?</p><p>A sum-abs would have the same motivations as a sum-squared as far as I<br/>can see, except it&apos;s harder to optimize.</p><p>                        Graham</p></div><h3><a id=13391 href="#13391">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/16/2005 2:23:02 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/16/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; Wow -- so if all the primes had the same error, the standard<br/>deviation<br/>&gt; &gt; would be zero, and the RMS error would therefore have to be<br/>&gt; &gt; approximately zero?<br/>&gt;<br/>&gt; If all the primes have the same error before you optimize, you can<br/>&gt; stretch the scale so they all have a very small error.</p><p>Aha.</p><p>&gt; If all the<br/>&gt; primes have the same error after you optimize, that error must be<br/>&gt; zero.  This is taking account of the sign of the error (which the<br/>&gt; standard deviation uses but the RMS doesn&apos;t).<br/>&gt;<br/>&gt; &gt;  It&apos;s explained in<br/>&gt; &gt; &gt; the bit you told me to remind you to look at.<br/>&gt; &gt; &gt;    Graham<br/>&gt; &gt;<br/>&gt; &gt; Excellent. What post was that again?<br/>&gt;<br/>&gt; November 2nd</p><p>Gotta run but I&apos;ll try to find it next time.</p></div><h3><a id=13392 href="#13392">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/16/2005 2:28:32 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/16/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt; &gt; &gt; Why not? You can use expressibility, in which case any odd<br/>limit<br/>&gt; &gt; is<br/>&gt; &gt; &gt; &gt; just a certain bound on it.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; I don&apos;t count that as &quot;independent&quot;.<br/>&gt; &gt;<br/>&gt; &gt; What would count?<br/>&gt;<br/>&gt; An octave-equivalent harmonic entropy would count, I suppose.</p><p>Is it a problem that this is a continuous function, so that complex<br/>ratios very close to a simple ratios would have to be considered<br/>somehow &apos;simple&apos;?</p><p>&gt; &gt; I am. I see all tempered tunings as imposing some periodicity on,<br/>or<br/>&gt; &gt; rolling up, an infinite JI lattice.<br/>&gt;<br/>&gt; Right.  The prime limit&apos;s only relevant before you roll up the<br/>lattice.</p><p>I think it&apos;s still relevant after.</p><p>&gt; &gt; Gene has a method on his TOP page.<br/>&gt;<br/>&gt; And where&apos;s that?</p><p><a href="http://66.98.148.43/~xenharmo/top.htm">http://66.98.148.43/~xenharmo/top.htm</a></p><p>&gt; &gt; &gt; How can you search without iterating?<br/>&gt; &gt;<br/>&gt; &gt; Just look at each of the corners. One of the corners is<br/>guaranteed to<br/>&gt; &gt; be exactly at the minimax solution.<br/>&gt;<br/>&gt; So you have to iterate over the corners!  Which makes it slower<br/>than a<br/>&gt; least squares optimization because you have to evaluate the function<br/>&gt; at all corners.</p><p>Have you actually tested the speeds?</p><p>&gt; &gt; &gt; 7-limit music in Orwell will use a load of 11-limit<br/>&gt; &gt; &gt; intervals as dissonances.<br/>&gt; &gt;<br/>&gt; &gt; If they&apos;re indeed being heard that way . . .<br/>&gt;<br/>&gt; You don&apos;t know the 9-limit intervals will be either, but a prime<br/>limit<br/>&gt; includes them.</p><p>But it doesn&apos;t necessarily call them &quot;consonances&quot; or &quot;dissonances&quot;,<br/>so whether they&apos;re heard as the relevant ratios is irrelevant :)</p></div><h3><a id=13397 href="#13397">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/16/2005 7:06:45 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/16/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; On 11/15/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; As a rough guess for roughness? You&apos;d say that an interval 1<br/>cent<br/>&gt; &gt; &gt; &gt; from 2:1 is just as rough as an interval 1 cent from 8:5??<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt;  No, but the question would never arise with an odd limit<br/>because<br/>&gt; &gt; octaves<br/>&gt; &gt; &gt; are taken out of the equation.<br/>&gt; &gt;<br/>&gt; &gt; So substitute 3:2 for 2:1.<br/>&gt;<br/>&gt; It&apos;s a rough bet.  Quarter comma meantone gives equal error to 3:2<br/>and<br/>&gt; 6:5, but that works with the weighted minmax anyway.</p><p>And with sum-abs error, among other things.</p><p>&gt; Really, absolute<br/>&gt; error&apos;s already a compromise between pure roughness and pure<br/>weighted<br/>&gt; error for small errors.</p><p>What would better capture &quot;pure roughness&quot; in your view?</p><p>&gt; &gt; &gt; I said that octave equivalence, odd limits,<br/>&gt; &gt; &gt; minimax and absolute error all go together.<br/>&gt; &gt;<br/>&gt; &gt; ?<br/>&gt;<br/>&gt; The odd limit only works for octave equivalence.</p><p>Right.</p><p>&gt; Without octave<br/>&gt; equivalence, you have to consider 2 alongside other primes, when it<br/>&gt; should naturally be given some different weighting (although integer<br/>&gt; limits would sort this out anyway, but it breaks the spirit of a<br/>&gt; minimax).  The minimax makes more sense for a small set, which the<br/>odd<br/>&gt; limit gives you.</p><p>One could argue that the minimax makes more sense for a large set.</p><p>And absolute error?</p><p>&gt; &gt; &gt;  Error as roughness works better.<br/>&gt; &gt;<br/>&gt; &gt; Then JI has some error already, a contradiction in terms.<br/>&gt;<br/>&gt; That&apos;s true however you weight the errors.  No weighting gets closer<br/>&gt; than Tenney weighting.</p><p>You mean equal weighting gets closer than Tenney weighting? I don&apos;t<br/>believe that&apos;s true, when you&apos;re talking about comparing different<br/>tunings with it, and how well that parallels comparing their<br/>roughnesses.</p><p>&gt; &gt; &gt;  The minimax absolute error relative to an odd limit at least<br/>puts<br/>&gt; &gt; some cap<br/>&gt; &gt; &gt; on the roughness.<br/>&gt; &gt;<br/>&gt; &gt; Huh? How so? I don&apos;t see this at all.<br/>&gt;<br/>&gt; More complex intervals have more natural roughness.</p><p>Right.</p><p>&gt; Simpler intervals<br/>&gt; get rougher more quickly as the error gets larger.</p><p>Right.</p><p>&gt; So for some given<br/>&gt; error, a simpler and more complex interval will have the same<br/>&gt; roughness.</p><p>I don&apos;t know about that. You may leave the local minimum associated<br/>with the more complex interval altogether before the given error<br/>brings the simpler interval up to the same roughness.</p><p>&gt; For some given odd limit and worst error, you know how<br/>&gt; rough any of those consonances can get.</p><p>If the worst error is small, the simple intervals will never (even if<br/>they have that worst error) approach the roughness of even JI<br/>renditions of the purer intervals.</p><p>&gt; &gt; And a sum-abs should be ____? Sorry, I don&apos;t follow this at all.<br/>Am I<br/>&gt; &gt; trying your patience to ask you to explain this again?<br/>&gt;<br/>&gt; A sum-abs would have the same motivations as a sum-squared as far<br/>as I<br/>&gt; can see, except it&apos;s harder to optimize.</p><p>I doubt it&apos;s significantly harder for modern computers.</p></div><h3><a id=13412 href="#13412">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/17/2005 5:14:30 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/17/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:</p><p>&gt; What would better capture &quot;pure roughness&quot; in your view?</p><p>Dunno, that&apos;s why I stick with the equal weighted errors.  I suppose<br/>you could add a Tenny residual roughness to a Tenney weighted error.</p><p>&gt; One could argue that the minimax makes more sense for a large set.</p><p>It&apos;s harder to calculate at any rate.</p><p>&gt; And absolute error?</p><p>Much the same as squared error with anything.</p><p>&gt; You mean equal weighting gets closer than Tenney weighting? I don&apos;t<br/>&gt; believe that&apos;s true, when you&apos;re talking about comparing different<br/>&gt; tunings with it, and how well that parallels comparing their<br/>&gt; roughnesses.</p><p>The simpler intervals should have smaller bands but Tenney weighting<br/>gives them larger bands.  Equal sized bands are a compromise.</p><p>&gt; I don&apos;t know about that. You may leave the local minimum associated<br/>&gt; with the more complex interval altogether before the given error<br/>&gt; brings the simpler interval up to the same roughness.</p><p>Oh yes, there&apos;s that.</p><p>&gt; If the worst error is small, the simple intervals will never (even if<br/>&gt; they have that worst error) approach the roughness of even JI<br/>&gt; renditions of the purer intervals.</p><p>But at least you have some pure, simple intervals.</p><p>&gt; &gt; &gt; And a sum-abs should be ____? Sorry, I don&apos;t follow this at all.<br/>&gt; Am I<br/>&gt; &gt; &gt; trying your patience to ask you to explain this again?<br/>&gt; &gt;<br/>&gt; &gt; A sum-abs would have the same motivations as a sum-squared as far<br/>&gt; as I<br/>&gt; &gt; can see, except it&apos;s harder to optimize.<br/>&gt;<br/>&gt; I doubt it&apos;s significantly harder for modern computers.</p><p>It&apos;s significantly harder on any computer if you do it often enough<br/>with a high level language.</p><p>                            Graham</p></div><h3><a id=13421 href="#13421">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/17/2005 1:36:25 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/17/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; &gt; What would better capture &quot;pure roughness&quot; in your view?<br/>&gt;<br/>&gt; Dunno, that&apos;s why I stick with the equal weighted errors.  I suppose<br/>&gt; you could add a Tenny residual roughness to a Tenney weighted error.</p><p>Could you elaborate?</p><p>&gt; &gt; One could argue that the minimax makes more sense for a large set.<br/>&gt;<br/>&gt; It&apos;s harder to calculate at any rate.</p><p>Maybe.</p><p>&gt; &gt; And absolute error?<br/>&gt;<br/>&gt; Much the same as squared error with anything.</p><p>But you specifically said absolute error goes with octave-equivalence<br/>and the rest (?)</p><p>&gt; &gt; You mean equal weighting gets closer than Tenney weighting? I<br/>don&apos;t<br/>&gt; &gt; believe that&apos;s true, when you&apos;re talking about comparing different<br/>&gt; &gt; tunings with it, and how well that parallels comparing their<br/>&gt; &gt; roughnesses.<br/>&gt;<br/>&gt; The simpler intervals should have smaller bands but Tenney weighting<br/>&gt; gives them larger bands.</p><p>Quite the contrary -- Tenney weighting gives them smaller bands.</p><p>&gt; Equal sized bands are a compromise.<br/>&gt;<br/>&gt; &gt; I don&apos;t know about that. You may leave the local minimum<br/>associated<br/>&gt; &gt; with the more complex interval altogether before the given error<br/>&gt; &gt; brings the simpler interval up to the same roughness.<br/>&gt;<br/>&gt; Oh yes, there&apos;s that.<br/>&gt;<br/>&gt; &gt; If the worst error is small, the simple intervals will never<br/>(even if<br/>&gt; &gt; they have that worst error) approach the roughness of even JI<br/>&gt; &gt; renditions of the purer intervals.<br/>&gt;<br/>&gt; But at least you have some pure, simple intervals.</p><p>So it&apos;s not really all about roughness then, is it? :)</p><p>&gt; &gt; &gt; &gt; And a sum-abs should be ____? Sorry, I don&apos;t follow this at<br/>all.<br/>&gt; &gt; Am I<br/>&gt; &gt; &gt; &gt; trying your patience to ask you to explain this again?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; A sum-abs would have the same motivations as a sum-squared as<br/>far<br/>&gt; &gt; as I<br/>&gt; &gt; &gt; can see, except it&apos;s harder to optimize.<br/>&gt; &gt;<br/>&gt; &gt; I doubt it&apos;s significantly harder for modern computers.<br/>&gt;<br/>&gt; It&apos;s significantly harder on any computer if you do it often enough<br/>&gt; with a high level language.</p><p>Compared to other kinds of optimizations, which involve numerical<br/>searches and the like, I think all of these kinds have essentially<br/>zero complexity and take essentially zero computer time.</p></div><h3><a id=13430 href="#13430">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/17/2005 4:03:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/17/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:</p><p>&gt; &gt; An octave-equivalent harmonic entropy would count, I suppose.<br/>&gt;<br/>&gt; Is it a problem that this is a continuous function, so that complex<br/>&gt; ratios very close to a simple ratios would have to be considered<br/>&gt; somehow &apos;simple&apos;?</p><p>It&apos;d be a problem if an interval within the limit you&apos;re looking at<br/>came out simpler than it should be.</p><p>&gt; &gt; &gt; Gene has a method on his TOP page.<br/>&gt; &gt;<br/>&gt; &gt; And where&apos;s that?<br/>&gt;<br/>&gt; <a href="http://66.98.148.43/~xenharmo/top.htm">http://66.98.148.43/~xenharmo/top.htm</a></p><p>There&apos;s one method for equal temperaments, which is simple but will be<br/>fairly slow.  For higher ranks, he doesn&apos;t give a method but says in<br/>what branch of mathematics the solution can be found.</p><p>&gt; Have you actually tested the speeds?</p><p>I can&apos;t test the speed of an algorithm I haven&apos;t implemented.  For the<br/>odd limit calculations, I found the minimax optimization to be<br/>significantly slower than the RMS, by measuring the time taken to do<br/>the search for linear temperaments.  I have a faster algorithm for the<br/>minimax now, which assumes the function only has a single, global<br/>minimum.</p><p>If you think an O(n) algorithm might be slower than an O(n**3) one,<br/>the onus is really on you to demonstrate it.  But I might look at this<br/>anyway.  The only TOP algorithm I can code directly is for equal<br/>temperaments, though.  So I won&apos;t be able to show anything about<br/>linear temperaments.  Octave equivalent optimizations are very<br/>efficient for equal temperaments because you don&apos;t have to do<br/>anything.</p><p>                     Graham</p><p>                      Graham</p></div><h3><a id=13431 href="#13431">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/17/2005 4:13:15 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/18/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt; &gt; Dunno, that&apos;s why I stick with the equal weighted errors.  I suppose<br/>&gt; &gt; you could add a Tenny residual roughness to a Tenney weighted error.<br/>&gt;<br/>&gt; Could you elaborate?</p><p>No.  I don&apos;t know enough about the perception of dissonance to specify<br/>a quantitative model.  Instead, I stick with the simplest algorithm<br/>that has some meaning.</p><p>&gt; &gt; &gt; One could argue that the minimax makes more sense for a large set.<br/>&gt; &gt;<br/>&gt; &gt; It&apos;s harder to calculate at any rate.<br/>&gt;<br/>&gt; Maybe.</p><p>The other thing is that minimax is more meaningful when you can grasp<br/>the whole set that the maximum is over.</p><p>&gt; But you specifically said absolute error goes with octave-equivalence<br/>&gt; and the rest (?)</p><p>Oh, yes.  You can&apos;t optimize for absolute error over a whole prime<br/>limit.  Ludicrously complex ratios have equal weight to the simple<br/>ones.  So you have to choose a specific set.  An odd limit is a<br/>convenient set to choose.  You could use an integer limit or some such<br/>instead, but it&apos;s bound to be bigger than an equivalent odd limit and<br/>there&apos;s no need for it because it&apos;s easier to use weighted primes.</p><p>&gt; &gt; &gt; You mean equal weighting gets closer than Tenney weighting? I<br/>&gt; don&apos;t<br/>&gt; &gt; &gt; believe that&apos;s true, when you&apos;re talking about comparing different<br/>&gt; &gt; &gt; tunings with it, and how well that parallels comparing their<br/>&gt; &gt; &gt; roughnesses.<br/>&gt; &gt;<br/>&gt; &gt; The simpler intervals should have smaller bands but Tenney weighting<br/>&gt; &gt; gives them larger bands.<br/>&gt;<br/>&gt; Quite the contrary -- Tenney weighting gives them smaller bands.</p><p>Yes, the contrary.  Simple intervals can have larger mistunings for a<br/>given roughness.</p><p>&gt; So it&apos;s not really all about roughness then, is it? :)</p><p>It&apos;s mostly about simplicity.</p><p>&gt; Compared to other kinds of optimizations, which involve numerical<br/>&gt; searches and the like, I think all of these kinds have essentially<br/>&gt; zero complexity and take essentially zero computer time.</p><p>Prime limit algorithms will be easier than odd limit ones, but I<br/>expect it&apos;ll still matter if you use a large enough search space.<br/>Large searches are good because the user doesn&apos;t have to specify<br/>arbitrary parameters.</p><p>                        Graham</p></div><h3><a id=13439 href="#13439">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/21/2005 2:31:44 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/17/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; &gt; &gt; An octave-equivalent harmonic entropy would count, I suppose.<br/>&gt; &gt;<br/>&gt; &gt; Is it a problem that this is a continuous function, so that<br/>complex<br/>&gt; &gt; ratios very close to a simple ratios would have to be considered<br/>&gt; &gt; somehow &apos;simple&apos;?<br/>&gt;<br/>&gt; It&apos;d be a problem if an interval within the limit you&apos;re looking at<br/>&gt; came out simpler than it should be.</p><p>So where does this leave us?</p><p>&gt; &gt; &gt; &gt; Gene has a method on his TOP page.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; And where&apos;s that?<br/>&gt; &gt;<br/>&gt; &gt; <a href="http://66.98.148.43/~xenharmo/top.htm">http://66.98.148.43/~xenharmo/top.htm</a><br/>&gt;<br/>&gt; There&apos;s one method for equal temperaments, which is simple but will<br/>be<br/>&gt; fairly slow.</p><p>My method for equal temperaments is referenced in my paper and is<br/>immediate.</p><p>&gt; For higher ranks, he doesn&apos;t give a method but says in<br/>&gt; what branch of mathematics the solution can be found.</p><p>I wish Gene himself would chime in at this point.</p></div><h3><a id=13440 href="#13440">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/21/2005 2:38:28 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/18/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt; &gt; Dunno, that&apos;s why I stick with the equal weighted errors.  I<br/>suppose<br/>&gt; &gt; &gt; you could add a Tenny residual roughness to a Tenney weighted<br/>error.<br/>&gt; &gt;<br/>&gt; &gt; Could you elaborate?<br/>&gt;<br/>&gt; No.  I don&apos;t know enough about the perception of dissonance to<br/>specify<br/>&gt; a quantitative model.  Instead, I stick with the simplest algorithm<br/>&gt; that has some meaning.</p><p>I&apos;d like to do more than simply ignore the ideas you throw out.</p><p>&gt; &gt; &gt; &gt; One could argue that the minimax makes more sense for a large<br/>set.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; It&apos;s harder to calculate at any rate.<br/>&gt; &gt;<br/>&gt; &gt; Maybe.<br/>&gt;<br/>&gt; The other thing is that minimax is more meaningful when you can<br/>grasp<br/>&gt; the whole set that the maximum is over.<br/>&gt;<br/>&gt; &gt; But you specifically said absolute error goes with octave-<br/>equivalence<br/>&gt; &gt; and the rest (?)<br/>&gt;<br/>&gt; Oh, yes.  You can&apos;t optimize for absolute error over a whole prime<br/>&gt; limit.  Ludicrously complex ratios have equal weight to the simple<br/>&gt; ones.</p><p>Seems like you said &quot;absolute error&quot; but meant &quot;unweighted error&quot;.<br/>Absolute error means something different to me -- that you&apos;re taking<br/>the absolute value, rather than the square or something else, of the<br/>error.</p><p>&gt; So you have to choose a specific set.  An odd limit is a<br/>&gt; convenient set to choose.  You could use an integer limit or some<br/>such<br/>&gt; instead, but it&apos;s bound to be bigger than an equivalent odd limit<br/>and<br/>&gt; there&apos;s no need for it because it&apos;s easier to use weighted primes.<br/>&gt;<br/>&gt; &gt; &gt; &gt; You mean equal weighting gets closer than Tenney weighting? I<br/>&gt; &gt; don&apos;t<br/>&gt; &gt; &gt; &gt; believe that&apos;s true, when you&apos;re talking about comparing<br/>different<br/>&gt; &gt; &gt; &gt; tunings with it, and how well that parallels comparing their<br/>&gt; &gt; &gt; &gt; roughnesses.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; The simpler intervals should have smaller bands but Tenney<br/>weighting<br/>&gt; &gt; &gt; gives them larger bands.<br/>&gt; &gt;<br/>&gt; &gt; Quite the contrary -- Tenney weighting gives them smaller bands.<br/>&gt;<br/>&gt; Yes, the contrary.  Simple intervals can have larger mistunings for<br/>a<br/>&gt; given roughness.</p><p>Not if you&apos;re comparing them against like intervals. If you&apos;re<br/>comparing like against like, a given mistuning will contribute more<br/>roughness for a simple interval than for a complex interval. As<br/>different tunings of the same temperament will have the same set of<br/>important intervals, you can always compare them purely by comparing<br/>like against like.</p><p>&gt; &gt; So it&apos;s not really all about roughness then, is it? :)<br/>&gt;<br/>&gt; It&apos;s mostly about simplicity.</p><p>I can sympathize with that sentiment.</p><p>&gt; &gt; Compared to other kinds of optimizations, which involve numerical<br/>&gt; &gt; searches and the like, I think all of these kinds have essentially<br/>&gt; &gt; zero complexity and take essentially zero computer time.<br/>&gt;<br/>&gt; Prime limit algorithms will be easier than odd limit ones, but I<br/>&gt; expect it&apos;ll still matter if you use a large enough search space.<br/>&gt; Large searches are good because the user doesn&apos;t have to specify<br/>&gt; arbitrary parameters.</p></div><h3><a id=13456 href="#13456">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/21/2005 4:52:36 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; I wish Gene himself would chime in at this point.</p><p>Quite recently I explained how you set up the linear programming<br/>prblem. What more needs to be said?</p></div><h3><a id=13482 href="#13482">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/22/2005 5:50:18 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/22/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:</p><p>&gt; I&apos;d like to do more than simply ignore the ideas you throw out.</p><p>Oh, well, that&apos;s sweet :)  My life&apos;s a bit too chaotic to follow these<br/>things through now, and I&apos;m not sure it&apos;s an interesting problem<br/>anyway.  It&apos;s not so difficult to tell a good temperament from a bad<br/>one, and telling a quite good one from a middling good one isn&apos;t so<br/>important.  Generally, I can&apos;t see many interesting gaps to be filled<br/>in.  A systematic way of deciding what ET pairs to look at for the<br/>temperament search would be nice.  I&apos;m also trying to get the search<br/>working in C.</p><p>&gt; Seems like you said &quot;absolute error&quot; but meant &quot;unweighted error&quot;.<br/>&gt; Absolute error means something different to me -- that you&apos;re taking<br/>&gt; the absolute value, rather than the square or something else, of the<br/>&gt; error.</p><p>Yes, I agree with that.  I thought you shifted meaning somewhere, but<br/>it&apos;s not worth looking back now.  Unweighted error goes with odd<br/>limits.</p><p>&gt; &gt; Yes, the contrary.  Simple intervals can have larger mistunings for<br/>&gt; a<br/>&gt; &gt; given roughness.<br/>&gt;<br/>&gt; Not if you&apos;re comparing them against like intervals. If you&apos;re<br/>&gt; comparing like against like, a given mistuning will contribute more<br/>&gt; roughness for a simple interval than for a complex interval. As<br/>&gt; different tunings of the same temperament will have the same set of<br/>&gt; important intervals, you can always compare them purely by comparing<br/>&gt; like against like.</p><p>I&apos;m not really sure what you mean here.  If you&apos;re comparing like<br/>intervals, the weighting shouldn&apos;t matter.  And you&apos;re talking about<br/>the contribution of the mistuning again.  We seem to be going round in<br/>circles.  For the minimax, I&apos;m not interested in the contribution but<br/>the absolute pain, however that&apos;s likely to be perceived.  Also, I&apos;m<br/>interested in comparing different temperaments more than different<br/>tunings of the same temperament.  To get a figure for the badness you<br/>need to first do an optimization.</p><p>&gt; &gt; &gt; So it&apos;s not really all about roughness then, is it? :)<br/>&gt; &gt;<br/>&gt; &gt; It&apos;s mostly about simplicity.<br/>&gt;<br/>&gt; I can sympathize with that sentiment.</p><p>I didn&apos;t see a reply to my benchmarking post, so I&apos;ll mention it here:<br/>I sped up the TOP optimization by removing the repeated calls to the<br/>weighted primes function.  It&apos;s now much faster, but still an order of<br/>magnitude slower than the RMS for higher limits.  There&apos;s also one<br/>more line of code.</p><p>                             Graham</p></div><h3><a id=13489 href="#13489">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/23/2005 1:08:22 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; I&apos;m not really sure what you mean here.  If you&apos;re comparing like<br/>&gt; intervals, the weighting shouldn&apos;t matter.</p><p>Why not? There will be several comparisons, each of which compares<br/>like intervals to one another, but overall many types of intervals<br/>will get compared.</p><p>&gt; And you&apos;re talking about<br/>&gt; the contribution of the mistuning again.  We seem to be going round<br/>in<br/>&gt; circles.</p><p>I&apos;d like to understand your point of view better, then.</p><p>&gt; For the minimax, I&apos;m not interested in the contribution but<br/>&gt; the absolute pain,</p><p>In each interval, right?</p><p>&gt; however that&apos;s likely to be perceived.</p><p>And you can&apos;t assume this is zero in JI, right?</p><p>&gt; Also, I&apos;m<br/>&gt; interested in comparing different temperaments more than different<br/>&gt; tunings of the same temperament.</p><p>I should have included comparing different temperaments of the same<br/>JI system too, as my arguments applied to that as well.</p><p>&gt; To get a figure for the badness you<br/>&gt; need to first do an optimization.<br/>&gt;<br/>&gt; &gt; &gt; &gt; So it&apos;s not really all about roughness then, is it? :)<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; It&apos;s mostly about simplicity.<br/>&gt; &gt;<br/>&gt; &gt; I can sympathize with that sentiment.<br/>&gt;<br/>&gt; I didn&apos;t see a reply to my benchmarking post,</p><p>It seemed clear you weren&apos;t looking at the best formulae at least for<br/>the ET case, so I wanted to make sure that was cleared up first . . .</p><p>&gt; so I&apos;ll mention it here:<br/>&gt; I sped up the TOP optimization by removing the repeated calls to the<br/>&gt; weighted primes function.  It&apos;s now much faster, but still an order<br/>of<br/>&gt; magnitude slower than the RMS for higher limits.  There&apos;s also one<br/>&gt; more line of code.<br/>&gt;<br/>&gt;<br/>&gt;                              Graham<br/>&gt;</p></div><h3><a id=13490 href="#13490">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/23/2005 1:39:04 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;m still waiting for a reply on this from you, Gene.</p><p>Also, everyone seems to think Yahoo&apos;s search feature improved like a<br/>few months ago, but as far as I can tell, it&apos;s gotten worse. It<br/>refuses to find my posts!</p><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot;<br/>&lt;gwsmith@s...&gt;<br/>&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; &gt; Any differences for<br/>&gt; &gt; &gt; the systems in my paper?<br/>&gt; &gt;<br/>&gt; &gt; I&apos;ll check, but if you have handy a table of wedgies you could<br/>post<br/>&gt; &gt; here it would be nice.<br/>&gt;<br/>&gt; I know you insist that wedgies are wedge products of vals and not<br/>&gt; wedge products of commas, but since taking the dual is so easy,<br/>I&apos;ll<br/>&gt; give you the latter anyway :)<br/>&gt;<br/>&gt; Commas&apos; bivector        Horagram name<br/>&gt; [[-14 0 8 0 -5 0&gt;&gt;&#x9;Blacksmith<br/>&gt; [[2 -5 3 -4 4 -4&gt;&gt;&#x9;Dimisept<br/>&gt; [[16 -6 -4 2 4 -1&gt;&gt;&#x9;Dominant<br/>&gt; [[-14 1 7 -6 0 -3&gt;&gt;&#x9;August<br/>&gt; [[-2 -12 11 4 -4 -2&gt;&gt;&#x9;Pajara<br/>&gt; [[20 -4 -8 -1 8 -2&gt;&gt;&#x9;Semaphore<br/>&gt; [[-12 13 -4 -10 4 -1&gt;&gt;&#x9;Meantone<br/>&gt; [[4 7 -8 -8 8 -2&gt;&gt;&#x9;Injera<br/>&gt; [[13 8 -14 2 3 4&gt;&gt;&#x9;Negrisept<br/>&gt; [[14 -18 7 6 0 -3&gt;&gt;&#x9;Augene<br/>&gt; [[-7 12 -6 3 -5 6&gt;&gt;&#x9;Keemun<br/>&gt; [[28 -19 0 12 0 0&gt;&gt;&#x9;Catler<br/>&gt; [[-5 1 2 10 -10 6&gt;&gt;&#x9;Hedgehog<br/>&gt; [[-30 6 12 -2 -9 1&gt;&gt;&#x9;Superpyth<br/>&gt; [[-5 1 2 -13 9 -7&gt;&gt;&#x9;Sensisept<br/>&gt; [[1 20 -17 -2 2 6&gt;&gt;&#x9;Lemba<br/>&gt; [[-28 18 1 -6 -5 3&gt;&gt;&#x9;Porcupine<br/>&gt; [[32 -17 -4 9 4 -1&gt;&gt;&#x9;Flattone<br/>&gt; [[25 -5 -10 12 -1 5&gt;&gt;&#x9;Magic<br/>&gt; [[-3 13 -9 6 -6 8&gt;&gt;&#x9;Doublewide<br/>&gt; [[-21 12 2 3 -10 6&gt;&gt;&#x9;Nautilus<br/>&gt; [[-16 -12 19 4 -9 -2&gt;&gt;&#x9;Beatles<br/>&gt; [[8 9 -12 -11 12 -3&gt;&gt;&#x9;Liese<br/>&gt; [[36 -10 -12 1 12 -3&gt;&gt;&#x9;Cynder<br/>&gt; [[27 7 -21 8 3 7&gt;&gt;&#x9;Orwell<br/>&gt; [[-10 25 -15 -14 8 1&gt;&gt;&#x9;Garibaldi<br/>&gt; [[9 -17 9 -7 9 -10&gt;&gt;&#x9;Myna<br/>&gt; [[15 20 -25 -2 7 6&gt;&gt;&#x9;Miracle<br/>&gt;<br/>&gt; bonus:<br/>&gt;<br/>&gt; [[-34 22 1 18 -27 18&gt;&gt;&#x9;Ennealimmal<br/>&gt;<br/>&gt; I&apos;d also like to know, when the TOP tuning is not unique (such as<br/>for<br/>&gt; 5-limit Blackwood), whether a stretched Kees tuning *could* be a<br/>(non-<br/>&gt; canonical) TOP tuning.<br/>&gt;<br/>&gt; &gt; &gt; Based on TOP, I told Igliashon that 13-equal<br/>&gt; &gt; &gt; is better in the 7-limit using the Orwell approximation than<br/>any<br/>&gt; other<br/>&gt; &gt; &gt; approximation of the 7-limit in 13-equal. Is this still true<br/>&gt; based on<br/>&gt; &gt; &gt; Kees?<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t even know what you mean.<br/>&gt;<br/>&gt; I think I could state this as: Is the best 13-equal val for the 7-<br/>&gt; limit the one which is an Orwell val?</p></div><h3><a id=13494 href="#13494">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/24/2005 7:26:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/24/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; &gt; And you&apos;re talking about<br/>&gt; &gt; the contribution of the mistuning again.  We seem to be going round<br/>&gt; in<br/>&gt; &gt; circles.<br/>&gt;<br/>&gt; I&apos;d like to understand your point of view better, then.</p><p>For a minmax, it&apos;s how bad the interval sounds that&apos;s important, not<br/>how much the mistuning contributes to the badness.</p><p>&gt; &gt; For the minimax, I&apos;m not interested in the contribution but<br/>&gt; &gt; the absolute pain,<br/>&gt;<br/>&gt; In each interval, right?<br/>&gt;<br/>&gt; &gt; however that&apos;s likely to be perceived.<br/>&gt;<br/>&gt; And you can&apos;t assume this is zero in JI, right?</p><p>If you&apos;re looking at a minimax, the intervals closest to JI don&apos;t<br/>matter.  Only the interval furthest away is scored.  So it&apos;s only that<br/>interval we need to worry about.  If some intervals are reasonably far<br/>from JI, they&apos;ll be at the point where an interval sounds bad<br/>according to how mistuned it is, by unweighted mistuning.  Some<br/>intervals may be so mistuned that they become meaningless, and with a<br/>minimax you can also specify that no such intervals exist.  If they&apos;re<br/>all close to JI, at least the minimax can tell you that.  You can<br/>specify how close you want to be to JI, and make sure all intervals<br/>are within that distance.  Perhaps the unweighted minimax isn&apos;t so<br/>useful for temperaments that are close to JI.  But, for a given<br/>complexity, such temperaments are difficult to find so the precise<br/>scoring isn&apos;t so important.</p><p>Then again, from a practical point of view, you may not be able to<br/>tune to better than 1 cent say.  So if all intervals are within 1 cent<br/>of JI the temperament is as good as (and pointless compared to ;) JI.<br/>You can select the minimax according to your tuning resolution.</p><p>                                     Graham</p></div><h3><a id=13497 href="#13497">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/25/2005 2:00:11 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/24/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt; &gt;<br/>&gt; &gt; &gt; And you&apos;re talking about<br/>&gt; &gt; &gt; the contribution of the mistuning again.  We seem to be going<br/>round<br/>&gt; &gt; in<br/>&gt; &gt; &gt; circles.<br/>&gt; &gt;<br/>&gt; &gt; I&apos;d like to understand your point of view better, then.<br/>&gt;<br/>&gt; For a minmax, it&apos;s how bad the interval sounds that&apos;s important,</p><p>I don&apos;t think you can state that as a matter of dogma. For one thing,<br/>would you consider a pure 8:5 to sound &quot;badder&quot; than a somewhat-<br/>mistuned 3:2?</p><p>&gt; not<br/>&gt; how much the mistuning contributes to the badness.</p><p>&gt; &gt; &gt; For the minimax, I&apos;m not interested in the contribution but<br/>&gt; &gt; &gt; the absolute pain,<br/>&gt; &gt;<br/>&gt; &gt; In each interval, right?<br/>&gt; &gt;<br/>&gt; &gt; &gt; however that&apos;s likely to be perceived.<br/>&gt; &gt;<br/>&gt; &gt; And you can&apos;t assume this is zero in JI, right?<br/>&gt;<br/>&gt; If you&apos;re looking at a minimax, the intervals closest to JI don&apos;t<br/>&gt; matter.  Only the interval furthest away is scored.  So it&apos;s only<br/>that<br/>&gt; interval we need to worry about.  If some intervals are reasonably<br/>far<br/>&gt; from JI, they&apos;ll be at the point where an interval sounds bad<br/>&gt; according to how mistuned it is, by unweighted mistuning.  Some<br/>&gt; intervals may be so mistuned that they become meaningless, and with<br/>a<br/>&gt; minimax you can also specify that no such intervals exist.  If<br/>they&apos;re<br/>&gt; all close to JI, at least the minimax can tell you that.  You can<br/>&gt; specify how close you want to be to JI, and make sure all intervals<br/>&gt; are within that distance.  Perhaps the unweighted minimax isn&apos;t so<br/>&gt; useful for temperaments that are close to JI.  But, for a given<br/>&gt; complexity, such temperaments are difficult to find so the precise<br/>&gt; scoring isn&apos;t so important.</p><p>Sounds like a lot of hand-waving to me, but my mind is still<br/>open . . . We also haven&apos;t much considered how much various intervals<br/>can be mistuned when they coexist in *chords*, which you claim is the<br/>normal case . . .</p><p>&gt; Then again, from a practical point of view, you may not be able to<br/>&gt; tune to better than 1 cent say.  So if all intervals are within 1<br/>cent<br/>&gt; of JI the temperament is as good as (and pointless compared to ;)<br/>JI.<br/>&gt; You can select the minimax according to your tuning resolution.</p><p>?</p></div><h3><a id=13506 href="#13506">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/25/2005 11:45:29 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; I&apos;m still waiting for a reply on this from you, Gene.</p><p>Problems only arise for blacksmith and catler, and this is because the<br/>5 (in the case of blacksmith) or the 7 (in the case of catler) are not<br/>determined by the minimax condition. Hence, an exact value for a 5 or<br/>7, when the octave is adjusted, becomes an inexact value, whereas the<br/>exact value is probably what is wanted.</p></div><h3><a id=13512 href="#13512">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/27/2005 4:40:10 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/26/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:</p><p>&gt; &gt; For a minmax, it&apos;s how bad the interval sounds that&apos;s important,<br/>&gt;<br/>&gt; I don&apos;t think you can state that as a matter of dogma. For one thing,<br/>&gt; would you consider a pure 8:5 to sound &quot;badder&quot; than a somewhat-<br/>&gt; mistuned 3:2?</p><p>Who&apos;s being dogmatic?  But yes, I would.  Fifths are still the<br/>strongest (octaves aside) consonances in quarter comma meantone<br/>despite being out of tune.</p><p>&gt; Sounds like a lot of hand-waving to me, but my mind is still<br/>&gt; open . . . We also haven&apos;t much considered how much various intervals<br/>&gt; can be mistuned when they coexist in *chords*, which you claim is the<br/>&gt; normal case . . .</p><p>Everything&apos;s hand-waving.  What else do we have?  The idea is that the<br/>dissonance of the chord is determined by the most dissonant<br/>constituent interval.  That&apos;s what led to odd limits in the first<br/>place.</p><p>                    Graham</p></div><h3><a id=13515 href="#13515">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/27/2005 9:32:03 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Sounds like a lot of hand-waving to me, but my mind is still<br/>&gt;&gt; open . . . We also haven&apos;t much considered how much various intervals<br/>&gt;&gt; can be mistuned when they coexist in *chords*, which you claim is the<br/>&gt;&gt; normal case . . .<br/>&gt;<br/>&gt;Everything&apos;s hand-waving.  What else do we have?  The idea is that the<br/>&gt;dissonance of the chord is determined by the most dissonant<br/>&gt;constituent interval.  That&apos;s what led to odd limits in the first<br/>&gt;place.</p><p>I&apos;m not sure what proposal this is, but for the record I wouldn&apos;t take<br/>this in support of weighting more heavily the errors of the most complex<br/>intervals in a chord.  Neighborhood-of-the-octave-you&apos;re-in dissonance<br/>isn&apos;t at all like mistuned-by-a-small-amount dissonance.</p><p>Odd limits work decently well overall, but something which takes all<br/>the chord&apos;s intervals into account would be better.  Similarly, I<br/>think RMS that took the errors of all the intervals into account<br/>would be better than what I think you&apos;re doing (and better than TOP).<br/>You said somewhere that the latter tracks the former, but I didn&apos;t<br/>notice if you posted evidence.  I bet the former is harder to optimize<br/>for?</p><p>-Carl</p></div><h3><a id=13518 href="#13518">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/28/2005 11:06:40 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p>&gt; Odd limits work decently well overall, but something which takes all<br/>&gt; the chord&apos;s intervals into account would be better.  Similarly, I<br/>&gt; think RMS that took the errors of all the intervals into account<br/>&gt; would be better than what I think you&apos;re doing (and better than TOP).<br/>&gt; You said somewhere that the latter tracks the former, but I didn&apos;t<br/>&gt; notice if you posted evidence.  I bet the former is harder to optimize<br/>&gt; for?</p><p>RMS with all the intervals is easy to optimize for, and is what is<br/>normally done.</p></div><h3><a id=13520 href="#13520">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/28/2005 11:47:20 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Odd limits work decently well overall, but something which takes all<br/>&gt;&gt; the chord&apos;s intervals into account would be better.  Similarly, I<br/>&gt;&gt; think RMS that took the errors of all the intervals into account<br/>&gt;&gt; would be better than what I think you&apos;re doing (and better than TOP).<br/>&gt;&gt; You said somewhere that the latter tracks the former, but I didn&apos;t<br/>&gt;&gt; notice if you posted evidence.  I bet the former is harder to optimize<br/>&gt;&gt; for?<br/>&gt;<br/>&gt;RMS with all the intervals is easy to optimize for, and is what is<br/>&gt;normally done.</p><p>I thought that was the pre-TOP paradigm (though perfect octaves<br/>were generally assumed).</p><p>But Paul has a point about the &apos;assuming a chord&apos; thing.  TOP<br/>doesn&apos;t require that assumption.</p><p>-Carl</p></div><h3><a id=13521 href="#13521">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>11/28/2005 11:50:06 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Odd limits work decently well overall, but something which takes all<br/>&gt;&gt; the chord&apos;s intervals into account would be better.  Similarly, I<br/>&gt;&gt; think RMS that took the errors of all the intervals into account<br/>&gt;&gt; would be better than what I think you&apos;re doing (and better than TOP).<br/>&gt;&gt; You said somewhere that the latter tracks the former, but I didn&apos;t<br/>&gt;&gt; notice if you posted evidence.  I bet the former is harder to optimize<br/>&gt;&gt; for?<br/>&gt;<br/>&gt;RMS with all the intervals is easy to optimize for, and is what is<br/>&gt;normally done.</p><p>Also, pairwise (dyadic) consideration isn&apos;t necessarily what I<br/>meant here.  Chordadic harmonic entropy, long one of the most<br/>interesting problems in tuning theory IMO, now seems to be the<br/>single most significant missing piece.  I know you&apos;ve made a<br/>few stabs at getting into harmonic entropy, Gene (&quot;poor man&apos;s&quot;,<br/>and maybe others), but I never saw Paul reply.  Meanwhile, it&apos;s<br/>been quite a while since I thought Paul had an approach for<br/>triads that he was just waiting for computer time to run......</p><p>-Carl</p></div><h3><a id=13523 href="#13523">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/28/2005 4:38:58 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt; &gt;<br/>&gt; &gt; I&apos;m still waiting for a reply on this from you, Gene.<br/>&gt;<br/>&gt; Problems only arise for blacksmith and catler, and this is because<br/>the<br/>&gt; 5 (in the case of blacksmith) or the 7 (in the case of catler) are not<br/>&gt; determined by the minimax condition. Hence, an exact value for a 5 or<br/>&gt; 7, when the octave is adjusted, becomes an inexact value, whereas the<br/>&gt; exact value is probably what is wanted.</p><p>Probably? What *are* the minimax Kees tunings of (the 7-limit)<br/>Blacksmith and Catler? I seriously doubt the latter has pure 7s!</p></div><h3><a id=13527 href="#13527">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/28/2005 11:02:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p>&gt; I know you&apos;ve made a<br/>&gt; few stabs at getting into harmonic entropy, Gene (&quot;poor man&apos;s&quot;,<br/>&gt; and maybe others), but I never saw Paul reply.  Meanwhile, it&apos;s<br/>&gt; been quite a while since I thought Paul had an approach for<br/>&gt; triads that he was just waiting for computer time to run......</p><p>My idea of using Stieltjes integration with the ? function mever got<br/>followed up on, but I don&apos;t see that that&apos;s going to help with chords.</p></div><h3><a id=13528 href="#13528">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/28/2005 11:07:21 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; Probably? What *are* the minimax Kees tunings of (the 7-limit)<br/>&gt; Blacksmith and Catler? I seriously doubt the latter has pure 7s!</p><p>I get &lt;1200 1900 2800 3368.8259| for Catler. Why is this wrong, in<br/>your view?</p></div><h3><a id=13537 href="#13537">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/29/2005 2:02:51 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; Probably? What *are* the minimax Kees tunings of (the 7-limit)<br/>&gt; &gt; Blacksmith and Catler? I seriously doubt the latter has pure 7s!<br/>&gt;<br/>&gt; I get &lt;1200 1900 2800 3368.8259| for Catler. Why is this wrong, in<br/>&gt; your view?</p><p>See below. I think problem is that the minimax Kees result is not<br/>unique. So I&apos;m beginning to wonder if your statement that for some<br/>temperaments, a minimax Kees tuning cannot be a stretched or<br/>compressed TOP tuning is incorrect.</p><p>Here are some of the Kees-weighted errors for your result. We only<br/>need to look at ratios involving 7 since the rest of the tuning is<br/>fixed:</p><p>error(7/4)/lg2(7) = 0<br/>error(7/5)/lg2(7) = 4.88<br/>error(7/6)/lg2(7) = 0.70<br/>error(9/7)/lg2(9) = 1.23<br/>error(15/14)/lg2(15) = 3.00</p><p>Let&apos;s try a stretched TOP Catler instead:</p><p>&lt;1200 1900 2800 3375.37|</p><p>error(7/4)/lg2(7) = 2.33<br/>error(7/5)/lg2(7) = 2.54<br/>error(7/6)/lg2(7) = 3.03<br/>error(9/7)/lg2(9) = 3.30<br/>error(15/14)/lg2(15) = 1.33</p><p>This seems to have a lower minimax! It certainly looks like a better<br/>way to implement Catler in an octave-equivalent world, qualitatively<br/>speaking.</p><p>What I think is going on is that, since 6/5 has a Kees-weighted error<br/>of 6.74 in both tunings, is that 7 is free to wiggle around quite a<br/>bit without changing the minimax.</p><p>But we have a convention for non-unique TOP tunings which is to state<br/>the tuning that acheives the minimax over the subset of ratios whose<br/>tuning can vary without changing the overall minimax. Can we apply<br/>this sort of convention for minimax Kees as well?</p></div><h3><a id=13541 href="#13541">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/29/2005 2:34:49 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/26/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; &gt; For a minmax, it&apos;s how bad the interval sounds that&apos;s important,<br/>&gt; &gt;<br/>&gt; &gt; I don&apos;t think you can state that as a matter of dogma. For one<br/>thing,<br/>&gt; &gt; would you consider a pure 8:5 to sound &quot;badder&quot; than a somewhat-<br/>&gt; &gt; mistuned 3:2?<br/>&gt;<br/>&gt; Who&apos;s being dogmatic?  But yes, I would.  Fifths are still the<br/>&gt; strongest (octaves aside) consonances in quarter comma meantone<br/>&gt; despite being out of tune.</p><p>You say that for a minimax it&apos;s how bad the intervals sound that&apos;s<br/>important. But if the pure 8:5 sounds &quot;badder&quot; than a whole host of<br/>different fifths, and so on, doesn&apos;t that imply that only the tuning<br/>of the most complex intervals is going to matter? I certainly don&apos;t<br/>see the use of minimax as forcing you into anything like that kind of<br/>scenario; hence your statement seems like groundless dogma to me<br/>right now. (But I&apos;m sure it isn&apos;t . . . you just have to explain it<br/>better)</p><p>&gt; &gt; Sounds like a lot of hand-waving to me, but my mind is still<br/>&gt; &gt; open . . . We also haven&apos;t much considered how much various<br/>intervals<br/>&gt; &gt; can be mistuned when they coexist in *chords*, which you claim is<br/>the<br/>&gt; &gt; normal case . . .<br/>&gt;<br/>&gt; Everything&apos;s hand-waving.  What else do we have?  The idea is that<br/>the<br/>&gt; dissonance of the chord</p><p>What&apos;s &quot;the chord&quot;?</p><p>&gt; is determined by the most dissonant<br/>&gt; constituent interval.</p><p>That&apos;s &quot;the idea&quot; of what?</p><p>&gt; That&apos;s what led to odd limits in the first<br/>&gt; place.</p><p>When we use RMS or MAD with odd limits, how are we following the idea<br/>that the dissonance of &quot;the chord&quot; is determined by the most<br/>dissonant constituent interval?</p></div><h3><a id=13544 href="#13544">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>11/29/2005 7:58:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 11/30/05, Paul Erlich &lt;<a href="mailto:perlich@aya.yale.edu">perlich@aya.yale.edu</a>&gt; wrote:</p><p>&gt; You say that for a minimax it&apos;s how bad the intervals sound that&apos;s<br/>&gt; important. But if the pure 8:5 sounds &quot;badder&quot; than a whole host of<br/>&gt; different fifths, and so on, doesn&apos;t that imply that only the tuning<br/>&gt; of the most complex intervals is going to matter? I certainly don&apos;t<br/>&gt; see the use of minimax as forcing you into anything like that kind of<br/>&gt; scenario; hence your statement seems like groundless dogma to me<br/>&gt; right now. (But I&apos;m sure it isn&apos;t . . . you just have to explain it<br/>&gt; better)</p><p>It means the unweighted minimax isn&apos;t perfect.  But I&apos;m only looking<br/>at a simple set of options:</p><p>Tenney weighting vs no weighting<br/>octave equivalent vs octave specific<br/>minimax vs rms<br/>primes vs interval sets</p><p>These are the simple things you can do to calculate the tuning damage.<br/> I don&apos;t think the basic theory&apos;s solid enough to justify more<br/>complicated methods.  So as Tenney weighting goes the wrong way, we<br/>use no weighting.  There&apos;s got to be some other weighting which is<br/>better, but I don&apos;t know what it is, and it&apos;s all going to depend on<br/>context anyway.</p><p>&gt; &gt; Everything&apos;s hand-waving.  What else do we have?  The idea is that<br/>&gt; the<br/>&gt; &gt; dissonance of the chord<br/>&gt;<br/>&gt; What&apos;s &quot;the chord&quot;?</p><p>Whatever chord you want to find the dissonance of.</p><p>&gt; &gt; is determined by the most dissonant<br/>&gt; &gt; constituent interval.<br/>&gt;<br/>&gt; That&apos;s &quot;the idea&quot; of what?</p><p>The idea of odd limits being a measure of dissonance.</p><p>&gt; &gt; That&apos;s what led to odd limits in the first<br/>&gt; &gt; place.<br/>&gt;<br/>&gt; When we use RMS or MAD with odd limits, how are we following the idea<br/>&gt; that the dissonance of &quot;the chord&quot; is determined by the most<br/>&gt; dissonant constituent interval?</p><p>We aren&apos;t for RMS.  I don&apos;t know what MAD is.  Maximum absolute<br/>deviation?  In that case, it assumes that there&apos;s a particular<br/>mistuning beyond which an interval becomes unrecognizable, like<br/>there&apos;s a particular complexity beyond which the ratio becomes<br/>unrecognizable.</p><p>                       Graham</p></div><h3><a id=13548 href="#13548">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>11/30/2005 11:31:31 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p>&gt; See below. I think problem is that the minimax Kees result is not<br/>&gt; unique. So I&apos;m beginning to wonder if your statement that for some<br/>&gt; temperaments, a minimax Kees tuning cannot be a stretched or<br/>&gt; compressed TOP tuning is incorrect.</p><p>Usually it is unique, and that&apos;s how you find examples.</p><p>&gt; But we have a convention for non-unique TOP tunings which is to state<br/>&gt; the tuning that acheives the minimax over the subset of ratios whose<br/>&gt; tuning can vary without changing the overall minimax. Can we apply<br/>&gt; this sort of convention for minimax Kees as well?</p><p>That&apos;s certainly a plan.</p></div><h3><a id=13551 href="#13551">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/30/2005 3:25:35 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Graham Breed &lt;gbreed@g...&gt; wrote:<br/>&gt;<br/>&gt; On 11/30/05, Paul Erlich &lt;perlich@a...&gt; wrote:<br/>&gt;<br/>&gt; &gt; You say that for a minimax it&apos;s how bad the intervals sound that&apos;s<br/>&gt; &gt; important. But if the pure 8:5 sounds &quot;badder&quot; than a whole host<br/>of<br/>&gt; &gt; different fifths, and so on, doesn&apos;t that imply that only the<br/>tuning<br/>&gt; &gt; of the most complex intervals is going to matter? I certainly<br/>don&apos;t<br/>&gt; &gt; see the use of minimax as forcing you into anything like that<br/>kind of<br/>&gt; &gt; scenario; hence your statement seems like groundless dogma to me<br/>&gt; &gt; right now. (But I&apos;m sure it isn&apos;t . . . you just have to explain<br/>it<br/>&gt; &gt; better)<br/>&gt;<br/>&gt; It means the unweighted minimax isn&apos;t perfect.</p><p>This seems to be a different issue, one that remains regardless of<br/>what weighting you use.</p><p>&gt; But I&apos;m only looking<br/>&gt; at a simple set of options:<br/>&gt;<br/>&gt; Tenney weighting vs no weighting<br/>&gt; octave equivalent vs octave specific<br/>&gt; minimax vs rms<br/>&gt; primes vs interval sets<br/>&gt;<br/>&gt; These are the simple things you can do to calculate the tuning<br/>damage.<br/>&gt;  I don&apos;t think the basic theory&apos;s solid enough to justify more<br/>&gt; complicated methods.  So as Tenney weighting goes the wrong way,</p><p>The wrong way??</p><p>&gt; we<br/>&gt; use no weighting.  There&apos;s got to be some other weighting which is<br/>&gt; better, but I don&apos;t know what it is, and it&apos;s all going to depend on<br/>&gt; context anyway.<br/>&gt;<br/>&gt; &gt; &gt; Everything&apos;s hand-waving.  What else do we have?  The idea is<br/>that<br/>&gt; &gt; the<br/>&gt; &gt; &gt; dissonance of the chord<br/>&gt; &gt;<br/>&gt; &gt; What&apos;s &quot;the chord&quot;?<br/>&gt;<br/>&gt; Whatever chord you want to find the dissonance of.<br/>&gt;<br/>&gt; &gt; &gt; is determined by the most dissonant<br/>&gt; &gt; &gt; constituent interval.<br/>&gt; &gt;<br/>&gt; &gt; That&apos;s &quot;the idea&quot; of what?<br/>&gt;<br/>&gt; The idea of odd limits being a measure of dissonance.</p><p>It is? I don&apos;t see it that way at all. Why do you say this?</p><p>&gt; &gt; &gt; That&apos;s what led to odd limits in the first<br/>&gt; &gt; &gt; place.<br/>&gt; &gt;<br/>&gt; &gt; When we use RMS or MAD with odd limits, how are we following the<br/>idea<br/>&gt; &gt; that the dissonance of &quot;the chord&quot; is determined by the most<br/>&gt; &gt; dissonant constituent interval?<br/>&gt;<br/>&gt; We aren&apos;t for RMS.</p><p>Oh, so you&apos;re talking specifically about the use of minimax in an odd<br/>limit here?</p><p>&gt; I don&apos;t know what MAD is.  Maximum absolute<br/>&gt; deviation?</p><p>Mean absolute deviation. What you said sounds like minimax.</p><p>&gt; In that case,</p><p>In the minimax case then?</p><p>&gt; it assumes that there&apos;s a particular<br/>&gt; mistuning beyond which an interval becomes unrecognizable,</p><p>It does? I think I&apos;ve lost your reasoning. Can you clarify?</p><p>&gt; like<br/>&gt; there&apos;s a particular complexity beyond which the ratio becomes<br/>&gt; unrecognizable.<br/>&gt;<br/>&gt;<br/>&gt;                        Graham<br/>&gt;</p></div><h3><a id=13557 href="#13557">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>11/30/2005 3:43:39 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt;<br/>wrote:<br/>&gt;<br/>&gt; &gt; See below. I think problem is that the minimax Kees result is not<br/>&gt; &gt; unique. So I&apos;m beginning to wonder if your statement that for<br/>some<br/>&gt; &gt; temperaments, a minimax Kees tuning cannot be a stretched or<br/>&gt; &gt; compressed TOP tuning is incorrect.<br/>&gt;<br/>&gt; Usually it is unique, and that&apos;s how you find examples.</p><p>Are there any 7-limit examples?</p><p>&gt; &gt; But we have a convention for non-unique TOP tunings which is to<br/>state<br/>&gt; &gt; the tuning that acheives the minimax over the subset of ratios<br/>whose<br/>&gt; &gt; tuning can vary without changing the overall minimax. Can we<br/>apply<br/>&gt; &gt; this sort of convention for minimax Kees as well?<br/>&gt;<br/>&gt; That&apos;s certainly a plan.</p><p>I think that would be useful for the list of systems I gave in this<br/>thread (same as the list in my paper). Is it something you could try<br/>calculating?</p></div>