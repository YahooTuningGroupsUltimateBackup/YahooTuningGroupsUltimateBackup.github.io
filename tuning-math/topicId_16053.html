<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning-math Scalar Complexity</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning-math">back to list</a><h1>Scalar Complexity</h1><h3><a id=16053 href="#16053">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/4/2007 1:51:28 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>After I posted the latest version of my Prime Errors and Complexities<br/>paper, which&apos;ll be in the files section of this list, I noted that one<br/>equation implied a new complexity measure.  After much pondering I&apos;ve<br/>decided it&apos;s the best candidate for a standard weighted complexity<br/>measure.  I&apos;m moving my scripts to use it as a default.</p><p>The general formula is</p><p>sqrt(abs(square(M)))/n</p><p>where sqrt is the square root, abs is a way of getting a positive<br/>number from whatever objects you&apos;re using, square is a way of squaring<br/>whatever objects, you&apos;re using, M is a weighted representation of a<br/>regular temperament, and n is the number of primes you&apos;re<br/>approximating.</p><p>As it came up before, M is a matrix containing the Tenney-weighted<br/>mapping where each column is the weighted mapping of an equal<br/>temperament.  In matrix terms, the formula becomes</p><p>sqrt(abs(det(trans(M)*M)))/n</p><p>where abs is the normal absolute value of a real number, det is the<br/>determinant, trans is the transpose, and * is the matrix product.  The<br/>most obvious way of getting a real number to represent a rectangular<br/>matrix is det(trans(M)*M).  (You could do the multiplication the other<br/>way round, and maybe it makes a difference, but in context it has to<br/>be this way round.)  Because the size should be an absolute value, and<br/>the sign of the determinant is fairly arbitrary, remove it.  Then,<br/>because the original matrix was squared, take the square root to give<br/>the result dimensions of a mapping.  Dividing by n turns the implied<br/>sums into means.</p><p>M can also be the weighted wedgie of the temperament, as a<br/>multivector.  In that case the formula is</p><p>sqrt(abs(comp(M)^M))/n</p><p>where comp is the complement and ^ is the wedge product.  In Grassman<br/>algebra, comp(M)^M is the scalar product of M with itself, hence I&apos;m<br/>calling this scalar complexity for now.  This is the most obvious way<br/>of getting the size of a multivector in Grassman algebra.  It either<br/>implies a Euclidian metric for the weighted wedgie, or means the Tenny<br/>weights serve as the metric for the unweighted wedgie.  The abs, sqrt<br/>and division follow the same logic as for the matrices above.</p><p>I don&apos;t know if this has been proposed before.  I remember thinking<br/>that geometric complexity, way back when, *should* be similar to this.<br/>As I never understood it I can&apos;t be sure if it was or not.  We<br/>generally shied away from this obvious wedgie complexity because we<br/>don&apos;t like Euclidian distances in music space.  But I was never<br/>convinced that max-abs, sum-abs, or standard deviations were valid.<br/>Now that I&apos;ve tied this way in with other, non-wedgie complexities I&apos;m<br/>happy to say that it&apos;s the right way to go.</p><p>Scalar complexity is very close to the standard deviation complexity I<br/>show in the PDF file.  Scalar complexity times TOP-RMS error gives the<br/>same simple badness as STD complexity times Tenney-weighted STD error.<br/>That means scalar complexity is as close to STD complexity as STD<br/>error is to the true RMS error, which means 3 significant figures for<br/>common cases like miracle and mystery.  The smaller the error in the<br/>temperament, the closer these pairs of complexities and errors get to<br/>each other.</p><p>The Tenney-weighted STD complexity is always smaller than half the<br/>Kees-max complexity.  Because the scalar complexity is so close to the<br/>STD complexity, it should also be smaller than the Kees-max<br/>complexity.  I can&apos;t prove it always is, but it should be useful as an<br/>approximation if you want to filter for the Kees-max.  Because of this<br/>relationship, I&apos;m going to call for the Kees-max complexity to be<br/>divided by two as standard so that it looks more like scalar<br/>complexity.  The more primes you look at the smaller the average case<br/>tends to be compared to the worst case.</p><p>It&apos;s possible, though not very elegant, to express scalar complexity<br/>using the same means, variances and covariance as the TOP-RMS error,<br/>STD complexity, and so on.  That means it&apos;s very efficient to<br/>calculate if you keep using the same equal temperaments --- order n<br/>for n primes.  It&apos;s also efficient to update if you want to keep<br/>extending the prime limit.  All you do is update each of the sums of<br/>products of weighted mapping elements in the RxR (for a rank R<br/>temperament) matrix.</p><p>Compared to the Kees-max and STD complexities, the advantages of<br/>scalar complexity are that it doesn&apos;t depend on the choice of<br/>equivalence interval and can be generalized to any rank of<br/>temperament.  For an equal temperament, the scalar complexity is the<br/>RMS of the weighted mapping, which will always be close to the number<br/>of notes to the octave (when the mapping&apos;s in terms of octaves).  For<br/>a temperament with a single unison vector, it&apos;ll be proportional to<br/>the RMS of the Tenney-weighted prime factorization of that unison<br/>vector, the same way that the sum-abs of the weighted wedgie is<br/>proportional to the Tenney-size of the unison vector.</p><p>It does imply an RMS, rather than a minimax, outlook.  I still think<br/>this is they right way of treating weighting because the set of<br/>intervals is unbounded, so you can throw away whichever ones you like.<br/>Knowing the average badness is more informative than the worst<br/>badness.  But even then I don&apos;t know of a minimax equivalent that<br/>works as well as this, so I&apos;m hoping weighted minimax folks can still<br/>use this scalar complexity as the default.</p><p>                              Graham</p></div><h3><a id=16054 href="#16054">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/4/2007 12:41:55 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I understand weighted error.  What&apos;s weighted complexity?</p><p>-Carl</p><p>At 01:51 AM 2/4/2007, you wrote:<br/>&gt;After I posted the latest version of my Prime Errors and Complexities<br/>&gt;paper, which&apos;ll be in the files section of this list, I noted that one<br/>&gt;equation implied a new complexity measure.  After much pondering I&apos;ve<br/>&gt;decided it&apos;s the best candidate for a standard weighted complexity<br/>&gt;measure.  I&apos;m moving my scripts to use it as a default.<br/>&gt;<br/>&gt;The general formula is<br/>&gt;<br/>&gt;sqrt(abs(square(M)))/n<br/>&gt;<br/>&gt;where sqrt is the square root, abs is a way of getting a positive<br/>&gt;number from whatever objects you&apos;re using, square is a way of squaring<br/>&gt;whatever objects, you&apos;re using, M is a weighted representation of a<br/>&gt;regular temperament, and n is the number of primes you&apos;re<br/>&gt;approximating.<br/>&gt;<br/>&gt;As it came up before, M is a matrix containing the Tenney-weighted<br/>&gt;mapping where each column is the weighted mapping of an equal<br/>&gt;temperament.  In matrix terms, the formula becomes<br/>&gt;<br/>&gt;sqrt(abs(det(trans(M)*M)))/n<br/>&gt;<br/>&gt;where abs is the normal absolute value of a real number, det is the<br/>&gt;determinant, trans is the transpose, and * is the matrix product.  The<br/>&gt;most obvious way of getting a real number to represent a rectangular<br/>&gt;matrix is det(trans(M)*M).  (You could do the multiplication the other<br/>&gt;way round, and maybe it makes a difference, but in context it has to<br/>&gt;be this way round.)  Because the size should be an absolute value, and<br/>&gt;the sign of the determinant is fairly arbitrary, remove it.  Then,<br/>&gt;because the original matrix was squared, take the square root to give<br/>&gt;the result dimensions of a mapping.  Dividing by n turns the implied<br/>&gt;sums into means.<br/>&gt;<br/>&gt;M can also be the weighted wedgie of the temperament, as a<br/>&gt;multivector.  In that case the formula is<br/>&gt;<br/>&gt;sqrt(abs(comp(M)^M))/n<br/>&gt;<br/>&gt;where comp is the complement and ^ is the wedge product.  In Grassman<br/>&gt;algebra, comp(M)^M is the scalar product of M with itself, hence I&apos;m<br/>&gt;calling this scalar complexity for now.  This is the most obvious way<br/>&gt;of getting the size of a multivector in Grassman algebra.  It either<br/>&gt;implies a Euclidian metric for the weighted wedgie, or means the Tenny<br/>&gt;weights serve as the metric for the unweighted wedgie.  The abs, sqrt<br/>&gt;and division follow the same logic as for the matrices above.<br/>&gt;<br/>&gt;I don&apos;t know if this has been proposed before.  I remember thinking<br/>&gt;that geometric complexity, way back when, *should* be similar to this.<br/>&gt; As I never understood it I can&apos;t be sure if it was or not.  We<br/>&gt;generally shied away from this obvious wedgie complexity because we<br/>&gt;don&apos;t like Euclidian distances in music space.  But I was never<br/>&gt;convinced that max-abs, sum-abs, or standard deviations were valid.<br/>&gt;Now that I&apos;ve tied this way in with other, non-wedgie complexities I&apos;m<br/>&gt;happy to say that it&apos;s the right way to go.<br/>&gt;<br/>&gt;Scalar complexity is very close to the standard deviation complexity I<br/>&gt;show in the PDF file.  Scalar complexity times TOP-RMS error gives the<br/>&gt;same simple badness as STD complexity times Tenney-weighted STD error.<br/>&gt; That means scalar complexity is as close to STD complexity as STD<br/>&gt;error is to the true RMS error, which means 3 significant figures for<br/>&gt;common cases like miracle and mystery.  The smaller the error in the<br/>&gt;temperament, the closer these pairs of complexities and errors get to<br/>&gt;each other.<br/>&gt;<br/>&gt;The Tenney-weighted STD complexity is always smaller than half the<br/>&gt;Kees-max complexity.  Because the scalar complexity is so close to the<br/>&gt;STD complexity, it should also be smaller than the Kees-max<br/>&gt;complexity.  I can&apos;t prove it always is, but it should be useful as an<br/>&gt;approximation if you want to filter for the Kees-max.  Because of this<br/>&gt;relationship, I&apos;m going to call for the Kees-max complexity to be<br/>&gt;divided by two as standard so that it looks more like scalar<br/>&gt;complexity.  The more primes you look at the smaller the average case<br/>&gt;tends to be compared to the worst case.<br/>&gt;<br/>&gt;It&apos;s possible, though not very elegant, to express scalar complexity<br/>&gt;using the same means, variances and covariance as the TOP-RMS error,<br/>&gt;STD complexity, and so on.  That means it&apos;s very efficient to<br/>&gt;calculate if you keep using the same equal temperaments --- order n<br/>&gt;for n primes.  It&apos;s also efficient to update if you want to keep<br/>&gt;extending the prime limit.  All you do is update each of the sums of<br/>&gt;products of weighted mapping elements in the RxR (for a rank R<br/>&gt;temperament) matrix.<br/>&gt;<br/>&gt;Compared to the Kees-max and STD complexities, the advantages of<br/>&gt;scalar complexity are that it doesn&apos;t depend on the choice of<br/>&gt;equivalence interval and can be generalized to any rank of<br/>&gt;temperament.  For an equal temperament, the scalar complexity is the<br/>&gt;RMS of the weighted mapping, which will always be close to the number<br/>&gt;of notes to the octave (when the mapping&apos;s in terms of octaves).  For<br/>&gt;a temperament with a single unison vector, it&apos;ll be proportional to<br/>&gt;the RMS of the Tenney-weighted prime factorization of that unison<br/>&gt;vector, the same way that the sum-abs of the weighted wedgie is<br/>&gt;proportional to the Tenney-size of the unison vector.<br/>&gt;<br/>&gt;It does imply an RMS, rather than a minimax, outlook.  I still think<br/>&gt;this is they right way of treating weighting because the set of<br/>&gt;intervals is unbounded, so you can throw away whichever ones you like.<br/>&gt; Knowing the average badness is more informative than the worst<br/>&gt;badness.  But even then I don&apos;t know of a minimax equivalent that<br/>&gt;works as well as this, so I&apos;m hoping weighted minimax folks can still<br/>&gt;use this scalar complexity as the default.<br/>&gt;<br/>&gt;<br/>&gt;                               Graham</p></div><h3><a id=16056 href="#16056">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/4/2007 11:04:03 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 05/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; I understand weighted error.  What&apos;s weighted complexity?</p><p>Like normal complexity but weighted.  The odd limit complexity tells<br/>you how many tempered notes you need for a complete chord.  Weighted<br/>complexity is some indication of the number of tempered notes you need<br/>to get approximate arbitrarily complex ratios.  For weighted error you<br/>weight, well, the error.  For weighted complexity you weight the<br/>number of steps.</p><p>                           Graham</p></div><h3><a id=16058 href="#16058">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/5/2007 12:23:28 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; I understand weighted error.  What&apos;s weighted complexity?<br/>&gt;<br/>&gt;Like normal complexity but weighted.  The odd limit complexity tells<br/>&gt;you how many tempered notes you need for a complete chord.  Weighted<br/>&gt;complexity is some indication of the number of tempered notes you need<br/>&gt;to get approximate arbitrarily complex ratios.  For weighted error you<br/>&gt;weight, well, the error.  For weighted complexity you weight the<br/>&gt;number of steps.<br/>&gt;                            Graham</p><p>Why should it be &quot;harder&quot; to approximate 11 than 7?  Or are<br/>you just weighting for the number of notes in a complete chord?</p><p>-Carl</p></div><h3><a id=16063 href="#16063">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/6/2007 2:33:07 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 05/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; &gt;&gt; I understand weighted error.  What&apos;s weighted complexity?<br/>&gt; &gt;<br/>&gt; &gt;Like normal complexity but weighted.  The odd limit complexity tells<br/>&gt; &gt;you how many tempered notes you need for a complete chord.  Weighted<br/>&gt; &gt;complexity is some indication of the number of tempered notes you need<br/>&gt; &gt;to get approximate arbitrarily complex ratios.  For weighted error you<br/>&gt; &gt;weight, well, the error.  For weighted complexity you weight the<br/>&gt; &gt;number of steps.<br/>&gt;<br/>&gt; Why should it be &quot;harder&quot; to approximate 11 than 7?  Or are<br/>&gt; you just weighting for the number of notes in a complete chord?</p><p>I&apos;m assuming you want simple ratios to approximate in a simple way.<br/>It&apos;s always going to be harder to approximate 9 than 3 with a<br/>prime-based regular temperament.  All Tenney weighting does is treat<br/>all intervals equally in this respect.  It may not be harder to<br/>approximate 11 -- the easier the better.  There&apos;s nothing to penalize<br/>an approximation for being too good.</p><p>It&apos;s nothing to do with complete chords.  Prime limits don&apos;t have<br/>complete chords anyway.  I divide through by the number of primes so<br/>they don&apos;t affect it.  One of the nice things about this measure is<br/>that it&apos;s roughly the same if you randomly add or remove a prime.<br/>It&apos;s about the average complexity of an interval.  Although the RMS is<br/>really a compromise between mean-abs (the most obvious kind of<br/>average) and max-abs.</p><p>I thought of maybe &quot;quadratic complexity&quot; instead of &quot;scalar<br/>complexity&quot; as it&apos;s always about squaring things.</p><p>                          Graham</p></div><h3><a id=16064 href="#16064">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/6/2007 2:55:40 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>At 02:33 AM 2/6/2007, you wrote:<br/>&gt;On 05/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt;&gt; &gt;&gt; I understand weighted error.  What&apos;s weighted complexity?<br/>&gt;&gt; &gt;<br/>&gt;&gt; &gt;Like normal complexity but weighted.  The odd limit complexity tells<br/>&gt;&gt; &gt;you how many tempered notes you need for a complete chord.  Weighted<br/>&gt;&gt; &gt;complexity is some indication of the number of tempered notes you need<br/>&gt;&gt; &gt;to get approximate arbitrarily complex ratios.  For weighted error you<br/>&gt;&gt; &gt;weight, well, the error.  For weighted complexity you weight the<br/>&gt;&gt; &gt;number of steps.<br/>&gt;&gt;<br/>&gt;&gt; Why should it be &quot;harder&quot; to approximate 11 than 7?  Or are<br/>&gt;&gt; you just weighting for the number of notes in a complete chord?<br/>&gt;<br/>&gt;I&apos;m assuming you want simple ratios to approximate in a simple way.</p><p>Aha; I see.</p><p>&gt;It&apos;s always going to be harder to approximate 9 than 3 with a<br/>&gt;prime-based regular temperament.  All Tenney weighting does is treat<br/>&gt;all intervals equally in this respect.  It may not be harder to<br/>&gt;approximate 11 -- the easier the better.  There&apos;s nothing to penalize<br/>&gt;an approximation for being too good.</p><p>Well, you&apos;re penalizing temperaments that are good at<br/>1:3:7 chords but bad at 1:3:5 chords -- correct?  Not a bad<br/>assumption at the end of the day.  A better one, from my<br/>point of view, than equal-weighted complexity for complete<br/>chords.</p><p>&gt;It&apos;s nothing to do with complete chords.  Prime limits don&apos;t have<br/>&gt;complete chords anyway.</p><p>Is this prime limit, then?  I seem to remember you thinking<br/>considering error of primes individually was a good proxy for<br/>the complete chord method.</p><p>-Carl</p></div><h3><a id=16070 href="#16070">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/6/2007 9:25:49 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 06/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; At 02:33 AM 2/6/2007, you wrote:<br/>&gt; &gt;On 05/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:<br/>&gt; &gt;&gt; &gt;&gt; I understand weighted error.  What&apos;s weighted complexity?<br/>&gt; &gt;&gt; &gt;<br/>&gt; &gt;&gt; &gt;Like normal complexity but weighted.  The odd limit complexity tells<br/>&gt; &gt;&gt; &gt;you how many tempered notes you need for a complete chord.  Weighted<br/>&gt; &gt;&gt; &gt;complexity is some indication of the number of tempered notes you need<br/>&gt; &gt;&gt; &gt;to get approximate arbitrarily complex ratios.  For weighted error you<br/>&gt; &gt;&gt; &gt;weight, well, the error.  For weighted complexity you weight the<br/>&gt; &gt;&gt; &gt;number of steps.<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Why should it be &quot;harder&quot; to approximate 11 than 7?  Or are<br/>&gt; &gt;&gt; you just weighting for the number of notes in a complete chord?<br/>&gt; &gt;<br/>&gt; &gt;I&apos;m assuming you want simple ratios to approximate in a simple way.<br/>&gt;<br/>&gt; Aha; I see.<br/>&gt;<br/>&gt; &gt;It&apos;s always going to be harder to approximate 9 than 3 with a<br/>&gt; &gt;prime-based regular temperament.  All Tenney weighting does is treat<br/>&gt; &gt;all intervals equally in this respect.  It may not be harder to<br/>&gt; &gt;approximate 11 -- the easier the better.  There&apos;s nothing to penalize<br/>&gt; &gt;an approximation for being too good.<br/>&gt;<br/>&gt; Well, you&apos;re penalizing temperaments that are good at<br/>&gt; 1:3:7 chords but bad at 1:3:5 chords -- correct?  Not a bad<br/>&gt; assumption at the end of the day.  A better one, from my<br/>&gt; point of view, than equal-weighted complexity for complete<br/>&gt; chords.</p><p>Relative to equal weighting, yes.  Of course the weights of 5 and 7<br/>don&apos;t differ greatly.  It&apos;s more a case of 1:3:5 compared to 1:11:13.</p><p>I still think equal-weighted complexity for complete chords makes<br/>sense if you&apos;re actually planning to use compete chords and you want<br/>as many of them as possible.  The prime-based Tenney-weighted RMS is<br/>never likely to be what you want, but close enough that it&apos;s a good<br/>standard.  We&apos;re all going to want different things in different<br/>contexts, and most of the time don&apos;t even know what we want, so<br/>standards are certainly useful here.</p><p>&gt; &gt;It&apos;s nothing to do with complete chords.  Prime limits don&apos;t have<br/>&gt; &gt;complete chords anyway.<br/>&gt;<br/>&gt; Is this prime limit, then?  I seem to remember you thinking<br/>&gt; considering error of primes individually was a good proxy for<br/>&gt; the complete chord method.</p><p>Yes, this is prime limit.</p><p>If you&apos;re only going to look at primes, Tenney weighting gets you<br/>closest to odd limits.  You also need to consider the intervals<br/>between primes in some way.  For the error you can handle that by<br/>optimizing the scale stretch.  But using a standard deviation(STD)<br/>instead of an RMS does the same job if you want to keep pure octaves<br/>(or have some other rule for setting the scale stretch).  I&apos;m still<br/>not entirely clear why STDs work for complexity as well, but they<br/>clearly do.  It&apos;s nice that they&apos;re always less than half the max Kees<br/>complexity, which I do understand.  But the standard deviation is<br/>supposed to be the RMS deviation from the mean and I don&apos;t see that<br/>the mean means anything in this context.  I don&apos;t see why a<br/>determinant or wedgie scalar product should consider intervals between<br/>primes either but they obviously do because the result is extremely<br/>close to the standard deviation.</p><p>For actual comparisons with complete chords, worst weighted errors and<br/>complexities will get you closer than RMS/STD.  I prefer the RMS/STD<br/>partly because it gets closer to the average case (but also for<br/>pragmatic reasons).  This is especially true for higher prime limits.<br/>There are certainly people out there who want to use 19-limit<br/>intervals in a context where they could be tempered.  But 19-limit<br/>complete chords?  I think not!  I can even see a point to my 31-limit<br/>searches if you&apos;re going to throw away the intervals that are too<br/>complex or too far from JI.  One reason for stopping at 31 is that<br/>it&apos;s the highest prime Ptolemy used IIRC.</p><p>Ultimately, the software should be able to find useful subsets of a<br/>prime limit and give you the worst error and complexity (weighted or<br/>otherwise) for that subset.  I can also see an advantage in the<br/>error-weighting following the complexity of the tempered interval<br/>rather than the ratio being approximated.  But these are all<br/>complications --- for now, I&apos;m concentrating on getting the weighted<br/>primes worked out, implemented and documented.</p><p>                                    Graham</p></div><h3><a id=16072 href="#16072">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/7/2007 10:05:22 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; Well, you&apos;re penalizing temperaments that are good at<br/>&gt;&gt; 1:3:7 chords but bad at 1:3:5 chords -- correct?  Not a bad<br/>&gt;&gt; assumption at the end of the day.  A better one, from my<br/>&gt;&gt; point of view, than equal-weighted complexity for complete<br/>&gt;&gt; chords.<br/>&gt;<br/>&gt;Relative to equal weighting, yes.  Of course the weights of 5 and 7<br/>&gt;don&apos;t differ greatly.  It&apos;s more a case of 1:3:5 compared to 1:11:13.<br/>&gt;<br/>&gt;I still think equal-weighted complexity for complete chords makes<br/>&gt;sense if you&apos;re actually planning to use compete chords and you want<br/>&gt;as many of them as possible.</p><p>Yes.</p><p>&gt;The prime-based Tenney-weighted RMS is<br/>&gt;never likely to be what you want, but close enough that it&apos;s a good<br/>&gt;standard.  We&apos;re all going to want different things in different<br/>&gt;contexts, and most of the time don&apos;t even know what we want, so<br/>&gt;standards are certainly useful here.</p><p>That&apos;s why I like my idea of finding the best chords to use<br/>with each ET.</p><p>&gt;If you&apos;re only going to look at primes, Tenney weighting gets you<br/>&gt;closest to odd limits.  You also need to consider the intervals<br/>&gt;between primes in some way.  For the error you can handle that by<br/>&gt;optimizing the scale stretch.</p><p>In the past people suggested doing it by paying attention to<br/>the signs of the errors.</p><p>Scale stretch??</p><p>&gt;For actual comparisons with complete chords, worst weighted errors and<br/>&gt;complexities will get you closer than RMS/STD.  I prefer the RMS/STD<br/>&gt;partly because it gets closer to the average case (but also for<br/>&gt;pragmatic reasons).  This is especially true for higher prime limits.<br/>&gt;There are certainly people out there who want to use 19-limit<br/>&gt;intervals in a context where they could be tempered.  But 19-limit<br/>&gt;complete chords?  I think not!</p><p>Why not?  Denny and I use to play 21-limit marimbas and stuff.</p><p>&gt;But these are all<br/>&gt;complications --- for now, I&apos;m concentrating on getting the weighted<br/>&gt;primes worked out, implemented and documented.</p><p>Very good then.</p><p>-Carl</p></div><h3><a id=16073 href="#16073">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/7/2007 11:16:18 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 08/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:</p><p>&gt; &gt;If you&apos;re only going to look at primes, Tenney weighting gets you<br/>&gt; &gt;closest to odd limits.  You also need to consider the intervals<br/>&gt; &gt;between primes in some way.  For the error you can handle that by<br/>&gt; &gt;optimizing the scale stretch.<br/>&gt;<br/>&gt; In the past people suggested doing it by paying attention to<br/>&gt; the signs of the errors.</p><p>Did they have any concrete suggestions, or was it only a case of &quot;hey,<br/>why not look at the signs of the errors?&quot;</p><p>&gt; Scale stretch??</p><p>Yes, if you treat octaves on an equal footing with the other primes<br/>and optimize for them you get the right results from a naive way of<br/>looking at the primes.  I explained it all in the Prime Errors and<br/>Complexities PDF so that I don&apos;t have to keep on doing so.  This is<br/>probably why the new complexity measure works as well -- it uses the<br/>whole definition of the temperament instead of treating octaves as<br/>special.</p><p>&gt; &gt;For actual comparisons with complete chords, worst weighted errors and<br/>&gt; &gt;complexities will get you closer than RMS/STD.  I prefer the RMS/STD<br/>&gt; &gt;partly because it gets closer to the average case (but also for<br/>&gt; &gt;pragmatic reasons).  This is especially true for higher prime limits.<br/>&gt; &gt;There are certainly people out there who want to use 19-limit<br/>&gt; &gt;intervals in a context where they could be tempered.  But 19-limit<br/>&gt; &gt;complete chords?  I think not!<br/>&gt;<br/>&gt; Why not?  Denny and I use to play 21-limit marimbas and stuff.</p><p>So one of you had 5 mallets and the other 6 to hit a complete chord between you?</p><p>                                 Graham</p></div><h3><a id=16074 href="#16074">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/8/2007 10:06:58 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; &gt;If you&apos;re only going to look at primes, Tenney weighting gets you<br/>&gt;&gt; &gt;closest to odd limits.  You also need to consider the intervals<br/>&gt;&gt; &gt;between primes in some way.  For the error you can handle that by<br/>&gt;&gt; &gt;optimizing the scale stretch.<br/>&gt;&gt;<br/>&gt;&gt; In the past people suggested doing it by paying attention to<br/>&gt;&gt; the signs of the errors.<br/>&gt;<br/>&gt;Did they have any concrete suggestions, or was it only a case of &quot;hey,<br/>&gt;why not look at the signs of the errors?&quot;</p><p>I don&apos;t really know what you&apos;re talking about, so there&apos;s no point<br/>of me going into detail yet.</p><p>&gt;&gt; Scale stretch??<br/>&gt;<br/>&gt;Yes, if you treat octaves on an equal footing with the other primes<br/>&gt;and optimize for them you get the right results from a naive way of<br/>&gt;looking at the primes.  I explained it all in the Prime Errors and<br/>&gt;Complexities PDF so that I don&apos;t have to keep on doing so.  This is<br/>&gt;probably why the new complexity measure works as well -- it uses the<br/>&gt;whole definition of the temperament instead of treating octaves as<br/>&gt;special.</p><p>I&apos;m lost.  You lost me in the PDF, too.  But I guess I can try<br/>again.  Do you have a url for it these days?</p><p>&gt;&gt; &gt;For actual comparisons with complete chords, worst weighted errors and<br/>&gt;&gt; &gt;complexities will get you closer than RMS/STD.  I prefer the RMS/STD<br/>&gt;&gt; &gt;partly because it gets closer to the average case (but also for<br/>&gt;&gt; &gt;pragmatic reasons).  This is especially true for higher prime limits.<br/>&gt;&gt; &gt;There are certainly people out there who want to use 19-limit<br/>&gt;&gt; &gt;intervals in a context where they could be tempered.  But 19-limit<br/>&gt;&gt; &gt;complete chords?  I think not!<br/>&gt;&gt;<br/>&gt;&gt; Why not?  Denny and I use to play 21-limit marimbas and stuff.<br/>&gt;<br/>&gt;So one of you had 5 mallets and the other 6 to hit a complete chord<br/>&gt;between you?</p><p>We had four mallets between the two of us, and the sustain and<br/>resonance in the instrument was plenty to consider something<br/>like 19-limit error.  As I should think it would be in any music<br/>that uses the harmonic series as a scale.</p><p>-Carl</p></div><h3><a id=16077 href="#16077">ðŸ”—</a>Graham Breed &#x3C;gbreed@gmail.com&#x3E;</h3><span>2/9/2007 3:16:07 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On 09/02/07, Carl Lumma &lt;<a href="mailto:ekin@lumma.org">ekin@lumma.org</a>&gt; wrote:</p><p>&gt; &gt;&gt; Scale stretch??<br/>&gt; &gt;<br/>&gt; &gt;Yes, if you treat octaves on an equal footing with the other primes<br/>&gt; &gt;and optimize for them you get the right results from a naive way of<br/>&gt; &gt;looking at the primes.  I explained it all in the Prime Errors and<br/>&gt; &gt;Complexities PDF so that I don&apos;t have to keep on doing so.  This is<br/>&gt; &gt;probably why the new complexity measure works as well -- it uses the<br/>&gt; &gt;whole definition of the temperament instead of treating octaves as<br/>&gt; &gt;special.<br/>&gt;<br/>&gt; I&apos;m lost.  You lost me in the PDF, too.  But I guess I can try<br/>&gt; again.  Do you have a url for it these days?</p><p>No.  There&apos;s a copy in the files section here and I think you have it.</p><p>Not that I can remember what I said there, but I don&apos;t think I can<br/>make it any clearer.  You&apos;re going to have to think it through.  It&apos;s<br/>a simple idea but it has to click with you.</p><p>&gt; &gt;&gt; &gt;For actual comparisons with complete chords, worst weighted errors and<br/>&gt; &gt;&gt; &gt;complexities will get you closer than RMS/STD.  I prefer the RMS/STD<br/>&gt; &gt;&gt; &gt;partly because it gets closer to the average case (but also for<br/>&gt; &gt;&gt; &gt;pragmatic reasons).  This is especially true for higher prime limits.<br/>&gt; &gt;&gt; &gt;There are certainly people out there who want to use 19-limit<br/>&gt; &gt;&gt; &gt;intervals in a context where they could be tempered.  But 19-limit<br/>&gt; &gt;&gt; &gt;complete chords?  I think not!<br/>&gt; &gt;&gt;<br/>&gt; &gt;&gt; Why not?  Denny and I use to play 21-limit marimbas and stuff.<br/>&gt; &gt;<br/>&gt; &gt;So one of you had 5 mallets and the other 6 to hit a complete chord<br/>&gt; &gt;between you?<br/>&gt;<br/>&gt; We had four mallets between the two of us, and the sustain and<br/>&gt; resonance in the instrument was plenty to consider something<br/>&gt; like 19-limit error.  As I should think it would be in any music<br/>&gt; that uses the harmonic series as a scale.</p><p>Would there be any point in tempering it?  It&apos;s something to consider anyway.<br/>And the odd-limit search does work up to the 21-limit.</p><p>                         Graham</p></div><h3><a id=16078 href="#16078">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>2/9/2007 8:13:04 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt; We had four mallets between the two of us, and the sustain and<br/>&gt;&gt; resonance in the instrument was plenty to consider something<br/>&gt;&gt; like 19-limit error.  As I should think it would be in any music<br/>&gt;&gt; that uses the harmonic series as a scale.<br/>&gt;<br/>&gt;Would there be any point in tempering it?</p><p>Not really, no.  I guess you&apos;ve got me there.</p><p>-Carl</p></div><h3><a id=16079 href="#16079">ðŸ”—</a>Dan Amateur &#x3C;xamateur_dan@yahoo.ca&#x3E;</h3><span>2/11/2007 8:58:06 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Saw this on another list, looks interesting and<br/>familiar but can&apos;t place it?</p><p>Anyone out there have any ideas?</p><p>To: <a href="mailto:MakeMicroMusic@yahoogroups.com">MakeMicroMusic@yahoogroups.com</a><br/>From: &quot;dar kone&quot; &lt;<a href="mailto:zarkorgon@yahoo.com">zarkorgon@yahoo.com</a>&gt;  Add to Address<br/>Book  Add Mobile Alert<br/>Date: Sun, 11 Feb 2007 20:15:46 -0800 (PST)<br/>Subject: [MMM] Weird Frequency Phenomenon</p><p>    Help, can someone explain how and what is<br/>happening in the below?</p><p>Is there some standard musical, math explanation for<br/>this?</p><p>Frequency #1  Reciprocal of Freq 1 *2<br/>------------ --------- --------- --------- -<br/>1.851351      1.080292</p><p>Frequency #2    Reciprocal of Freq 1 *2<br/>------------ --------- --------- --------- -<br/>-1.167568       -1.712963</p><p>Frequency #1 / Reciprocal of Freq 1 *2<br/>------------ --------- --------- --------- ----<br/>1.851351    / -1.712963 = -1.080789</p><p>-1.080789 = Close to Reciprocal of Freq # 1</p><p>Reciprocal of Freq 1 *2   / Frequency #2<br/>------------ --------- --------- --------- ----<br/>1.080292               / -1.167568 = -1.850501</p><p>-1.850501 = Close to Freq # 1</p><p>__________________________________________________<br/>Do You Yahoo!?<br/>Tired of spam?  Yahoo! Mail has the best spam protection around<br/><a href="http://mail.yahoo.com">http://mail.yahoo.com</a></p></div><h3><a id=16082 href="#16082">ðŸ”—</a>Gene Ward Smith &#x3C;genewardsmith@coolgoose.com&#x3E;</h3><span>2/12/2007 3:03:33 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning-math@yahoogroups.com">tuning-math@yahoogroups.com</a>, Dan Amateur &lt;xamateur_dan@...&gt;<br/>wrote:<br/>&gt;<br/>&gt; Saw this on another list, looks interesting and<br/>&gt; familiar but can&apos;t place it?<br/>&gt;<br/>&gt; Anyone out there have any ideas?</p><p>It makes zero sense to me, sorry.</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            