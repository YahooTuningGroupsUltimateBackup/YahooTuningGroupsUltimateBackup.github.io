<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning Using adjusted error for subgroup badness calculations</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning">back to list</a><h1>Using adjusted error for subgroup badness calculations</h1><h3><a id=102529 href="#102529">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@...&#x3E;</h3><span>1/5/2012 1:03:08 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>This is a bad idea, right? Because adjusted error is just TE error<br/>multiplied by some constant proportional to the largest prime in the<br/>limit, which is why this has ~14 cents of adjusted error</p><p><a href="http://x31eq.com/cgi-bin/rt.cgi?ets=19&limit=2.3.5.128">http://x31eq.com/cgi-bin/rt.cgi?ets=19&limit=2.3.5.128</a></p><p>and this has only 4.4 cents of adjusted error</p><p><a href="http://x31eq.com/cgi-bin/rt.cgi?ets=19&limit=2.3.5">http://x31eq.com/cgi-bin/rt.cgi?ets=19&limit=2.3.5</a></p><p>despite that the TE error differs by about a tenth of a cent.</p><p>In short, the point of adjusted error is to be able to compare<br/>temperaments of different limits, but the way it&apos;s currently defined<br/>isn&apos;t useful for comparing things on different subgroups. You need a<br/>better way to assign each subgroup a scalar that&apos;s consistent across<br/>subgroups. I would suggest something like rms(weighted L2 complexity)<br/>of all the intervals in the subgroup, but that&apos;s going to weight 5/3<br/>and 15/1 the same, which you may not necessarily want. The magic<br/>bullet might be rms(weighted triangular L2 complexity of each interval<br/>in the subgroup).</p><p>I&apos;d recommend that Igs redo the results again, but instead of looking<br/>at adjusted error, to look at the actual TE error, and then multiply<br/>it by some constant that&apos;s consistent with the weighting of the<br/>subgroup.</p><p>-Mike</p></div><h3><a id=102530 href="#102530">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@...&#x3E;</h3><span>1/5/2012 1:18:53 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I think Graham just confirmed in another thread</p><p>Graham wrote:<br/>&gt; The adjusted error is the TE error multiplied by the log of the largest prime. It matches the target error.</p><p>-Mike</p><p>On Thu, Jan 5, 2012 at 4:03 PM, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; This is a bad idea, right? Because adjusted error is just TE error<br/>&gt; multiplied by some constant proportional to the largest prime in the<br/>&gt; limit, which is why this has ~14 cents of adjusted error</p></div><h3><a id=102538 href="#102538">ðŸ”—</a>cityoftheasleep &#x3C;igliashon@...&#x3E;</h3><span>1/5/2012 6:54:42 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:</p><p>&gt; I&apos;d recommend that Igs redo the results again, but instead of looking<br/>&gt; at adjusted error, to look at the actual TE error, and then multiply<br/>&gt; it by some constant that&apos;s consistent with the weighting of the<br/>&gt; subgroup.</p><p>FFFFFFFFFFFFFFFFFFFFUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU---!!!!!!!</p><p>If I give you the subgroups I&apos;m looking at, can you calculate the constants by which I&apos;ll be multiplying TE error for each ET on that subgroup?</p><p>-Igs</p></div><h3><a id=102542 href="#102542">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@...&#x3E;</h3><span>1/5/2012 7:23:33 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, Jan 5, 2012 at 9:54 PM, cityoftheasleep &lt;igliashon@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt;<br/>&gt; &gt; I&apos;d recommend that Igs redo the results again, but instead of looking<br/>&gt; &gt; at adjusted error, to look at the actual TE error, and then multiply<br/>&gt; &gt; it by some constant that&apos;s consistent with the weighting of the<br/>&gt; &gt; subgroup.<br/>&gt;<br/>&gt; FFFFFFFFFFFFFFFFFFFFUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU---!!!!!!!<br/>&gt;<br/>&gt; If I give you the subgroups I&apos;m looking at, can you calculate the constants by which I&apos;ll be multiplying TE error for each ET on that subgroup?</p><p>I&apos;m actually curious why we need to re-weight the error at all. Isn&apos;t<br/>it already weighted? Why not just use TE error by itself?</p><p>One thing that&apos;s notable about TE error is that it goes -down- as more<br/>accurate primes are added. For example, consider 31-EDO in the 5-limit</p><p><a href="http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=5">http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=5</a></p><p>1.628 cents of error. Now look at it in the 7-limit:</p><p><a href="http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=7">http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=7</a></p><p>1.432 cents, which is less.</p><p>This is because TE error is a type of average prime error. If you add<br/>another, really accurate prime into the mix, the &quot;average&quot; error goes<br/>down. Multiplication by the largest prime undoes some of this, but so<br/>would just looking at something like weighted sum-squared error<br/>instead of weighted RMS error.</p><p>So what exactly is the behavior you&apos;re looking for? Should 7-limit<br/>31-EDO be lower in error than 5-limit 31-EDO?</p><p>-Mike</p></div><h3><a id=102548 href="#102548">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>1/5/2012 8:20:12 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Graham wrote:<br/>&gt; The adjusted error is the TE error multiplied by the log of<br/>&gt; the largest prime.  It matches the target error.</p><p>Mike wrote:<br/>&gt; This is a bad idea, right?</p><p>Yes, sure sounds like it&apos;ll screw up Igs&apos; particular search<br/>big-time.</p><p>-Carl</p></div><h3><a id=102552 href="#102552">ðŸ”—</a>cityoftheasleep &#x3C;igliashon@...&#x3E;</h3><span>1/5/2012 9:38:28 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Mike Battaglia &lt;battaglia01@...&gt; wrote:<br/>&gt; I&apos;m actually curious why we need to re-weight the error at all. Isn&apos;t<br/>&gt; it already weighted? Why not just use TE error by itself?</p><p>I think I need an re-education on this whole &quot;error&quot; thing.  Is it correct that error is weighted because, for a fixed amount of error, there is a larger relative increase in discordance for that error on a simple ratio vs. a more complex one?  So error weighting was introduced to help maximize concordance in temperament optimization?  What if someone just cares about accuracy, and not concordance?  Wouldn&apos;t unweighted error make more sense then?</p><p>&gt; One thing that&apos;s notable about TE error is that it goes -down- as more<br/>&gt; accurate primes are added. For example, consider 31-EDO in the 5-limit<br/>&gt;<br/>&gt; <a href="http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=5">http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=5</a><br/>&gt;<br/>&gt; 1.628 cents of error. Now look at it in the 7-limit:<br/>&gt;<br/>&gt; <a href="http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=7">http://x31eq.com/cgi-bin/rt.cgi?ets=31&limit=7</a><br/>&gt;<br/>&gt; 1.432 cents, which is less.</p><p>Okay, where do these error figures come from, or maybe, what do they mean?  What does it mean that 31-TET has 1.628 cents of error on the 5-limit?</p><p>&gt; So what exactly is the behavior you&apos;re looking for? Should 7-limit<br/>&gt; 31-EDO be lower in error than 5-limit 31-EDO?</p><p>I hate that this is subjective.  What should I be looking for?  What, exactly, is the problem with not weighting the error?  (And how would I even calculate unweighted error?  Average the error for all the dyads on the n-odd-limit (or subgroup) tonality diamond?</p><p>-Igs</p></div><h3><a id=102661 href="#102661">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>1/7/2012 6:58:42 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Pfff... log(largest prime) is exactly the subgroup penalty<br/>I came up with!  The thing is, it should be used only in the<br/>competition for best subgroup for a given ET, not in the<br/>badness calc.  The score for each subgroup in the competitions<br/>is going to be</p><p>error * log(largest prime)/rank(subgroup)</p><p>The badness is going to be</p><p>error/rank(subgroup) * (notes/oct)^exponent</p><p>-Carl</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:<br/>&gt;<br/>&gt; Graham wrote:<br/>&gt; &gt; The adjusted error is the TE error multiplied by the log of<br/>&gt; &gt; the largest prime.  It matches the target error.<br/>&gt;<br/>&gt; Mike wrote:<br/>&gt; &gt; This is a bad idea, right?<br/>&gt;<br/>&gt; Yes, sure sounds like it&apos;ll screw up Igs&apos; particular search<br/>&gt; big-time.<br/>&gt;<br/>&gt; -Carl</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            