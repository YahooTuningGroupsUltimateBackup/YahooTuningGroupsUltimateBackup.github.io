<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning About harmonic entropy</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning">back to list</a><h1>About harmonic entropy</h1><h3><a id=82031 href="#82031">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/3/2009 11:47:34 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I would like to understand better the concept of harmonic entropy illustrated by Erlich</p><p><a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a></p><p>Unfortunately I wasn&apos;t able to find much further documentation about it. Particularly I would like to learn more about the procedure to calculate such entropy, that is described only qualitatively in the above note.</p><p>I have tried myself to translate in mathematical terms the algorithm described in the note above, ending out in the following formula for a &quot;kind of&quot; harmonic entropy calculated from fractions of maximum numerator and denominator of N, as a function of frequency ratio x:</p><p>S(x)=1 - Sum(for j=1 to N) [Sum (for i=j to N) [(1/(i j)) Exp(-(x-(i/j))^2 / (2 sigma^2))]]</p><p>where the Exponential is a normal distribution of standard deviation sigma, and 1/(i j) is a weight function.</p><p>For givex frequency ratio x and integer numerator i and denominator j, some value is subtracted to S(x) only when x is as close as (about) sigma to the ratio i/j. The larger both i and j, the smaller the weight (this is to give more importance to small ratios than to large ratios). For instance, when x = 1.52, a value ( Exp[-(1.52-1.50)^2/(2 sigma^2)] ) will be subtracted to S(x) when i=3 and j=2 (so i/j = 1.50) with weight 1/(3 2) = 1/6, while for instance when i = 23 and j = 15 (so i/j = 1.533) a smaller value will be subtracted (because i/j is further than 1 sigma from x, and also because the weight 1/(23 15) = 0.0029 is small), and when i = 79 and j = 52 (i/j = 1.519), the normal function will have higher value ( Exp[-(1.52-1.519)^2/(2 sigma^2)] ) but the weight will be even smaller ( 1/(79 52)= 0.00024 ).</p><p>For sigma = 0.01, n=20,50 and 80, I get plots (uploaded in my directory Max here on Tuning list named &quot;HE.jpg&quot;) that resemble the ones reported by Erlich. However, I have not intentionally used information theory definition for entropy, in particular any function of probability p of the form p log p. Also, I have not used Farey series, but just summed over all possible ratios, therefore including ratios not contained in the Farey series (for instance, I have used 3/2 as well as 6/4.... but the weight of 6/4 is less than the one of 3/2 although their probability is the same).</p><p>Nevertheless, the result I get looks to me rather similar to the one by Erlich.</p><p>The formula I used for calculations is reported on the plot in clearer form than above in my post.</p><p>In order to assess similarity and differences, would it be possible to find a reference, or some of you knows how to get some information on how exactly are the plots on the Erlich&apos;s note calculated?</p><p>Thanks a lot,</p><p>Max</p></div><h3><a id=82033 href="#82033">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/4/2009 1:03:15 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@...&gt; wrote:<br/>&gt;<br/>&gt; I would like to understand better the concept of harmonic entropy<br/>&gt; illustrated by Erlich<br/>&gt;<br/>&gt; <a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a><br/>&gt;<br/>&gt; Unfortunately I wasn&apos;t able to find much further documentation<br/>&gt; about it. Particularly I would like to learn more about the<br/>&gt; procedure to calculate such entropy, that is described only<br/>&gt; qualitatively in the above note.</p><p>Have you seen these two posts?<br/><a href="/harmonic_entropy/topicId_347.html#350">/harmonic_entropy/topicId_347.html#350</a><br/><a href="/harmonic_entropy/topicId_707.html#708">/harmonic_entropy/topicId_707.html#708</a></p><p>-Carl</p></div><h3><a id=82034 href="#82034">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/4/2009 1:16:43 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thank you Carl, looks great. Lots of formulae in there.</p><p>Surely it will be useful.</p><p>Max</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:<br/>&gt;<br/>&gt; Have you seen these two posts?<br/>&gt; <a href="/harmonic_entropy/topicId_347.html#350">/harmonic_entropy/topicId_347.html#350</a><br/>&gt; <a href="/harmonic_entropy/topicId_707.html#708">/harmonic_entropy/topicId_707.html#708</a><br/>&gt;<br/>&gt; -Carl<br/>&gt;</p></div><h3><a id=82050 href="#82050">ðŸ”—</a>rick_ballan &#x3C;rick_ballan@...&#x3E;</h3><span>3/4/2009 7:46:06 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; I would like to understand better the concept of harmonic entropy<br/>&gt; &gt; illustrated by Erlich<br/>&gt; &gt;<br/>&gt; &gt; <a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a><br/>&gt; &gt;<br/>&gt; &gt; Unfortunately I wasn&apos;t able to find much further documentation<br/>&gt; &gt; about it. Particularly I would like to learn more about the<br/>&gt; &gt; procedure to calculate such entropy, that is described only<br/>&gt; &gt; qualitatively in the above note.<br/>&gt;<br/>&gt; Have you seen these two posts?<br/>&gt; <a href="/harmonic_entropy/topicId_347.html#350">/harmonic_entropy/topicId_347.html#350</a><br/>&gt; <a href="/harmonic_entropy/topicId_707.html#708">/harmonic_entropy/topicId_707.html#708</a><br/>&gt;<br/>&gt; -Carl</p><p>&gt;Hi Carl,</p><p>I too am interested in finding out more about harmonic entropy. Haven&apos;t delved into the maths of these postings yet but just one preliminary question. It&apos;s been a vague idea building in the back of my mind for some time now that in reality a type of uncertainty principle applies to each harmonic interval. For eg, the brain/ear can still hear a perfect fifth when it is fairly out of tune, but how far before it becomes, say, a flat fifth? Like many things we take for granted, this seems basic enough but it is actually very difficult to answer properly (like trying to pin down how far is &quot;almost&quot;). I&apos;ve also imagined that probability maths will come into it at some point. My question is, does harmonic entropy provide a method for explaining this &apos;give&apos; around each interval?</p><p>Thanks mate</p><p>-Rick</p></div><h3><a id=82053 href="#82053">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/4/2009 8:31:20 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;rick_ballan&quot; &lt;rick_ballan@...&gt; wrote:</p><p>&gt; My question is, does harmonic entropy provide a method for<br/>&gt; explaining this &apos;give&apos; around each interval?<br/>&gt;<br/>&gt; Thanks mate<br/>&gt;<br/>&gt; -Rick</p><p>That&apos;s exactly right.  Try this article:</p><p><a href="http://www.soundofindia.com/showarticle.asp?in_article_id=1905806937">http://www.soundofindia.com/showarticle.asp?in_article_id=1905806937</a></p><p>-Carl</p></div><h3><a id=82071 href="#82071">ðŸ”—</a>Michael Sheiman &#x3C;djtrancendance@...&#x3E;</h3><span>3/4/2009 8:03:35 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Taken from <a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a>,<br/>---But a phenomenon called &quot;virtual pitch&quot;<br/>or &quot;fundamental ---tracking&quot; is central to Parncutt&apos;s treatment of<br/>dissonance and ---does represent, I believe, an additional factor besides<br/>critical ---band roughness.</p><p>---There is a very strong propensity for the ear to<br/>try to fit what it ---hears into one or a small number of harmonic series,<br/>and the ---fundamentals of these series, even if not physically present,<br/>are ---either heard outright, or provide a more subtle sense of overall<br/>---pitch known to musicians as the &quot;root&quot;.</p><p>&nbsp;&nbsp; In the past I have only really looked at Plomp&apos;s algorithm in the form of a dissonance curves (as done by Sethares) before and, on the side, known that the harmonic series is categorized in the brain as a stronger tone than, say, a sine wave played on the root tone.<br/>*****************************<br/>&nbsp;&nbsp; Oddly enough, in fact, my last JI-type scale based on the x/16 harmonic series tuning (where x = about 16 to 32) was built on the idea of simplifying a series of tone to point to one root tone.&nbsp; And, meanwhile, my 9/8 * 10/9 * 11/10 * 12/11 * 10/9 * 11/10 * 12/11 pointed to two different harmonic roots about the ratio of 6/5th away from each other.&nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp; So they both appear to lend themselves toward the concept of virtual pitch...if I have it right.&nbsp; In fact, I wonder (if virtual pitch is so important), why he don&apos;t make all non-adaptive scales based on only one or two harmonic series when searching for consonance in that form?<br/>********************************<br/>&nbsp;&nbsp; But, still...I&apos;ve had as good if not better luck with my PHI scale (as explained on <a href="http://www.geocities.com/djtrancendance/PHINOT.html">http://www.geocities.com/djtrancendance/PHINOT.html</a> in which virtually every note beats, but the beating somehow feels uniform/predictable and not random.&nbsp;<br/>&nbsp;&nbsp;&nbsp; So it seems to hint that some degree of beating can actually aid consonance and result in a different kind of consonance.&nbsp; Same goes with, say, violins (whose strings cause beating all over the place) vs. organs (whose overtones barely beat)...violins &quot;mysteriously&quot; sound just as good if not better.<br/>*****************************************************<br/>&nbsp; &nbsp; I believe, Max, you had said something before about that either all tones must be beating or virtually none across the spectrum to result in true consonance.<br/>&nbsp;&nbsp; In that case I wonder if perhaps my PHI scale finds the solution of the scenario where all notes are beating while stacking harmonic series (ALA JI) approaches the solution where no notes are beating (although that solution, ultimately, appears to lie in adaptive JI and not a fixed scale).</p><p>-Michael</p></div><h3><a id=82074 href="#82074">ðŸ”—</a>Chris Vaisvil &#x3C;chrisvaisvil@...&#x3E;</h3><span>3/5/2009 1:45:43 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Good subject.</p><p>I find it interesting and counter-intuitive that 3:1 and 4:1 would be ranked<br/>substantially less consonant that 2:1</p><p>(I found a graph to 4:1 by following a link somewhere in this thread with<br/>Indian music in the title on the page.)</p><p>A question if anyone knows the answer</p><p>is a triad perceived as 3 notes sounding at once</p><p>or 3 dyads?  C-E, C-G, E-G  ?</p><p>Chris</p><p>On Wed, Mar 4, 2009 at 2:47 AM, massimilianolabardi &lt;labardi@...&gt;wrote:</p><p>&gt;   I would like to understand better the concept of harmonic entropy<br/>&gt; illustrated by Erlich<br/>&gt;<br/>&gt; <a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a><br/>&gt;<br/>&gt; Unfortunately I wasn&apos;t able to find much further documentation about it.<br/>&gt; Particularly I would like to learn more about the procedure to calculate<br/>&gt; such entropy, that is described only qualitatively in the above note.<br/>&gt;<br/>&gt; I have tried myself to translate in mathematical terms the algorithm<br/>&gt; described in the note above, ending out in the following formula for a &quot;kind<br/>&gt; of&quot; harmonic entropy calculated from fractions of maximum numerator and<br/>&gt; denominator of N, as a function of frequency ratio x:<br/>&gt;<br/>&gt; S(x)=1 - Sum(for j=1 to N) [Sum (for i=j to N) [(1/(i j)) Exp(-(x-(i/j))^2<br/>&gt; / (2 sigma^2))]]<br/>&gt;<br/>&gt; where the Exponential is a normal distribution of standard deviation sigma,<br/>&gt; and 1/(i j) is a weight function.<br/>&gt;<br/>&gt; For givex frequency ratio x and integer numerator i and denominator j, some<br/>&gt; value is subtracted to S(x) only when x is as close as (about) sigma to the<br/>&gt; ratio i/j. The larger both i and j, the smaller the weight (this is to give<br/>&gt; more importance to small ratios than to large ratios). For instance, when x<br/>&gt; = 1.52, a value ( Exp[-(1.52-1.50)^2/(2 sigma^2)] ) will be subtracted to<br/>&gt; S(x) when i=3 and j=2 (so i/j = 1.50) with weight 1/(3 2) = 1/6, while for<br/>&gt; instance when i = 23 and j = 15 (so i/j = 1.533) a smaller value will be<br/>&gt; subtracted (because i/j is further than 1 sigma from x, and also because the<br/>&gt; weight 1/(23 15) = 0.0029 is small), and when i = 79 and j = 52 (i/j =<br/>&gt; 1.519), the normal function will have higher value ( Exp[-(1.52-1.519)^2/(2<br/>&gt; sigma^2)] ) but the weight will be even smaller ( 1/(79 52)= 0.00024 ).<br/>&gt;<br/>&gt; For sigma = 0.01, n=20,50 and 80, I get plots (uploaded in my directory Max<br/>&gt; here on Tuning list named &quot;HE.jpg&quot;) that resemble the ones reported by<br/>&gt; Erlich. However, I have not intentionally used information theory definition<br/>&gt; for entropy, in particular any function of probability p of the form p log<br/>&gt; p. Also, I have not used Farey series, but just summed over all possible<br/>&gt; ratios, therefore including ratios not contained in the Farey series (for<br/>&gt; instance, I have used 3/2 as well as 6/4.... but the weight of 6/4 is less<br/>&gt; than the one of 3/2 although their probability is the same).<br/>&gt;<br/>&gt; Nevertheless, the result I get looks to me rather similar to the one by<br/>&gt; Erlich.<br/>&gt;<br/>&gt; The formula I used for calculations is reported on the plot in clearer form<br/>&gt; than above in my post.<br/>&gt;<br/>&gt; In order to assess similarity and differences, would it be possible to find<br/>&gt; a reference, or some of you knows how to get some information on how exactly<br/>&gt; are the plots on the Erlich&apos;s note calculated?<br/>&gt;<br/>&gt; Thanks a lot,<br/>&gt;<br/>&gt; Max<br/>&gt;<br/>&gt;<br/>&gt;</p></div><h3><a id=82075 href="#82075">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/5/2009 1:59:03 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Chris Vaisvil &lt;chrisvaisvil@...&gt; wrote:<br/>&gt;<br/>&gt; Good subject.<br/>&gt;<br/>&gt; I find it interesting and counter-intuitive that 3:1 and 4:1<br/>&gt; would be ranked substantially less consonant that 2:1<br/>&gt; (I found a graph to 4:1 by following a link somewhere in this<br/>&gt; thread with Indian music in the title on the page.)</p><p>As I recall some later versions of harmonic entropy got rid<br/>of the overall downward slope.  But generally, the consensus<br/>of theorists here has been that as intervals get very wide<br/>(4:1, 8:1, etc.) they lose both consonance and dissonance.<br/>That is, they do not interact as strongly either way.  In that<br/>light, I would certainly call 4:1 less consonant than 2:1.</p><p>This is part of the view of consonance as a thing in and of<br/>itself -- not merely the lack of dissonance.</p><p>&gt; A question if anyone knows the answer<br/>&gt; is a triad perceived as 3 notes sounding at once<br/>&gt; or 3 dyads?  C-E, C-G, E-G  ?</p><p>Presently, _all_ models of consonance (which have any serious<br/>support) are dyadic theories.  It remains a major challenge in<br/>music theory to directly model larger chords like triads and<br/>tetrads.  Paul came tantalizingly to doing triadic harmonic<br/>entropy calculations.  We know that for triads composed of small<br/>whole numbers, the triadic harmonic entropy is proportional to<br/>the geometric mean of the product of the chord tones -- the<br/>a*b*c rule.  A full triadic harmonic entropy calc would also be<br/>able to handle chords containing irrational (or large rational)<br/>intervals.</p><p>-Carl</p></div><h3><a id=82076 href="#82076">ðŸ”—</a>Chris Vaisvil &#x3C;chrisvaisvil@...&#x3E;</h3><span>3/5/2009 2:18:08 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thanks for the really good information.</p><p>Now here is a thought.</p><p>If :</p><p>&quot; But generally, the consensus<br/>of theorists here has been that as intervals get very wide<br/>(4:1, 8:1, etc.) they lose both consonance and dissonance.<br/>That is, they do not interact as strongly either way. In that<br/>light, I would certainly call 4:1 less consonant than 2:1.&quot;</p><p>is true - and as you go up 3:1, 4:1, 5:1 you are replicating the harmonic<br/>series<br/>is there a correlation between the harmonic entropy ranking of the harmonic<br/>number and the interval its octave &quot;normalized&quot; (I guess reduced?) note in a<br/>dyad?</p><p>After some thought - this is probably trivial.</p><p>On Thu, Mar 5, 2009 at 4:59 PM, Carl Lumma &lt;carl@...&gt; wrote:</p><p>&gt;   --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a> &lt;tuning%<a href="http://40yahoogroups.com">40yahoogroups.com</a>&gt;, Chris Vaisvil<br/>&gt; &lt;chrisvaisvil@...&gt; wrote:<br/>&gt; &gt;<br/>&gt; &gt; Good subject.<br/>&gt; &gt;<br/>&gt; &gt; I find it interesting and counter-intuitive that 3:1 and 4:1<br/>&gt; &gt; would be ranked substantially less consonant that 2:1<br/>&gt; &gt; (I found a graph to 4:1 by following a link somewhere in this<br/>&gt; &gt; thread with Indian music in the title on the page.)<br/>&gt;<br/>&gt; As I recall some later versions of harmonic entropy got rid<br/>&gt; of the overall downward slope. But generally, the consensus<br/>&gt; of theorists here has been that as intervals get very wide<br/>&gt; (4:1, 8:1, etc.) they lose both consonance and dissonance.<br/>&gt; That is, they do not interact as strongly either way. In that<br/>&gt; light, I would certainly call 4:1 less consonant than 2:1.<br/>&gt;<br/>&gt; This is part of the view of consonance as a thing in and of<br/>&gt; itself -- not merely the lack of dissonance.<br/>&gt;<br/>&gt; &gt; A question if anyone knows the answer<br/>&gt; &gt; is a triad perceived as 3 notes sounding at once<br/>&gt; &gt; or 3 dyads? C-E, C-G, E-G ?<br/>&gt;<br/>&gt; Presently, _all_ models of consonance (which have any serious<br/>&gt; support) are dyadic theories. It remains a major challenge in<br/>&gt; music theory to directly model larger chords like triads and<br/>&gt; tetrads. Paul came tantalizingly to doing triadic harmonic<br/>&gt; entropy calculations. We know that for triads composed of small<br/>&gt; whole numbers, the triadic harmonic entropy is proportional to<br/>&gt; the geometric mean of the product of the chord tones -- the<br/>&gt; a*b*c rule. A full triadic harmonic entropy calc would also be<br/>&gt; able to handle chords containing irrational (or large rational)<br/>&gt; intervals.<br/>&gt;<br/>&gt; -Carl<br/>&gt;<br/>&gt;<br/>&gt;</p></div><h3><a id=82080 href="#82080">ðŸ”—</a>rick_ballan &#x3C;rick_ballan@...&#x3E;</h3><span>3/5/2009 9:28:36 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Michael Sheiman &lt;djtrancendance@...&gt; wrote:<br/>&gt;<br/>&gt; Taken from <a href="http://sonic-arts.org/td/erlich/entropy-erlich.htm">http://sonic-arts.org/td/erlich/entropy-erlich.htm</a>,<br/>&gt; ---But a phenomenon called &quot;virtual pitch&quot;<br/>&gt; or &quot;fundamental ---tracking&quot; is central to Parncutt&apos;s treatment of<br/>&gt; dissonance and ---does represent, I believe, an additional factor besides<br/>&gt; critical ---band roughness.<br/>&gt;<br/>&gt; ---There is a very strong propensity for the ear to<br/>&gt; try to fit what it ---hears into one or a small number of harmonic series,<br/>&gt; and the ---fundamentals of these series, even if not physically present,<br/>&gt; are ---either heard outright, or provide a more subtle sense of overall<br/>&gt; ---pitch known to musicians as the &quot;root&quot;.<br/>&gt;<br/>&gt;    In the past I have only really looked at Plomp&apos;s algorithm in the form of a dissonance curves (as done by Sethares) before and, on the side, known that the harmonic series is categorized in the brain as a stronger tone than, say, a sine wave played on the root tone.<br/>&gt; *****************************<br/>&gt;    Oddly enough, in fact, my last JI-type scale based on the x/16 harmonic series tuning (where x = about 16 to 32) was built on the idea of simplifying a series of tone to point to one root tone.  And, meanwhile, my 9/8 * 10/9 * 11/10 * 12/11 * 10/9 * 11/10 * 12/11 pointed to two different harmonic roots about the ratio of 6/5th away from each other.<br/>&gt;<br/>&gt;     So they both appear to lend themselves toward the concept of virtual pitch...if I have it right.  In fact, I wonder (if virtual pitch is so important), why he don&apos;t make all non-adaptive scales based on only one or two harmonic series when searching for consonance in that form?<br/>&gt; ********************************<br/>&gt;    But, still...I&apos;ve had as good if not better luck with my PHI scale (as explained on <a href="http://www.geocities.com/djtrancendance/PHINOT.html">http://www.geocities.com/djtrancendance/PHINOT.html</a> in which virtually every note beats, but the beating somehow feels uniform/predictable and not random.<br/>&gt;     So it seems to hint that some degree of beating can actually aid consonance and result in a different kind of consonance.  Same goes with, say, violins (whose strings cause beating all over the place) vs. organs (whose overtones barely beat)...violins &quot;mysteriously&quot; sound just as good if not better.<br/>&gt; *****************************************************<br/>&gt;     I believe, Max, you had said something before about that either all tones must be beating or virtually none across the spectrum to result in true consonance.<br/>&gt;    In that case I wonder if perhaps my PHI scale finds the solution of the scenario where all notes are beating while stacking harmonic series (ALA JI) approaches the solution where no notes are beating (although that solution, ultimately, appears to lie in adaptive JI and not a fixed scale).<br/>&gt;<br/>&gt; -Michael<br/>&gt;<br/>Hi Chris,</p><p>A question if anyone knows the answer<br/>&gt; is a triad perceived as 3 notes sounding at once<br/>&gt; or 3 dyads? C-E, C-G, E-G ?</p><p>It is a triad, firstly because this is the meaning of the Latin &quot;tri&quot; and secondly because if you play three G notes simultaneously then you are merely changing the tone or volume of a single G note.</p><p>-Rick</p></div><h3><a id=82081 href="#82081">ðŸ”—</a>Charles Lucy &#x3C;lucy@...&#x3E;</h3><span>3/6/2009 12:11:17 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p><a href="http://www.bbc.co.uk/radio4/history/inourtime/inourtime.shtml">http://www.bbc.co.uk/radio4/history/inourtime/inourtime.shtml</a></p><p>Roger Penrose and others discuss the measurement paradox, which (to my mind) is extremely pertinent to our studies of musical tuning.</p><p>Wave functions, probabilities, super-positions etc.</p><p>Charles Lucy<br/>lucy@...</p><p>- Promoting global harmony through LucyTuning -</p><p>for information on LucyTuning go to:<br/><a href="http://www.lucytune.com">http://www.lucytune.com</a></p><p>For LucyTuned Lullabies go to:<br/><a href="http://www.lullabies.co.uk">http://www.lullabies.co.uk</a></p></div><h3><a id=82082 href="#82082">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/6/2009 12:33:46 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Charles Lucy &lt;lucy@...&gt; wrote:<br/>&gt;<br/>&gt; <a href="http://www.bbc.co.uk/radio4/history/inourtime/inourtime.shtml">http://www.bbc.co.uk/radio4/history/inourtime/inourtime.shtml</a><br/>&gt;<br/>&gt; Roger Penrose and others discuss the measurement paradox, which (to my<br/>&gt; mind) is extremely pertinent to our studies of musical tuning.<br/>&gt;<br/>&gt; Wave functions, probabilities, super-positions etc.</p><p>Wave functions and superpositions are ingredients of wave mechanics, while probability concerns more properly statistical mechanics, thermodynamics (btw entropy is a thermodynamic quantity) and quantum physics. Sound is waves, therefore most probably wave mechanics applies pretty well there. In my view, statistical properties could concern the psychoacoustical problem, in the sense that our brain could perform a kind of statistical analysis of aural data. I might be wrong, but instead I see no way how uncertainty principle, that is the ground of &quot;measurement paradox,&quot; could apply to both acoustics and psychoacoustics. Uncertainty (or Heisemberg) principle applies to quantum objects (atoms, photons...) but its implications are hidden (kind of averaged out) when we deal with huge ensembles of such objects like in ordinary matter and at ordinary length and time scales. If quantum effects were observable at a macroscopic scale, they would be part of everyday&apos;s experience, while evidently they are not.</p><p>Max</p></div><h3><a id=82102 href="#82102">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/7/2009 7:37:01 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:</p><p>&gt; Have you seen these two posts?<br/>&gt; <a href="/harmonic_entropy/topicId_347.html#350">/harmonic_entropy/topicId_347.html#350</a><br/>&gt; <a href="/harmonic_entropy/topicId_707.html#708">/harmonic_entropy/topicId_707.html#708</a></p><p>I have tried to use information within. Now I apply (p log p) form recalling entropy definition. Still, I get qualitatively similar results compared to my previous simpler modeling. The new plot is at:</p><p><a href="/tuning/files/Max/HE2.jpg">/tuning/files/Max/HE2.jpg</a></p><p>(while the old one was at <a href="/tuning/files/Max/HE2.jpg">/tuning/files/Max/HE2.jpg</a>).</p><p>What I retain from the previous model is: weighing simpler ratios with higher probability 1/ij (i=numerator, j=denominator), and using all of the possible ratios up to an integer N for both i and j. So basically I am not using Farey sequence, nor its mediants, because still I don&apos;t fully understand whether it is just an arbitrary choice or, in turn, what is the underlying reason for such a choice. Also, I use linear scale and not log ones.</p><p>Before going on, I notice that, in essence, plots obtained by giving a higher weight to simpler ratios come out all similar to each other. Also, in my opinion, the difference between max and min of harmonic entropy for ratios like 5/4 and 6/5 looks too small, compared for instance to the depth of the minimum at x=3/2 and x=4/3, to account for clear differences in perception of dyadic consonance (as far as the latter is one of the aims of this model). So, at present, I am not really persuaded of the importance of &quot;details&quot; (like using different ensembles of fractions, or log units, or...) in this description.</p><p>I would be glad to learn whether I miss something...</p><p>Max</p></div><h3><a id=82105 href="#82105">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/7/2009 1:33:04 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@...&gt; wrote:<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@&gt; wrote:<br/>&gt;<br/>&gt; &gt; Have you seen these two posts?<br/>&gt; &gt; <a href="/harmonic_entropy/topicId_347.html#350">/harmonic_entropy/topicId_347.html#350</a><br/>&gt; &gt; <a href="/harmonic_entropy/topicId_707.html#708">/harmonic_entropy/topicId_707.html#708</a><br/>&gt;<br/>&gt; I have tried to use information within. Now I apply (p log p)<br/>&gt; form recalling entropy definition. Still, I get qualitatively<br/>&gt; similar results compared to my previous simpler modeling. The<br/>&gt; new plot is at:<br/>&gt;<br/>&gt; <a href="/tuning/files/Max/HE2.jpg">/tuning/files/Max/HE2.jpg</a></p><p>What are the three contours here?</p><p>&gt; (while the old one was at <a href="http://launch.groups.yahoo.com/group">http://launch.groups.yahoo.com/group</a><br/>&gt; /tuning/files/Max/HE2.jpg).</p><p>You gave the same link twice.</p><p>&gt; What I retain from the previous model is: weighing simpler<br/>&gt; ratios with higher probability 1/ij (i=numerator, j=denominator),<br/>&gt; and using all of the possible ratios up to an integer N for<br/>&gt; both i and j. So basically I am not using Farey sequence, nor its<br/>&gt; mediants, because still I don&apos;t fully understand whether it is<br/>&gt; just an arbitrary choice or, in turn, what is the underlying<br/>&gt; reason for such a choice. Also, I use linear scale and not log<br/>&gt; ones.</p><p>Paul showed that the harmonic entropy converges (or at least<br/>becomes stable in some sense) as the order of the farey series<br/>goes to infinity.</p><p>&gt; Before going on, I notice that, in essence, plots obtained by<br/>&gt; giving a higher weight to simpler ratios come out all similar<br/>&gt; to each other.</p><p>Yup.</p><p>&gt; Also, in my opinion, the difference between max<br/>&gt; and min of harmonic entropy for ratios like 5/4 and 6/5 looks<br/>&gt; too small, compared for instance to the depth of the minimum<br/>&gt; at x=3/2 and x=4/3, to account for clear differences in<br/>&gt; perception of dyadic consonance (as far as the latter is one<br/>&gt; of the aims of this model).</p><p>Can you explain a bit more what you mean here?</p><p>-Carl</p></div><h3><a id=82106 href="#82106">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/7/2009 3:01:59 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:</p><p>&gt; &gt;<br/>&gt; &gt; <a href="/tuning/files/Max/HE2.jpg">/tuning/files/Max/HE2.jpg</a><br/>&gt;<br/>&gt; What are the three contours here?</p><p>N=20, N=50, N=80 (from top to bottom).</p><p>&gt; &gt; (while the old one was at <a href="http://launch.groups.yahoo.com/group">http://launch.groups.yahoo.com/group</a><br/>&gt; &gt; /tuning/files/Max/HE2.jpg).<br/>&gt;<br/>&gt; You gave the same link twice.<br/>&gt;</p><p>Sorry. <a href="/tuning/files/Max/HE.jpg">/tuning/files/Max/HE.jpg</a></p><p>&gt;<br/>&gt; Paul showed that the harmonic entropy converges (or at least<br/>&gt; becomes stable in some sense) as the order of the farey series<br/>&gt; goes to infinity.<br/>&gt;</p><p>Ok. I noticed more convergence with the simpler model (HE.jpg). Instead by using &quot;p log p&quot; (HE2.jpg) there is less. I guess some normalization is needed, but that will depend on the choice of fraction ensemble. I&apos;ll try to work out this better.</p><p>&gt; &gt; Also, in my opinion, the difference between max<br/>&gt; &gt; and min of harmonic entropy for ratios like 5/4 and 6/5 looks<br/>&gt; &gt; too small, compared for instance to the depth of the minimum<br/>&gt; &gt; at x=3/2 and x=4/3, to account for clear differences in<br/>&gt; &gt; perception of dyadic consonance (as far as the latter is one<br/>&gt; &gt; of the aims of this model).<br/>&gt;<br/>&gt; Can you explain a bit more what you mean here?</p><p>I meant, by looking at Erlich&apos;s plots and concentrating on the entropy maximum at 348 cents, in between the two local minima at 315 and 387 cents (the ones corresponding to 6/5 and 5/4 fractions), the difference in entropy looks not so high (of the order of 0.1-0.2 units while the one corresponding to the interval 3/2 is 1 and to 4/3 is 0.5). In turn, if I  understand correctly, an interval close to 6/5 (minor third) is classified by our brain as belonging to a very different category than an interval close to 5/4 (major third), whereas an interval in between (i.e. on the local entropy maximum) is hard to classify in one or the other category and therefore would sound more &quot;strange&quot; or dissonant.</p><p>My observation is just that major and minor &quot;categories&quot; have very strong character (indeed, a lot in western music is based on the contrast between the two &quot;modes&quot;) but on the other hand their harmonic entropy is not so different as one could expect. However, this is just an impression.</p><p>Max</p></div><h3><a id=82107 href="#82107">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/7/2009 9:23:28 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@...&gt; wrote:<br/>&gt;<br/>&gt; &gt; &gt; <a href="/tuning/files/Max/HE2.jpg">/tuning/files/Max/HE2.jpg</a><br/>&gt; &gt;<br/>&gt; &gt; What are the three contours here?<br/>&gt;<br/>&gt; N=20, N=50, N=80 (from top to bottom).</p><p>Ok, yes, that looks about right.</p><p>&gt; &gt; &gt; (while the old one was at <a href="http://launch.groups.yahoo.com/group">http://launch.groups.yahoo.com/group</a><br/>&gt; &gt; &gt; /tuning/files/Max/HE2.jpg).<br/>&gt; &gt;<br/>&gt; &gt; You gave the same link twice.<br/>&gt;<br/>&gt; Sorry. <a href="/tuning/files/Max/HE.jpg">/tuning/files/Max/HE.jpg</a></p><p>How are you assigning i and j for a given x?  Just taking<br/>the nearest ratio i/j to x?</p><p>&gt; I meant, by looking at Erlich&apos;s plots and concentrating on the<br/>&gt; entropy maximum at 348 cents, in between the two local minima<br/>&gt; at 315 and 387 cents (the ones corresponding to 6/5 and 5/4<br/>&gt; fractions), the difference in entropy looks not so high (of the<br/>&gt; order of 0.1-0.2 units while the one corresponding to the<br/>&gt; interval 3/2 is 1 and to 4/3 is 0.5).</p><p>Using the formulation of harmonic entropy I prefer, the<br/>minimum at 5/4 is 4.4903844 nats, and the maximum near 11/9<br/>is 4.6245254 nats, for a difference of ~ 0.13.</p><p>3/2 is at 4.1300928 and 659 cents is at 4.6575969, for<br/>a difference of ~ 0.53.  You can get a difference on the<br/>order 1 nat, between the octave and a wolf octave.</p><p>&gt; In turn, if I understand<br/>&gt; correctly, an interval close to 6/5 (minor third) is classified<br/>&gt; by our brain as belonging to a very different category than an<br/>&gt; interval close to 5/4 (major third), whereas an interval in<br/>&gt; between (i.e. on the local entropy maximum) is hard to classify<br/>&gt; in one or the other category and therefore would sound more<br/>&gt; &quot;strange&quot; or dissonant.<br/>&gt;<br/>&gt; My observation is just that major and minor &quot;categories&quot; have<br/>&gt; very strong character (indeed, a lot in western music is based<br/>&gt; on the contrast between the two &quot;modes&quot;) but on the other hand<br/>&gt; their harmonic entropy is not so different as one could expect.<br/>&gt; However, this is just an impression.</p><p>One must bear in mind that these harmonic entropy calculations<br/>model what we hear between a pair of sine tones.  It&apos;s a<br/>simplification to apply it to the pitches of normal timbres.<br/>Nevertheless, the numbers above aren&apos;t entirely unreasonable.<br/>They say the dissonance increase between a 5th and a wolf 5th<br/>is about five times greater than the increase between a 3rd<br/>and a neutral 3rd.</p><p>-Carl</p></div><h3><a id=82110 href="#82110">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/8/2009 1:58:00 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Carl Lumma&quot; &lt;carl@...&gt; wrote:</p><p>&gt; &gt; Sorry. <a href="/tuning/files/Max/HE.jpg">/tuning/files/Max/HE.jpg</a><br/>&gt;<br/>&gt; How are you assigning i and j for a given x?  Just taking<br/>&gt; the nearest ratio i/j to x?<br/>&gt;</p><p>Basically, yes. More precisely, given a ratio x, one calculates the difference x - i/j for each of the fractions i/j in a given ensamble. Then you have a normal probability function centered at i/j-x and with a width sigma, that works as follows:</p><p>If x - i/j = 0 (that is, x is exactly equal to the considered fraction i/j) then the Gaussian is maximum.</p><p>If x - i/j is close to zero, say within 1 or 2 sigma (widths), there is still some probability but its value is smaller.</p><p>If x is far from i/j, then probability is negligible.</p><p>this would give the same significance to very close ratios, say to 3/2 and 3001/2000 when x=1.5. So we need a weight function to give more importance to ratios made up of the smallest numerator and denominator. I have chosen 1/ij as such function. This means, 3/2 will count as 1/6, while 3001/2000 will count as 1/6002000, that means, negligible.</p><p>So in essence I am not taking the nearest ratio, but taking all ratios within a width sigma around x, but weighted in such a way that simpler ratios contribute a lot while more complex ones contribute much less. I think this might be about the same of what done with Farey sequence and its mediants. Perhaps the role of the weight function is taken by the width of the integration interval for each couple of elements of the Farey series.</p><p>&gt; Using the formulation of harmonic entropy I prefer, the<br/>&gt; minimum at 5/4 is 4.4903844 nats, and the maximum near 11/9<br/>&gt; is 4.6245254 nats, for a difference of ~ 0.13.</p><p>What is such formulation, and the plot you refer to?</p><p>&gt; 3/2 is at 4.1300928 and 659 cents is at 4.6575969, for<br/>&gt; a difference of ~ 0.53.  You can get a difference on the<br/>&gt; order 1 nat, between the octave and a wolf octave.<br/>&lt;snip&gt;<br/>&gt; Nevertheless, the numbers above aren&apos;t entirely unreasonable.<br/>&gt; They say the dissonance increase between a 5th and a wolf 5th<br/>&gt; is about five times greater than the increase between a 3rd<br/>&gt; and a neutral 3rd.</p><p>Ok, I see. Comparing a consonant ratio with a close dissonant one makes more sense. Comparing absolute values of H.E. all over the octave is more difficult.</p><p>Thanks a lot Carl,</p><p>Max</p></div><h3><a id=82112 href="#82112">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/8/2009 5:45:17 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Michael Sheiman &lt;djtrancendance@...&gt; wrote:</p><p>&gt; &nbsp; &nbsp; I believe, Max, you had said something before about that either all tones must be beating or virtually none across the spectrum to result in true consonance.<br/>&gt; &nbsp;&nbsp; In that case I wonder if perhaps my PHI scale finds the solution of the scenario where all notes are beating while stacking harmonic series (ALA JI) approaches the solution where no notes are beating (although that solution, ultimately, appears to lie in adaptive JI and not a fixed scale).<br/>&gt;</p><p>At the beginnign I tried to put forward the hypothesis that most consonant triads could be characterized by equal (or related by simple ratios) beat frequency. Afterwards, the issue was to understand whether the effect of such beats could be audible or not, and conclusions were rather controversial. So, I really don&apos;t know.</p><p>About the tunings involvong Golden ratio, I think that the Phi irrational has some real peculiarity with respect to other irrationals - trascendental or not - sometimes used to build up temperaments. The interesting property of Phi is, in my opinion, that 1/Phi = 1 - Phi. This implies that, for instance, if you press a cello string at the golden ratio position (61.8% of full length) and pluck the two parts, the difference of such two frequencies is at the frequency of the full string. I have really no idea whether possible implications of this may exist for the musical world... but at least it&apos;s something! I couldn&apos;t find any similar mathematical or physical role for e.g. greek Pi or other peculiar irrationals so far.</p><p>Also, if you make a triad as 1:(Phi^2)/2:Phi, that is 1:1.309:1.618, it happens that the 2nd harmonic of 1.309 is 2.618 and then the difference frequency between the 2nd harmonic of 1.309 and 1.618 is exactly 1. Similarly, the difference frequency between the 2nd harmonic of 1.618 (3.236) and the fourth harmonic of 1.309 (5.236) is exactly the 2nd harmonic of 1 (2).... I have no idea of how such chord sounds like, but surely its harmonic properties seem peculiar, as long as difference tones play some role.</p><p>Max</p></div><h3><a id=82113 href="#82113">ðŸ”—</a>Carl Lumma &#x3C;carl@...&#x3E;</h3><span>3/8/2009 10:48:06 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@...&gt; wrote:</p><p>&gt; &gt; How are you assigning i and j for a given x?  Just taking<br/>&gt; &gt; the nearest ratio i/j to x?<br/>&gt;<br/>&gt; Basically, yes. More precisely, given a ratio x, one calculates<br/>&gt; the difference x - i/j for each of the fractions i/j in a given<br/>&gt; ensamble.</p><p>Ah, good.</p><p>&gt; So in essence I am not taking the nearest ratio, but taking all<br/>&gt; ratios within a width sigma around x, but weighted in such a way<br/>&gt; that simpler ratios contribute a lot while more complex ones<br/>&gt; contribute much less. I think this might be about the same of what<br/>&gt; done with Farey sequence and its mediants.</p><p>Yes indeed.</p><p>&gt;Perhaps the role of the weight function is taken by the width of<br/>&gt;the integration interval for each couple of elements of the Farey<br/>&gt;series.</p><p>When using the Farey series, there is no weight function.<br/>The mediants are used to partition the region under a<br/>Gaussian centered on x.  The weight function (1/sqrt(n*d))<br/>is used instead of the mediant-mediant distances.</p><p>&gt; &gt; Using the formulation of harmonic entropy I prefer, the<br/>&gt; &gt; minimum at 5/4 is 4.4903844 nats, and the maximum near 11/9<br/>&gt; &gt; is 4.6245254 nats, for a difference of ~ 0.13.<br/>&gt;<br/>&gt; What is such formulation, and the plot you refer to?</p><p>I&apos;ve never bothered to plot it; I just have the data.<br/>The formulation is 1/sqrt(n*d) weighting function where<br/>the Gaussian on x has a standard deviation of 1% in<br/>frequency space.</p><p>&gt; Ok, I see. Comparing a consonant ratio with a close dissonant<br/>&gt; one makes more sense. Comparing absolute values of H.E. all<br/>&gt; over the octave is more difficult.</p><p>No, that should be possible too.  I didn&apos;t follow the example<br/>you were giving, so I suggested an alternative.</p><p>-Carl</p></div><h3><a id=82143 href="#82143">ðŸ”—</a>massimilianolabardi &#x3C;labardi@...&#x3E;</h3><span>3/12/2009 3:07:12 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;massimilianolabardi&quot; &lt;labardi@...&gt; wrote:</p><p>&gt; Also, if you make a triad as 1:(Phi^2)/2:Phi, that is 1:1.309:1.618,<br/>&gt;it happens that the 2nd harmonic of 1.309 is 2.618 and then the<br/>&gt;difference frequency between the 2nd harmonic of 1.309 and 1.618 is<br/>&gt;exactly 1. Similarly, the difference frequency between the 2nd<br/>&gt;harmonic of 1.618 (3.236) and the fourth harmonic of 1.309 (5.236) is<br/>&gt;exactly the 2nd harmonic of 1 (2).... I have no idea of how such<br/>&gt;chord sounds like, but surely its harmonic properties seem peculiar,<br/>&gt;as long as difference tones play some role.</p><p>Actually, I realize that this is not a so special property. It is typical for rationals, e.g. 1:5/4:3/2 you get 2*(5/4)=10/4 and 10/4 - 3/2 =1. Also, 2*(3/2)=3 and (5/4)*4=5 so that their difference is 5 - 3 = 2 (second harmonic of 1), so same property. Therefore harmonic properties could be the same characterizing rationals.... only, Phi is irrational.</p><p>But... of course you can make any number (also irrational and even trascendental) behave like that, if you properly define intervals. E.g. taking greek Pi/2 = 1.57079.... instead of Phi, you can calculate the &quot;middle&quot; tone of the chord x having the same property: x/2 = Pi/2 - 1 and you get x = Pi - 2 (proof: (Pi - 2)/2 = Pi/2 - 1). So the chord would be in this case 1:Pi-2:Pi/2 (1:1.1415...:1.57079...).</p><p>Phi differs in that the middle tone coincides with the one resulting from the &quot;cicle of Phi&quot;, while in the other cases (included the ratonals) it seems it is not the case.</p><p>I have no idea whether this observation is a trivial one or it could give some hint, I hope so.</p><p>Max</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            