<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning Interesting idea - Musical Tuning and Human Biology - today's NewScientist - London</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning">back to list</a><h1>Interesting idea - Musical Tuning and Human Biology - today's NewScientist - London</h1><h3><a id=46259 href="#46259">ðŸ”—</a>Charles Lucy &#x3C;lucy@harmonics.com&#x3E;</h3><span>8/7/2003 5:28:56 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p><a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a></p><p>Charles Lucy - <a href="mailto:lucy@harmonics.com">lucy@harmonics.com</a> (LucyScaleDevelopments)<br/>------------  Promoting global harmony through LucyTuning  -------<br/>for information on LucyTuning go to:<br/><a href="http://www.harmonics.com/lucy/">http://www.harmonics.com/lucy/</a> for LucyTuned Lullabies go to <a href="http://www.lucytune.com">http://www.lucytune.com</a> <a href="http://www.lucytune.co.uk">http://www.lucytune.co.uk</a>  or<br/><a href="http://www.lullabies.co.uk">http://www.lullabies.co.uk</a></p></div><h3><a id=46263 href="#46263">ðŸ”—</a>Haresh BAKSHI &#x3C;hareshbakshi@hotmail.com&#x3E;</h3><span>8/7/2003 10:55:20 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Charles Lucy &lt;lucy@h...&gt; wrote:<br/>&gt; <a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a><br/>.............</p><p>Hello ALL, please also read<br/><a href="http://www.newscientist.com/news/news.jsp?id=ns9999994">http://www.newscientist.com/news/news.jsp?id=ns9999994</a><br/>where a study shows that babies&apos; musical memories are formed in womb.</p><p>This reminds me of Abhimanyu, mentioned in Mahabharata (the 88,000-<br/>verse long Sanskrit work) as a 16-year old son of Arjuna and<br/>Subhadra. Abhimanyu is reported to have learnt some very complex war<br/>stratagies while still in his womb -- his father Arjuna taught him<br/>war planning even before Abhimanyu was born.</p><p>Regards,<br/>Haresh.</p></div><h3><a id=46271 href="#46271">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/7/2003 2:01:22 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Charles Lucy wrote:<br/>&gt; <a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a></p><p>Great!  Looks like we can all give up and go back to 12-equal, because it&apos;s closest to speech patterns, and all world music uses it anyway!</p><p>The original abstract&apos;s here:</p><p><a href="http://www.jneurosci.org/cgi/content/abstract/23/18/7160">http://www.jneurosci.org/cgi/content/abstract/23/18/7160</a></p><p>and doesn&apos;t have any mention of tuning.  Although it does say &quot;probability distribution of amplitude-frequency  combinations in human utterances predicts both the structure of the chromatic scale and consonance ordering.&quot;  Does anybody have access to this journal to check the details?</p><p>                        Graham</p></div><h3><a id=46279 href="#46279">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>8/7/2003 3:12:17 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Charles Lucy &lt;lucy@h...&gt; wrote:<br/>&gt; <a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a></p><p>This is a really bad article. Does anyone know of a better on-line<br/>reference?</p></div><h3><a id=46290 href="#46290">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/7/2003 7:52:04 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thanks to my informant for slipping me a copy of the original J Neurosci article :)</p><p>It turns out that speech samples mostly use the first 6 partials of a harmonic series, with the emphasis on partials 3 and 4.  Everything else follows from that.</p><p>They mention pelog, but otherwise think the whole world uses subsets of the chromatic scale.</p><p>                   Graham</p></div><h3><a id=46302 href="#46302">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>8/7/2003 11:54:29 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>wrote:<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Charles Lucy &lt;lucy@h...&gt; wrote:<br/>&gt; &gt; <a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a><br/>&gt;<br/>&gt; This is a really bad article.</p><p>looks like a D+ high school paper.</p><p>&gt; Does anyone know of a better on-line<br/>&gt; reference?</p><p>i hope so!</p></div><h3><a id=46355 href="#46355">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/10/2003 11:50:25 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Graham Breed wrote (Aug 7):</p><p>&gt; Charles Lucy wrote:<br/>&gt;&gt; <a href="http://www.newscientist.com/news/news.jsp?id=ns99994031">http://www.newscientist.com/news/news.jsp?id=ns99994031</a></p><p>&gt; Great! Looks like we can all give up and go back to 12-equal, because<br/>&gt; it&apos;s closest to speech patterns, and all world music uses it anyway!</p><p>&gt; The original abstract&apos;s here:</p><p>&gt; <a href="http://www.jneurosci.org/cgi/content/abstract/23/18/7160">http://www.jneurosci.org/cgi/content/abstract/23/18/7160</a></p><p>The paper does not make any suggestion what anybody might like to give up or<br/>not give up. Those, however, who wonder why the distribution of tuning<br/>systems in the world is as it is might greatly benefit from the paper.</p><p>The study proves for the first time that the spectral content of human<br/>speech sounds is universally biased towards the frequency ratios that occur<br/>in the consonant intervals of the common 12-tone scale (irrespective of<br/>tuning system).</p><p>It had previously been known that the auditory system of humans, and of<br/>other mammals, is biased towards these frequency ratios, as seen in the<br/>anatomy of the apparatus of pitch extraction and in psychoacoustic<br/>experiments.</p><p>That it now turns out that the bias in hearing apparently is an adaptation<br/>to the bias in vocalization could be expected. Therefore the new results are<br/>not revolutionary, but they present an important missing link.</p><p>By the way, the press release from Duke is here:</p><p><a href="http://www.dukenews.duke.edu/news/newsrelease.asp?id=2653&catid=2,46&cpg=new">http://www.dukenews.duke.edu/news/newsrelease.asp?id=2653&catid=2,46&cpg=new</a><br/>srelease.asp</p><p>An interesting side result of the study is that male speech, but not female<br/>speech, includes a bias towards the minor third, whereas female speech<br/>includes a stronger bias towards the major third (Fig. 2D). The reason for<br/>this sex difference is the lower fundamental in male voices, which means<br/>that different partials are favored by the resonance of the vocal tract.</p><p>I am very pleased about this side result, because I found a corresponding<br/>sex difference in the distribution of frequency ratios in &gt;5000 pairs of<br/>&quot;ear tones&quot; (spontaneous otoacoustic emissions) in the mid-90s. In those<br/>days I could only publish the finding, but not discuss it, because I did not<br/>have a clue as to possible explanations:</p><p><a href="http://w1.570.telia.com/~u57011259/Braun%201997%20abstract.htm">http://w1.570.telia.com/~u57011259/Braun%201997%20abstract.htm</a></p><p>Later, when the close relation between hearing and speech became more and<br/>more compelling in other data, I guessed that the sex difference in partial<br/>resonance was probably related to this odd finding. I am glad that there is<br/>now an empirical confirmation of this detail in the speech-hearing<br/>coadaptation.</p><p>Martin</p></div><h3><a id=46356 href="#46356">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>8/10/2003 12:33:23 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:</p><p>It apparently confuses 5-limit consonances with 12-equal.</p><p>&gt; That it now turns out that the bias in hearing apparently is an<br/>adaptation<br/>&gt; to the bias in vocalization could be expected.</p><p>How do you know which is cause and which is effect?</p></div><h3><a id=46357 href="#46357">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/10/2003 2:06:54 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt;<br/>&gt; It apparently confuses 5-limit consonances with 12-equal.</p><p>You are right, if you replace &quot;12-equal&quot; by &quot;chromatic 12-tone<br/>scale&quot;. That&apos;s the term the authors used.</p><p>&quot;5-limit consonances&quot; would not have been a term the authors could<br/>have used. Not many readers would understand it.</p><p>&gt; &gt; That it now turns out that the bias in hearing apparently is an<br/>&gt; adaptation<br/>&gt; &gt; to the bias in vocalization could be expected.<br/>&gt;<br/>&gt; How do you know which is cause and which is effect?</p><p>The speech side has been the &quot;harder&quot; one in evolution, because here<br/>the parameters are mainly determined by the anatomy of the breathing<br/>and eating organs. The auditory neural system has been the &quot;softer&quot;<br/>side.</p><p>Martin</p></div><h3><a id=46358 href="#46358">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/10/2003 3:22:24 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; It apparently confuses 5-limit consonances with 12-equal.</p><p>They aren&apos;t all 5-limit.  The minor 7th is 7:4 and the tritone seems to be 31:22.</p><p>                 Graham</p></div><h3><a id=46359 href="#46359">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/10/2003 3:44:15 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Martin Braun wrote:</p><p>&gt; The paper does not make any suggestion what anybody might like to give up or<br/>&gt; not give up. Those, however, who wonder why the distribution of tuning<br/>&gt; systems in the world is as it is might greatly benefit from the paper.</p><p>The paper itself isn&apos;t as bad as the writeup suggests (as is usually the case).  But it still isn&apos;t that valuable.  It gives a very misleading impression of the range of tuning systems -- everybody uses a 12 note scale or subsets, except for &quot;some interesting variations&quot;.</p><p>I don&apos;t see it being much use at all to a general tuning enthusiast. There are plenty of instruments that emphasize the lower harmonics, and plenty of commentaries on this.</p><p>&gt; The study proves for the first time that the spectral content of human<br/>&gt; speech sounds is universally biased towards the frequency ratios that occur<br/>&gt; in the consonant intervals of the common 12-tone scale (irrespective of<br/>&gt; tuning system).</p><p>But *any* intervals can occur in any scale &quot;irrespective of tuning system&quot;.  They haven&apos;t shown any significant correlation between the intervals they get and either 12 note scale (equal temperament and Pythagorean intonation).  They don&apos;t consider any other scales as controls.  All they show is that speech emphasizes the lower partials.</p><p>&gt; That it now turns out that the bias in hearing apparently is an adaptation<br/>&gt; to the bias in vocalization could be expected. Therefore the new results are<br/>&gt; not revolutionary, but they present an important missing link.</p><p>The experimental results may be important, but not of widespread interest.  It&apos;s a shame they didn&apos;t stick with reporting those results -- a lot of the paper isn&apos;t that good at all.</p><p>&gt; By the way, the press release from Duke is here:<br/>&gt; &gt; <a href="http://www.dukenews.duke.edu/news/newsrelease.asp?id=2653&catid=2,46&cpg=new">http://www.dukenews.duke.edu/news/newsrelease.asp?id=2653&catid=2,46&cpg=new</a><br/>&gt; srelease.asp</p><p>That explains where New Scientist got all the wild ideas from.  They claim causation when they only have correlation, don&apos;t mention that the same results can be derived from musical instruments, and claim to predict the chromatic scale, instead of explaining it post hoc.</p><p>                         Graham</p></div><h3><a id=46360 href="#46360">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>8/10/2003 4:10:04 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Graham Breed &lt;graham@m...&gt; wrote:<br/>&gt; Gene Ward Smith wrote:<br/>&gt;<br/>&gt; &gt; It apparently confuses 5-limit consonances with 12-equal.<br/>&gt;<br/>&gt; They aren&apos;t all 5-limit.  The minor 7th is 7:4 and the tritone<br/>seems to<br/>&gt; be 31:22.</p><p>Then why in hell did they drag 12-et into it?</p><p>I find it hard to buy that we need to go to the 31-limit for this<br/>stuff.</p></div><h3><a id=46363 href="#46363">ðŸ”—</a>Jeff Olliff &#x3C;jolliff@dslnorthwest.net&#x3E;</h3><span>8/11/2003 12:16:00 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Abstract:<br/>The similarity of musical scales and consonance judgments across<br/>human populations has no generally accepted explanation. Here we<br/>present evidence that these aspects of auditory perception arise<br/>from the statistical structure of naturally occurring periodic sound<br/>stimuli. An analysis of speech sounds, the principal source of<br/>periodic sound stimuli in the human acoustical environment, shows<br/>that the probability distribution of amplitude-frequency<br/>combinations in human utterances predicts both the structure of the<br/>chromatic scale and consonance ordering. These observations suggest<br/>that what we hear is determined by the statistical relationship<br/>between acoustical stimuli and their naturally occurring sources,<br/>rather than by the physical parameters of the stimulus per se.</p><p>&gt;Martin Braun:<br/>The speech side has been the &quot;harder&quot; one in evolution, because here<br/>the parameters are mainly determined by the anatomy of the breathing<br/>and eating organs. The auditory neural system has been the &quot;softer&quot;<br/>side.</p><p>The amplitude-frequency combinations they are measuring seem to be<br/>an aggregate of the same thing phonologists measure at a finer<br/>grain, the formant distribution in vowel sounds.  Vowels are<br/>recognized by the distinct placement of at least three bands of<br/>frequencies at which the vocal tract resonates.  These resonances<br/>are controlled by movements of the tongue and lips, are independent<br/>of the excitation frequency from the vocal chords, but serve to<br/>emphasize harmonics falling within the formant regions.  Each<br/>language divides this available vowel space in a slightly different<br/>way, but the nature of the formant resonator is of a continuously<br/>variable device, starting perhaps with the tongue bunched at the<br/>roof and the lips either drawn or pursed for an ee or umlaut, and<br/>then while continuing to vocalize, drawing the tongue slowly back,<br/>down, and forward, adjusting the lips as one may please.  This may<br/>produce a continuous series of recognizable vowel sounds, something<br/>like ee-e-a-aw-o-oo-uu, according to ones linguistic heritage.  I<br/>view this recently evolved flexibility as catering to the<br/>sensitivities of the ear, which has been fine tuned (shall we say)<br/>during an enourmously longer period.</p><p>The investigators&apos; result suggests that an analysis of vowel<br/>formants within a language, and evidently across languages, will<br/>also show interval relationships among the three formants of the<br/>approximately ten vowels in a typical language.  This is<br/>interesting.  I did not notice it in my sophomoric studies.  These<br/>resonances are the same sort as those of wind instruments, which<br/>influence tone color and even perception of the fundamental, but are<br/>much broader than the acute definition with which we perceive pitch.</p><p>People who tune instruments by beats and measurement devices know<br/>that perceived consonance is directly related to ratios of physical<br/>frequencies.  The interesting neurophysiological question is why the<br/>ear cares about these physical relationships.  It is almost<br/>certainly connected with language, but has been hard to<br/>demonstrate.  The ear needs to recognize the patterns of formant<br/>frequencies, as well as non-vowel transients, and has just the<br/>coiled up continuous frequency sensitive organ necessary to do it.<br/>Since what we actually hear inside of the formant bands is selected<br/>harmonics filtered from the sawtooth vocal chord fundamental, we can<br/>perhaps understand why the ear-brain is sensitive to harmony.  It<br/>can intuit a fundamental from the harmonics in the formants.  For<br/>any single vowel, the three formants will contain perhaps three<br/>related harmonics of the fundamental.  The statistical analysis may<br/>see these harmonic relationships within the vowels as more than a<br/>random correlation.  Then any other harmonic occurring within the<br/>same formant may be statistically associated with the first (even if<br/>a different vowel, different speaker, different language, or<br/>different fundamental).  So given that the frequency structure of a<br/>human vowel is harmonically based, and that the lowest order<br/>intervals 2/1, 3/2, 4/3, 5/4, 6/5 expand upon each other to produce<br/>a first order approximation of a twelvish tone scale, we should not<br/>find the result unbelievable.</p><p>It still seems curious that the frequencies of the formant bands<br/>themselves are arranged in some kind of twelve-tone row, but worth a<br/>try.  The patterns vary by language and dialect, but should follow<br/>the rule of optimizing signal to noise by being distinct.  The<br/>suggestion that we hear statistically is uninformed, either by<br/>musical science or linguisitics.</p><p>All in fun,<br/>Jeff</p></div><h3><a id=46365 href="#46365">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/11/2003 12:35:29 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Me:<br/>&gt;&gt;They aren&apos;t all 5-limit.  The minor 7th is 7:4 and the tritone &gt; seems to &gt;&gt;be 31:22.</p><p>Gene:<br/>&gt; Then why in hell did they drag 12-et into it?</p><p>Not all intervals are of equal importance.  It&apos;s reasonable to assume that 12 would have a unique relationship with the lower partials, but they don&apos;t show it.</p><p>&gt; I find it hard to buy that we need to go to the 31-limit for this &gt; stuff. No, sorry, I calculated that wrong.  It&apos;s a frequency ratio of 1.406 which is probablyu 45:32.</p><p>                   Graham</p></div><h3><a id=46367 href="#46367">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>8/11/2003 3:42:10 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Graham Breed &lt;graham@m...&gt; wrote:</p><p>&gt; Not all intervals are of equal importance.  It&apos;s reasonable to<br/>assume<br/>&gt; that 12 would have a unique relationship with the lower partials,<br/>but<br/>&gt; they don&apos;t show it.</p><p>What makes it reasonable to assume that 12 or any other equal<br/>temperament has anything to do with the question?</p></div><h3><a id=46368 href="#46368">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/11/2003 5:28:31 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene Ward Smith wrote:</p><p>&gt; What makes it reasonable to assume that 12 or any other equal &gt; temperament has anything to do with the question?</p><p>Who cares?  I didn&apos;t mention equal temperament and the paper only uses it as an example.</p><p>                        Graham</p></div><h3><a id=46371 href="#46371">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/11/2003 6:01:55 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Jeff Olliff wrote (46363):</p><p>&gt; The suggestion that we hear statistically is uninformed, either by<br/>musical science or linguisitics.</p><p>The authors based it on results in vision research. But they could easily<br/>have based it on hearing research as well. We have a top-down neural<br/>signaling from cortex to inner ear. And data show that even the pick-up<br/>cells in the inner ear have increased sensitivity to the frequencies they<br/>have been most exposed to. (Not in the disco, of course ;-))</p><p>Martin</p></div><h3><a id=46372 href="#46372">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/11/2003 5:44:02 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Graham Breed wrote (46359):</p><p>&gt; It gives a very misleading impression of the range of tuning systems --<br/>everybody uses a 12 note scale or subsets, except for &quot;some interesting<br/>variations&quot;.</p><p>They never say &quot;everybody&quot;, but things like &quot;preferential use&quot; (p. 7160).<br/>This is perfectly correct and in no way &quot;a very misleading impression&quot;.</p><p>&gt; I don&apos;t see it being much use at all to a general tuning enthusiast.<br/>There are plenty of instruments that emphasize the lower harmonics, and<br/>plenty of commentaries on this.</p><p>Yes, there is a wide-spread view that humans have their harmony templates<br/>from hearing musical instruments. The data now show that this is wrong. They<br/>are, of course, no surprise for those who think in terms of biological<br/>evolution and development of individual human brains. The vocalization of<br/>mammals is quite a bit older than musical instruments, and most infants hear<br/>quite a bit more of speech than of musical instruments.</p><p>&gt; They ...claim to predict the chromatic scale, instead of explaining it<br/>post hoc.</p><p>It is the same in this case. The data (not the authors) predict a bias<br/>towards the chromatic 12-tone scale. The authors thus explain the dominance<br/>of this scale post hoc.</p><p>Martin</p></div><h3><a id=46377 href="#46377">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>8/11/2003 11:57:40 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;It is the same in this case. The data (not the authors) predict a<br/>&gt;bias towards the chromatic 12-tone scale. The authors thus explain<br/>&gt;the dominance of this scale post hoc.</p><p>Martin,</p><p>Our gripe is that they are really explaining the dominance of just<br/>intonation.  The 12-tone scale happens to be exceptionally good at<br/>representing JI among small equal temperaments, but there are many<br/>other scales that would fit the data.  The study seems fine, but<br/>the authors should have done more research to find the right music-<br/>theory terminology for their paper.  The problem is that people like<br/>Eytan might now use this as ammo to justify their insane revisionist<br/>accounts of history.</p><p>-Carl</p></div><h3><a id=46390 href="#46390">ðŸ”—</a>Graham Breed &#x3C;graham@microtonal.co.uk&#x3E;</h3><span>8/11/2003 2:37:25 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Martin Braun wrote:</p><p>&gt; They never say &quot;everybody&quot;, but things like &quot;preferential use&quot; (p. 7160).<br/>&gt; This is perfectly correct and in no way &quot;a very misleading impression&quot;.</p><p>They say &quot;all cultures over the centuries&quot; (p.7164).  That looks like &quot;everybody&quot; to me.  Are you saying there are people who don&apos;t belong to a culture, or lived outside the centuries?</p><p>They also say on p.7164 &quot;All musical traditions ... employ a relatively small set of tonal intervals ... each interval being defined by its relationship to the lowest tone of the set.&quot;  It&apos;s not important to the matter at hand, but what a howler!  Ancient Greeks, who measured from the *highest* tone in the set, don&apos;t constitute a musical tradition!</p><p>On the very p.7160, they say &quot;&grave;musical universals&apos; include: (1) a division of the continuous dimension of pitch into iterated sets of 12 intervals that define the chromatic scale.&quot;  &quot;Universal&quot; includes everything.  Something without a &quot;musical universal&quot; is not music. Hence anybody making music is obliged to use a 12 note scale, or subsets.  That&apos;s absurd.  They do say it.</p><p>The phrase &quot;preferential use&quot; refers not to use of the chromatic scale itself, but to subsets of it.  The universality of the chromatic scale itself has already been asserted at that point.</p><p>&gt; Yes, there is a wide-spread view that humans have their harmony templates<br/>&gt; from hearing musical instruments. The data now show that this is wrong. They<br/>&gt; are, of course, no surprise for those who think in terms of biological<br/>&gt; evolution and development of individual human brains. The vocalization of<br/>&gt; mammals is quite a bit older than musical instruments, and most infants hear<br/>&gt; quite a bit more of speech than of musical instruments.</p><p>The data do now show that!  All they do is not contradict it.  If you start out thinking that harmonic templates derive from speech sounds, you can still think so.  If you think they&apos;re irrelevant, you can say the results are a coincidence.  The only way of deciding between the theories is to test people who have been exposed to music that predominantly uses inharmonic timbres, and see if their templates are different.  Even then, harmonic templates are only one facet of a musical scale.</p><p>From what I can make out, the data show a &quot;diatonic&quot; 7-note scale for female English speakers, and Mandarin and Tamil speakers in general.  I think you&apos;ll find that infants have heard quite a bit more female than male speech!</p><p>&gt; It is the same in this case. The data (not the authors) predict a bias<br/>&gt; towards the chromatic 12-tone scale. The authors thus explain the dominance<br/>&gt; of this scale post hoc.</p><p>The data predict small integer ratios being consonant -- the same as every other theory of consonance with harmonic timbres.</p><p>                    Graham</p></div><h3><a id=46391 href="#46391">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>8/11/2003 3:07:44 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:</p><p>&gt; It is the same in this case. The data (not the authors) predict a<br/>bias<br/>&gt; towards the chromatic 12-tone scale. The authors thus explain the<br/>dominance<br/>&gt; of this scale post hoc.</p><p>If the data truly does point to 12-equal, that would be a strong<br/>argument that the thesis is wrong, and it is exposure to music, not<br/>speech, which is determinative.</p></div><h3><a id=46413 href="#46413">ðŸ”—</a>hstraub64 &#x3C;straub@datacomm.ch&#x3E;</h3><span>8/18/2003 3:58:53 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt; &gt;<br/>&gt; How do you know which is cause and which is effect?<br/>&gt;<br/>&gt; The speech side has been the &quot;harder&quot; one in evolution, because<br/>&gt; here the parameters are mainly determined by the anatomy of the<br/>&gt;breathing and eating organs. The auditory neural system has been<br/>&gt;the &quot;softer&quot; side.<br/>&gt;</p><p>I disagree. You cannot say the &quot;hardware&quot; was there before<br/>the &quot;software&quot;. Long before the development of speech &quot;harder&quot;<br/>and &quot;softer&quot; sides had already been evolving together, influencing<br/>each other in both directions.<br/>--<br/>HAns Straub</p></div><h3><a id=46417 href="#46417">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/19/2003 5:45:23 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hans Straub wrote:</p><p>&gt; &gt; The speech side has been the &quot;harder&quot; one in evolution, because<br/>&gt; &gt; here the parameters are mainly determined by the anatomy of the<br/>&gt; &gt;breathing and eating organs. The auditory neural system has been<br/>&gt; &gt;the &quot;softer&quot; side.<br/>&gt; &gt;<br/>&gt;<br/>&gt; I disagree. You cannot say the &quot;hardware&quot; was there before<br/>&gt; the &quot;software&quot;. Long before the development of speech &quot;harder&quot;<br/>&gt; and &quot;softer&quot; sides had already been evolving together, influencing<br/>&gt; each other in both directions.</p><p>The issue is not related to hardware and software. I just said that the<br/>parameters of the air spaces in the vocal tract are mainly determined by the<br/>needs of breathing and eating. Therefore they have very little room for<br/>adaptations in vocalization. The auditory neural system has no such<br/>limitations. Therefore I called it &quot;evolutionary softer&quot;.</p><p>Martin</p></div><h3><a id=46427 href="#46427">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>8/19/2003 2:31:22 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;hstraub64&quot; &lt;straub@d...&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt; &gt;<br/>&gt; &gt; How do you know which is cause and which is effect?<br/>&gt; &gt;<br/>&gt; &gt; The speech side has been the &quot;harder&quot; one in evolution, because<br/>&gt; &gt; here the parameters are mainly determined by the anatomy of the<br/>&gt; &gt;breathing and eating organs. The auditory neural system has been<br/>&gt; &gt;the &quot;softer&quot; side.<br/>&gt; &gt;<br/>&gt;<br/>&gt; I disagree. You cannot say the &quot;hardware&quot; was there before<br/>&gt; the &quot;software&quot;. Long before the development of speech &quot;harder&quot;<br/>&gt; and &quot;softer&quot; sides had already been evolving together, influencing<br/>&gt; each other in both directions.<br/>&gt; --<br/>&gt; HAns Straub</p><p>let&apos;s get a grip. what we&apos;re talking about is the harmonic spectrum<br/>of the human voice. every known mechanism of making an indefinitely<br/>sustainable acoustic sound with discrete spectrum (i.e., not noise or<br/>chaos) produces a harmonic spectrum, as it involves periodic<br/>vibration. there is not much chance that the human voice could have<br/>evolved to produce an inharmonic spectrum! the anatomy of the<br/>breathing and eating organs could be wildly different and still the<br/>same patterns observed in the paper would show up.</p></div><h3><a id=46460 href="#46460">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/20/2003 3:56:20 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:<br/>&gt; let&apos;s get a grip. what we&apos;re talking about is the harmonic spectrum<br/>&gt; of the human voice. every known mechanism of making an indefinitely<br/>&gt; sustainable acoustic sound with discrete spectrum (i.e., not noise or<br/>&gt; chaos) produces a harmonic spectrum,</p><p>many idiophones don&apos;t. The replicas of the Zeng bells were a drastic<br/>example. Discrete spectrum, yes, but only occasional low-order frequency<br/>ratios, and never a series of partials that could cause a pitch percept, or<br/>even interfere with the pitch percept based on the fundamental. The<br/>spreadsheets are still on my website:</p><p><a href="http://w1.570.telia.com/~u57011259/PartialsA-tone.htm">http://w1.570.telia.com/~u57011259/PartialsA-tone.htm</a></p><p><a href="http://w1.570.telia.com/~u57011259/PartialsB-tone.htm">http://w1.570.telia.com/~u57011259/PartialsB-tone.htm</a></p><p><a href="http://w1.570.telia.com/~u57011259/C:\website2\PartialsA+B-tone.htm">http://w1.570.telia.com/~u57011259/C:\website2\PartialsA+B-tone.htm</a></p><p>&gt; as it involves periodic<br/>&gt; vibration. there is not much chance that the human voice could have<br/>&gt; evolved to produce an inharmonic spectrum! the anatomy of the<br/>&gt; breathing and eating organs could be wildly different and still the<br/>&gt; same patterns observed in the paper would show up.</p><p>No. All throats produce harmonic patterns, but different throat sizes<br/>produce different harmonic patterns. Further, the pattern is f0 dependent,<br/>as the sex differences show.</p><p>The data of the study confirm that resonance in air spaces produces harmonic<br/>patterns. This alone, of course, would be the most boring result. The<br/>interesting result is the SPECIFIC pattern of harmonicity that comes out of<br/>human mouths.</p><p>Nobody could have predicted, based on the finest possible tuning maths, that<br/>the sex difference concerning the thirds is as it is.</p><p>And nobody could have predicted that the limit between visible peaks and<br/>noise in the spectrum statistics is just between 7:5 (still on the visible<br/>peaks&apos; side) and 7:6 (already on the lost-in-noise side).</p><p>These two findings were unexpected, and they had to remain unexpected, until<br/>&quot;some idiots&quot; started to collect data.</p><p>Martin</p></div><h3><a id=46461 href="#46461">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/20/2003 4:04:54 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:<br/>&gt; let&apos;s get a grip. what we&apos;re talking about is the harmonic spectrum<br/>&gt; of the human voice. every known mechanism of making an indefinitely<br/>&gt; sustainable acoustic sound with discrete spectrum (i.e., not noise or<br/>&gt; chaos) produces a harmonic spectrum,</p><p>many idiophones don&apos;t. The replicas of the Zeng bells were a drastic<br/>example. Discrete spectrum, yes, but only occasional low-order frequency<br/>ratios, and never a series of partials that could cause a pitch percept, or<br/>even interfere with the pitch percept based on the fundamental. The<br/>spreadsheets are still on my website:</p><p><a href="http://w1.570.telia.com/~u57011259/PartialsA-tone.htm">http://w1.570.telia.com/~u57011259/PartialsA-tone.htm</a></p><p><a href="http://w1.570.telia.com/~u57011259/PartialsB-tone.htm">http://w1.570.telia.com/~u57011259/PartialsB-tone.htm</a></p><p><a href="http://w1.570.telia.com/~u57011259/C:\website2\PartialsA+B-tone.htm">http://w1.570.telia.com/~u57011259/C:\website2\PartialsA+B-tone.htm</a></p><p>&gt; as it involves periodic<br/>&gt; vibration. there is not much chance that the human voice could have<br/>&gt; evolved to produce an inharmonic spectrum! the anatomy of the<br/>&gt; breathing and eating organs could be wildly different and still the<br/>&gt; same patterns observed in the paper would show up.</p><p>No. All throats produce harmonic patterns, but different throat sizes<br/>produce different harmonic patterns. Further, the pattern is f0 dependent,<br/>as the sex differences show.</p><p>The data of the study confirm that resonance in air spaces produces harmonic<br/>patterns. This alone, of course, would be the most boring result. The<br/>interesting result is the SPECIFIC pattern of harmonicity that comes out of<br/>human mouths.</p><p>Nobody could have predicted, based on the finest possible tuning maths, that<br/>the sex difference concerning the thirds is as it is.</p><p>And nobody could have predicted that the limit between visible peaks and<br/>noise in the spectrum statistics is just between 7:5 (still on the visible<br/>peaks&apos; side) and 7:6 (already on the lost-in-noise side).</p><p>These two findings were unexpected, and they had to remain unexpected, until<br/>&quot;some idiots&quot; started to collect data.</p><p>Martin</p></div><h3><a id=46462 href="#46462">ðŸ”—</a>hstraub64 &#x3C;straub@datacomm.ch&#x3E;</h3><span>8/20/2003 4:10:04 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt;<br/>&gt; The issue is not related to hardware and software. I just said<br/>that the<br/>&gt; parameters of the air spaces in the vocal tract are mainly<br/>determined by the<br/>&gt; needs of breathing and eating. Therefore they have very little<br/>room for<br/>&gt; adaptations in vocalization. The auditory neural system has no such<br/>&gt; limitations. Therefore I called it &quot;evolutionary softer&quot;.<br/>&gt;</p><p>Aha. So this would mean that the mentioned characteristics are not<br/>specific for humans, but would also be found in voices of apes or<br/>_any_ air-breathing animal. Is this so? I am not qualified, but it<br/>appears to be a rather bold statement.<br/>What I know is that apes, e.g., can never learn to speak properly,<br/>for anatomic reasons. (Some can learn sign language, so it is not a<br/>question of &quot;software&quot;...)</p><p>Hans Straub</p></div><h3><a id=46467 href="#46467">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/20/2003 10:57:38 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hans:</p><p>&gt; Aha. So this would mean that the mentioned characteristics are not<br/>&gt; specific for humans, but would also be found in voices of apes or<br/>&gt; _any_ air-breathing animal. Is this so? I am not qualified, but it<br/>&gt; appears to be a rather bold statement.</p><p>It is so. In general. There are some differences in the details, similar to<br/>those found between men, women, and children.</p><p>&gt; What I know is that apes, e.g., can never learn to speak properly,<br/>&gt; for anatomic reasons. (Some can learn sign language, so it is not a<br/>&gt; question of &quot;software&quot;...)</p><p>There is a theory that they have not enough space between the throat and the<br/>teeth to shape vowels as we do. But even if this is true, it would be<br/>irrelevant for speech. Humans have speech with 5 or 20 vowels. Apes surely<br/>would do fine with three, as well. For speech one vowel would also be<br/>enough.</p><p>Martin</p></div><h3><a id=46481 href="#46481">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>8/20/2003 3:51:20 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Paul:<br/>&gt; &gt; let&apos;s get a grip. what we&apos;re talking about is the harmonic<br/>spectrum<br/>&gt; &gt; of the human voice. every known mechanism of making an<br/>indefinitely<br/>&gt; &gt; sustainable acoustic sound with discrete spectrum (i.e., not<br/>noise or<br/>&gt; &gt; chaos) produces a harmonic spectrum,<br/>&gt;<br/>&gt; many idiophones don&apos;t.</p><p>those are not indefintely sustainable when played in the manner in<br/>which the measurements were made.</p><p>&gt; &gt; as it involves periodic<br/>&gt; &gt; vibration. there is not much chance that the human voice could<br/>have<br/>&gt; &gt; evolved to produce an inharmonic spectrum! the anatomy of the<br/>&gt; &gt; breathing and eating organs could be wildly different and still<br/>the<br/>&gt; &gt; same patterns observed in the paper would show up.<br/>&gt;<br/>&gt; No. All throats produce harmonic patterns, but different throat<br/>sizes<br/>&gt; produce different harmonic patterns.</p><p>right, but as long as the pattern is harmonic, the same general<br/>conclusion is reached.</p><p>&gt; The data of the study confirm that resonance in air spaces produces<br/>harmonic<br/>&gt; patterns.</p><p>how could they not?</p><p>&gt; Nobody could have predicted, based on the finest possible tuning<br/>maths, that<br/>&gt; the sex difference concerning the thirds is as it is.</p><p>ok, but that&apos;s not the conclusion i was referring to, and wasn&apos;t even<br/>mentioned in the summary that was linked to here.</p></div><h3><a id=46510 href="#46510">ðŸ”—</a>Joseph Pehrson &#x3C;jpehrson@rcn.com&#x3E;</h3><span>8/22/2003 8:37:24 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:</p><p><a href="/tuning/topicId_46259.html#46377">/tuning/topicId_46259.html#46377</a></p><p>&gt; &gt;It is the same in this case. The data (not the authors) predict a<br/>&gt; &gt;bias towards the chromatic 12-tone scale. The authors thus explain<br/>&gt; &gt;the dominance of this scale post hoc.<br/>&gt;<br/>&gt; Martin,<br/>&gt;<br/>&gt; Our gripe is that they are really explaining the dominance of just<br/>&gt; intonation.  The 12-tone scale happens to be exceptionally good at<br/>&gt; representing JI among small equal temperaments, but there are many<br/>&gt; other scales that would fit the data.</p><p>***This is exactly what I was thinking upon reading the sophomoric<br/>abstract.  Paul Erlich&apos;s chart of the ETs clearly points this out.<br/>Sure, 12 is a winner, but there are many others, and the musical<br/>resources of 12 have been much exhausted...</p><p>J. Pehrson</p></div><h3><a id=46512 href="#46512">ðŸ”—</a>Joseph Pehrson &#x3C;jpehrson@rcn.com&#x3E;</h3><span>8/22/2003 8:52:58 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:</p><p><a href="/tuning/topicId_46259.html#46427">/tuning/topicId_46259.html#46427</a></p><p>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;hstraub64&quot; &lt;straub@d...&gt; wrote:<br/>&gt; &gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt;<br/>wrote:<br/>&gt; &gt; &gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Gene Ward Smith&quot; &lt;gwsmith@s...&gt;<br/>&gt;<br/>&gt; &gt; &gt; How do you know which is cause and which is effect?<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; The speech side has been the &quot;harder&quot; one in evolution, because<br/>&gt; &gt; &gt; here the parameters are mainly determined by the anatomy of the<br/>&gt; &gt; &gt;breathing and eating organs. The auditory neural system has been<br/>&gt; &gt; &gt;the &quot;softer&quot; side.<br/>&gt; &gt; &gt;<br/>&gt; &gt;<br/>&gt; &gt; I disagree. You cannot say the &quot;hardware&quot; was there before<br/>&gt; &gt; the &quot;software&quot;. Long before the development of speech &quot;harder&quot;<br/>&gt; &gt; and &quot;softer&quot; sides had already been evolving together,<br/>influencing<br/>&gt; &gt; each other in both directions.<br/>&gt; &gt; --<br/>&gt; &gt; HAns Straub<br/>&gt;<br/>&gt; let&apos;s get a grip. what we&apos;re talking about is the harmonic spectrum<br/>&gt; of the human voice. every known mechanism of making an indefinitely<br/>&gt; sustainable acoustic sound with discrete spectrum (i.e., not noise<br/>or<br/>&gt; chaos) produces a harmonic spectrum, as it involves periodic<br/>&gt; vibration. there is not much chance that the human voice could have<br/>&gt; evolved to produce an inharmonic spectrum! the anatomy of the<br/>&gt; breathing and eating organs could be wildly different and still the<br/>&gt; same patterns observed in the paper would show up.</p><p>***This paper really seems dumber and dumber all the time... (A movie<br/>title??)</p><p>J. Pehrson</p></div><h3><a id=46516 href="#46516">ðŸ”—</a>Joseph Pehrson &#x3C;jpehrson@rcn.com&#x3E;</h3><span>8/22/2003 9:12:15 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:</p><p><a href="/tuning/topicId_46259.html#46467">/tuning/topicId_46259.html#46467</a></p><p>&gt; Hans:<br/>&gt;<br/>&gt; &gt; Aha. So this would mean that the mentioned characteristics are not<br/>&gt; &gt; specific for humans, but would also be found in voices of apes or<br/>&gt; &gt; _any_ air-breathing animal. Is this so? I am not qualified, but it<br/>&gt; &gt; appears to be a rather bold statement.<br/>&gt;<br/>&gt; It is so. In general. There are some differences in the details,<br/>similar to<br/>&gt; those found between men, women, and children.<br/>&gt;<br/>&gt; &gt; What I know is that apes, e.g., can never learn to speak properly,<br/>&gt; &gt; for anatomic reasons. (Some can learn sign language, so it is not<br/>a<br/>&gt; &gt; question of &quot;software&quot;...)<br/>&gt;<br/>&gt; There is a theory that they have not enough space between the<br/>throat and the<br/>&gt; teeth to shape vowels as we do. But even if this is true, it would<br/>be<br/>&gt; irrelevant for speech. Humans have speech with 5 or 20 vowels. Apes<br/>surely<br/>&gt; would do fine with three, as well. For speech one vowel would also<br/>be<br/>&gt; enough.<br/>&gt;<br/>&gt; Martin</p><p>***Maybe they just don&apos;t concentrate enough to do it...</p><p>J. Pehrson</p></div><h3><a id=46555 href="#46555">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>8/26/2003 5:43:49 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Paul and Martin,</p><p>I am just back from vacations. I read this tread with much interest<br/>but it seem that you had access to the original article. Is it<br/>possible to  provide me with it.</p><p>Thanks in advance</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46556 href="#46556">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/26/2003 6:25:42 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>OK, it&apos;s in your mail. Martin</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&lt;francois.laferriere@o...&gt; wrote:<br/>&gt; Hello Paul and Martin,<br/>&gt;<br/>&gt; I am just back from vacations. I read this tread with much interest<br/>&gt; but it seem that you had access to the original article. Is it<br/>&gt; possible to  provide me with it.<br/>&gt;<br/>&gt; Thanks in advance<br/>&gt;<br/>&gt; Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46560 href="#46560">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>8/26/2003 9:03:32 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Thank Martin for the paper</p><p>I didn&apos;t had to read the entire paper to discover a major flaw in the<br/>methodology that makes all the results void of any value.</p><p>With this methodology, you can take harmonic spectrum with random F0<br/>and random spectral enveloppe (let say with an average decay of a few<br/>dB per octave), you will get peaks corresponding to the simpler<br/>integer ratios (as in the article) and nothing else.</p><p>Let see how it work.</p><p>The normalized value is defined (in the paper) as</p><p>Fn = F/Fm</p><p>As the signal is harmonic F and Fm are restricted to values in the<br/>serie f0, 2f0, 3f0 ...</p><p>Thus,</p><p>Fn = ( a * f0 ) / ( b * f0 ) = a/b , a an b integer.</p><p>It should be noted that Fm is not the interpolated value of the first<br/>formant frequency (F1) but the frequency of the harmonic nearest to<br/>first formant.</p><p>As a male voice is unlikely to go below 100 Hz (in normal speech) and<br/>that the first formant rarely goes beyond 700 Hz, we can state that<br/>the maximum value for &quot;b&quot; is 7.</p><p>As the graph is presented between 1 and 2, the value of &quot;a&quot; is also<br/>constrained to be b &lt; a &lt; 14. That can only lead to a preeminence of<br/>the simple ratio corresponding to the &quot;natural&quot; scale.</p><p>In fact, the normalisation of frequency forces individual sample<br/>values to be distributed as they are. For instance, the peak at 2<br/>correspond to a/b = 2, the peak at 1.5 correspond at a/b 3/2 and so<br/>on. The spreading of the peaks correspond only to noise,  measurement<br/>error and roundoff error.</p><p>In this scheme 2 has good chance (2/1, 4/2, 6/3), 1.5 is not unlikely<br/>(3/2 6/4), 5/4 and 6/5 do exists, 11/9 or 81/80 dont have any chance.</p><p>I didn&apos;t dare to read carefully the conclusions as long as it is<br/>based on a totally flawed science.</p><p>Would I have been the reviewer, I would never have accepted such a<br/>paper.</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; OK, it&apos;s in your mail. Martin<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&gt; &lt;francois.laferriere@o...&gt; wrote:<br/>&gt; &gt; Hello Paul and Martin,<br/>&gt; &gt;<br/>&gt; &gt; I am just back from vacations. I read this tread with much<br/>interest<br/>&gt; &gt; but it seem that you had access to the original article. Is it<br/>&gt; &gt; possible to  provide me with it.<br/>&gt; &gt;<br/>&gt; &gt; Thanks in advance<br/>&gt; &gt;<br/>&gt; &gt; Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46561 href="#46561">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/26/2003 11:07:44 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hi Fran&ccedil;ois,</p><p>you can&apos;t simply state that the paper is flawed by writing an unclear<br/>criticism on a methods issue. You have to quote a finding, and after<br/>that you have to show why you think that this finding is false,<br/>uninteresting, or whatever.</p><p>Martin</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&lt;francois.laferriere@o...&gt; wrote:<br/>&gt; Thank Martin for the paper<br/>&gt;<br/>&gt; I didn&apos;t had to read the entire paper to discover a major flaw in<br/>the<br/>&gt; methodology that makes all the results void of any value.<br/>&gt;<br/>&gt; With this methodology, you can take harmonic spectrum with random<br/>F0<br/>&gt; and random spectral enveloppe (let say with an average decay of a<br/>few<br/>&gt; dB per octave), you will get peaks corresponding to the simpler<br/>&gt; integer ratios (as in the article) and nothing else.<br/>&gt;<br/>&gt; Let see how it work.<br/>&gt;<br/>&gt; The normalized value is defined (in the paper) as<br/>&gt;<br/>&gt; Fn = F/Fm<br/>&gt;<br/>&gt; As the signal is harmonic F and Fm are restricted to values in the<br/>&gt; serie f0, 2f0, 3f0 ...<br/>&gt;<br/>&gt; Thus,<br/>&gt;<br/>&gt; Fn = ( a * f0 ) / ( b * f0 ) = a/b , a an b integer.<br/>&gt;<br/>&gt; It should be noted that Fm is not the interpolated value of the<br/>first<br/>&gt; formant frequency (F1) but the frequency of the harmonic nearest to<br/>&gt; first formant.<br/>&gt;<br/>&gt; As a male voice is unlikely to go below 100 Hz (in normal speech)<br/>and<br/>&gt; that the first formant rarely goes beyond 700 Hz, we can state that<br/>&gt; the maximum value for &quot;b&quot; is 7.<br/>&gt;<br/>&gt; As the graph is presented between 1 and 2, the value of &quot;a&quot; is also<br/>&gt; constrained to be b &lt; a &lt; 14. That can only lead to a preeminence<br/>of<br/>&gt; the simple ratio corresponding to the &quot;natural&quot; scale.<br/>&gt;<br/>&gt; In fact, the normalisation of frequency forces individual sample<br/>&gt; values to be distributed as they are. For instance, the peak at 2<br/>&gt; correspond to a/b = 2, the peak at 1.5 correspond at a/b 3/2 and so<br/>&gt; on. The spreading of the peaks correspond only to noise,<br/>measurement<br/>&gt; error and roundoff error.<br/>&gt;<br/>&gt; In this scheme 2 has good chance (2/1, 4/2, 6/3), 1.5 is not<br/>unlikely<br/>&gt; (3/2 6/4), 5/4 and 6/5 do exists, 11/9 or 81/80 dont have any<br/>chance.<br/>&gt;<br/>&gt; I didn&apos;t dare to read carefully the conclusions as long as it is<br/>&gt; based on a totally flawed science.<br/>&gt;<br/>&gt; Would I have been the reviewer, I would never have accepted such a<br/>&gt; paper.<br/>&gt;<br/>&gt; yours truly<br/>&gt;<br/>&gt; Fran&ccedil;ois Laferri&egrave;re<br/>&gt;<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; &gt; OK, it&apos;s in your mail. Martin<br/>&gt; &gt;<br/>&gt; &gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&gt; &gt; &lt;francois.laferriere@o...&gt; wrote:<br/>&gt; &gt; &gt; Hello Paul and Martin,<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; I am just back from vacations. I read this tread with much<br/>&gt; interest<br/>&gt; &gt; &gt; but it seem that you had access to the original article. Is it<br/>&gt; &gt; &gt; possible to  provide me with it.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Thanks in advance<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46577 href="#46577">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>8/27/2003 6:33:14 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Martin,</p><p>&gt; Martin wrote:<br/>&gt; you can&apos;t simply state that the paper is flawed by writing an<br/>unclear<br/>&gt; criticism on a methods issue. You have to quote a finding, and<br/>after<br/>&gt; that you have to show why you think that this finding is false,<br/>&gt; uninteresting, or whatever.</p><p>OK, but may I state that the paper is flawed by writing a clearer<br/>criticism :)?<br/>Also this is not &quot;a method issue&quot; taht I criticize, it is the very<br/>basis of the whole protocol that is flawed. It cannot make appear<br/>values elsewhere than where they appears, so it signify nothing.</p><p>I tough I was clear enough in explaining the flaw in the protocol.<br/>Either you didn&apos;t read carefully what I wote, or I was not good<br/>enough at explaining it. But once you get it, I can assure you that<br/>the flaw is pretty obvious.</p><p>If you need any specific clarification about this post or the<br/>previous one, just ask, I will be pleased to try to be more<br/>understandable.</p><p>By the way, the paper is certainly honest, as far as the protocol is<br/>provided with great detail, so that there is no evidence of any<br/>attempt to hide the flaw I discovered (I must say that only a<br/>minority of scientific papers are that clear).</p><p>My first tought was that the graphs where only a flattened version of<br/>the classical &quot;vowel triangle&quot; representing the vowels of a phonetic<br/>system on a graph where axis are F1 and F2  (first and second formant<br/>frequencies). But in fact, it is something else, that is<br/>unfortunately, much less significant... In fact it makes peaks appear<br/>not especially where formants appear but where harmonics appear.</p><p>Let consider the &quot;typical&quot; spectrum of page 7161. Fm is the frequency<br/>of the 4th harmonic, so the 5th harmonic shall contribute, on the<br/>normalised spectrum of this sample, to intense value centered on 5/4,<br/>the sixth harmonic on 3/2 and so on. We shall also have an intense<br/>contribution on 2 due to the second formant around 8th harmonic.</p><p>From this exemple, wen can see that sample contribution can produce<br/>intense value on for value corresponding to integer ratio 5/4, 3/2,<br/>7/4. With Fm on 3rd harmonic, we would have contribution on 4/3, 5/3<br/>and 2, With Fm on 5th harmonic, contributions are 6/5, 7/5, 8/5, 9/5<br/>and so on for other location of Fm.</p><p>If Fm have been on 5th harmonic, we would have values on<br/>further the constraints explained in my previous mail, those ratio<br/>a/b are limited to value of b lower or equal to 7 (with a small<br/>contribution of 7 as it is an extreme value).</p><p>The sum of all contribution must have peaks at simple integer ratio,<br/>and matter of factly the simpler are more intense because they sum up<br/>more contribution: for instance 3/2 includes 3:2 but also 6:4 and<br/>9:6; 2/1 include 2:1, 4:2, 6:3, 8:4, 10:5 and 12:6 and is thus<br/>relatively intense.</p><p>Further, this value of b is practically limited to 4 for women<br/>voices. As Fm can hardly correspond to the 5th harmonic, the<br/>contribution of 6/5 is negligible for women voice. This has nothing<br/>to do with major/minor third stuff. That explains, in general, the<br/>simpler normalized spectrum for women.</p><p>OK I forgot one thing, when I wrote</p><p>&gt; The spreading of the peaks correspond only to noise, measurement<br/>&gt; error and roundoff error.</p><p>In fact the spreading of the peaks is also (and mostly) due to the<br/>FFT harmonic peak spreading caused by the shortness of the analysis<br/>window (0.1 sec). and to the possible instability of the pitch in the<br/>window.</p><p>For me, all this result from a numerical artifact due to</p><p>- the normalisation protocol<br/>- the harmonicity of the signal<br/>- the fact that Fm is contrained to correspond to low harmonics</p><p>Even though, the conclusion is attractive, this has absolutely no<br/>direct relation to musical scales.</p><p>Paul, any tought about this?</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46595 href="#46595">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>8/27/2003 2:02:02 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&lt;francois.laferriere@o...&gt; wrote:<br/>&gt; Hello Martin,<br/>&gt;<br/>&gt; &gt; Martin wrote:<br/>&gt; &gt; you can&apos;t simply state that the paper is flawed by writing an<br/>&gt; unclear<br/>&gt; &gt; criticism on a methods issue. You have to quote a finding, and<br/>&gt; after<br/>&gt; &gt; that you have to show why you think that this finding is false,<br/>&gt; &gt; uninteresting, or whatever.<br/>&gt;<br/>&gt; OK, but may I state that the paper is flawed by writing a clearer<br/>&gt; criticism :)?<br/>&gt; Also this is not &quot;a method issue&quot; taht I criticize, it is the very<br/>&gt; basis of the whole protocol that is flawed. It cannot make appear<br/>&gt; values elsewhere than where they appears, so it signify nothing.<br/>&gt;<br/>&gt; I tough I was clear enough in explaining the flaw in the protocol.<br/>&gt; Either you didn&apos;t read carefully what I wote, or I was not good<br/>&gt; enough at explaining it. But once you get it, I can assure you that<br/>&gt; the flaw is pretty obvious.<br/>&gt; [...]<br/>&gt; Paul, any tought about this?</p><p>i thought you explained it pretty well, and it agreed with what i<br/>said earlier.</p></div><h3><a id=46611 href="#46611">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/28/2003 6:56:50 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; Even though, the conclusion is attractive, this has absolutely no<br/>&gt; direct relation to musical scales.</p><p>As you can see in the figures of the paper, they only found peaks at ratios<br/>that are as simple and simpler than 7:5. The same applies to the majority of<br/>intervals within the chromatic 12-tone scale that is based on the repetition<br/>of 5ths and 3rds.</p><p>Is this a &quot;direct relation&quot;, or is it not?</p><p>Is there anything in the methods of this study that made the borderline<br/>between the inclusion of ratios (7:5 and lower) and the exclusion of ratios<br/>(7:6 and higher) lie where it lies?</p><p>If yes, please show us this &quot;trick&quot; in their methods. If no, I would suggest<br/>that you recall your criticism and apologize to list.</p><p>Martin</p></div><h3><a id=46613 href="#46613">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>8/28/2003 9:29:39 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&ccedil;ois:<br/>&gt; Even though, the conclusion is attractive, this has absolutely no</p><p>&gt; Martin:<br/>&gt; direct relation to musical scales.<br/>&gt; As you can see in the figures of the paper, they only found peaks<br/>at ratios<br/>&gt; that are as simple and simpler than 7:5. The same applies to the<br/>majority of<br/>&gt; intervals within the chromatic 12-tone scale that is based on the<br/>repetition<br/>&gt; of 5ths and 3rds.<br/>&gt;</p><p>Hello Martin,</p><p>seemingly, there is something you missed in my discussion, and it is<br/>unfortunate that you do not pinpoint exactly where in my post I have<br/>not been clear enough.</p><p>Here we go again, but that is the last time I explain at length,<br/>nobody else but you and me seems to have any interest in this thread.</p><p>stop me whenever you disagree :).</p><p>The normalized spectrum figures theoretically should be a spectrum of<br/>discrete peaks corresponding to integer ratio. If the measurement<br/>where done on much longer FFT analysis on spectrally stable segment<br/>of speech, that would be more obvious, but we can nevertheless make<br/>a &quot;though experiment&quot;.</p><p>Fm as defined in the paper represent the frequency of an harmonic<br/>near F1, that is related to the effective length of the vocal tract:<br/>the effective vocal tract length is a quarter of wavelenth. This<br/>effective length value is typically around 15 cm, so that F1<br/>typically varies between 500 and 750 Hz. It varies depending on the<br/>vowel (mouth and tongue position change effective vocal tract<br/>lenght). F1 values are higher for children and only slighly higher<br/>for women as vocal tract is shorter than men&apos;s.</p><p>The peaks of our though experiment graph appears at integer ratio<br/>corresponding to the ratio of N/Nm where N is wavenumer of harmonic<br/>and Nm is the wavenumber of Fm. The most common values for Nm are<br/>4,5,6. 7 can appears for low pitch men (around 100 Hz) and for some<br/>very open vowel only. So there is, overall, little contributions for<br/>denominator of 7 and nearly no value occurences above 7.</p><p>As the figures display only values, between 1 an 2, N can only be<br/>between 8 and 14 and the simpler ratio are redundant, more frequent,<br/>thus more intense. The exact shape of the real life curve depends on<br/>a lot of statical properties but predicticably is more intense at<br/>simpler ratio.</p><p>Make another tought experiment where subject are monsters with huge<br/>vocal folds who speak with pitch around 30 Hz (with normal vocal<br/>tract). For those, Nm is allowed to go up to 25, so we may have a<br/>much denser normalized spectrum.</p><p>But for us, normal human, we are not allowed to have complex<br/>normalized spectrum because</p><p>pitch &gt; 100 Hz<br/>F1    &lt; 750 Hz</p><p>That&apos;s it: pitch too high, vocal tract too long, to have very often 7<br/>as Nm.<br/>too bad for this theory</p><p>Is this a &quot;direct relation&quot;, or is it not?<br/>&gt; Is there anything in the methods of this study that made the<br/>borderline<br/>between the inclusion of ratios (7:5 and lower) and the exclusion of<br/>ratios<br/>(7:6 and higher) lie where it lies?</p><p>It is in the paper, check figure 3 A, 5 is nearly twice as probable<br/>to occur at denominator than 6.</p><p>&gt; If yes, please show us this &quot;trick&quot; in their methods.</p><p>As I stated in a previous post, I think that the paper is honest, and<br/>it is not a &quot;trick&quot;, or a hoax, only a misleading error in the way<br/>data was gathered</p><p>&gt; If no, I would suggest<br/>&gt; that you recall your criticism and apologize to list.</p><p>Did I offend anybody?</p><p>Should someone apologize to the list whenever one emit an opinion you<br/>disagree with?</p><p>come on ! ;-) it is a discussion group!</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46615 href="#46615">ðŸ”—</a>Carl Lumma &#x3C;ekin@lumma.org&#x3E;</h3><span>8/28/2003 11:38:46 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;Here we go again, but that is the last time I explain at length,<br/>&gt;nobody else but you and me seems to have any interest in this thread.<br/>&gt;<br/>&gt;stop me whenever you disagree :).</p><p>I&apos;m interested in it.</p><p>&gt;But for us, normal human, we are not allowed to have complex<br/>&gt;normalized spectrum because<br/>&gt;<br/>&gt;pitch &gt; 100 Hz<br/>&gt;F1    &lt; 750 Hz<br/>&gt;<br/>&gt;That&apos;s it: pitch too high, vocal tract too long, to have very<br/>&gt;often 7 as Nm. too bad for this theory</p><p>Ok, but this is not a methods objection.  It shows instead that<br/>the result is trivial.</p><p>I will re-read your earlier post, where I believe you describe<br/>the methods objection.  I didn&apos;t follow you the first time I<br/>read it.</p><p>&gt;Did I offend anybody?</p><p>Not me.  I&apos;m quite glad you spoke up!</p><p>-Carl</p></div><h3><a id=46627 href="#46627">ðŸ”—</a>hstraub64 &#x3C;straub@datacomm.ch&#x3E;</h3><span>8/29/2003 5:02:56 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, Carl Lumma &lt;ekin@l...&gt; wrote:<br/>&gt; &gt;Here we go again, but that is the last time I explain at length,<br/>&gt; &gt;nobody else but you and me seems to have any interest in this<br/>thread.<br/>&gt; &gt;<br/>&gt; &gt;stop me whenever you disagree :).<br/>&gt;<br/>&gt; I&apos;m interested in it.<br/>&gt;</p><p>I am interested, too. Just not contributing when I have nothing to<br/>say...</p></div><h3><a id=46637 href="#46637">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>8/29/2003 10:35:21 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt;&gt; Is there anything in the methods of this study that made the borderline<br/>&gt;&gt; between the inclusion of ratios (7:5 and lower) and the exclusion of<br/>&gt;&gt; ratios (7:6 and higher) lie where it lies?</p><p>&gt; It is in the paper, check figure 3 A, 5 is nearly twice as probable<br/>&gt; to occur at denominator than 6.</p><p>Of course, it&apos;s in the paper. My question was, did they put it into the<br/>paper by choosing a special trick to get it in. Fig 3A is an empirical one.<br/>It does reflect the nature of human speech, not a methods decision by the<br/>authors!!!</p><p>&gt; As I stated in a previous post, I think that the paper is honest, and<br/>&gt; it is not a &quot;trick&quot;, or a hoax, only a misleading error in the way<br/>&gt; data was gathered.</p><p>What then was this error? I can see no error in the FFT analysis.</p><p>&gt;&gt; If no, I would suggest that you recall your criticism and apologize to<br/>list.</p><p>&gt; Did I offend anybody?</p><p>That was not at issue. At issue was if you misinformed the group by stating<br/>false facts. If you did, an apology would well be in order. This has got<br/>nothing to do with having an opinion.</p><p>You claimed that the authors made a methodological error that lead to the<br/>results they received. But you did not say what their error was. If you<br/>cannot say this in three simple sentences, referring to a specific line of<br/>text in the paper, we must assume that there is no such error.</p><p>Martin</p></div><h3><a id=46662 href="#46662">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/1/2003 2:59:02 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Martin</p><p>I exposed the flaw in the method used three time, with diffrent<br/>wording and/or more detail, asking you to pinpoint exactly where you<br/>either disagree or do not understand (because seemingly I am not that<br/>good at being clear).</p><p>Seemingly, you are unwilling to enter a fair discussion, but just<br/>repeatedly state that I misinform the group (how?), that I state false<br/>fact (which one please?), and that my writing is unclear (in spite of<br/>the fact that some other group members like Paul tip me that I am not<br/>totally obscure).</p><p>After that carload of amabilities, you state that &quot;I&quot; should apologize!!!!</p><p>Ok, I take it as humour...</p><p>&gt; My question was, did they put it into the<br/>&gt; paper by choosing a special trick to get it in.<br/>&gt; Fig 3A is an empirical one.<br/>&gt; It does reflect the nature of human speech,<br/>&gt; not a methods decision by the authors!!!</p><p>In this figure 3A, there is indeed a method decision, even though,<br/>probably not a malicious one. It is &quot;currently admitted&quot; (sorry, no<br/>bibliography) that there is no significant physical coupling between<br/>the vocal fold and the vocal tract, so there is no significant<br/>correlate between f0 (the pitch) and F1, F2 .. (formant freequencies).<br/>So as random variables, f0 and F1 can be considered continuous<br/>independent random variable (or slightly correlated, that is not<br/>important for the following).</p><p>The value of Fm do not correspond to a physical property of the vocal<br/>tract conversely to F1 (effective vocal tract). F1 can be computed by<br/>various interpolation method from the signal. F1 is continuously<br/>distributed and is by no way forced to be an integer multiple of f0.</p><p>On the other hand side Fm is, by definition, a multiple of f0, thus,<br/>it is a roundoff of the real physical value (F1). Fm has no physical<br/>meaning (numerology excluded). By this simple process of rounding-off<br/>Fm to an integer multiple of f0, a continuous spectrum is artificially<br/>transformed in a discrete spectrum with a very limited number of<br/>significant value.</p><p>Harmonic number of Fm is not a &quot;natural&quot; measure, it is the result of<br/>a computation that creates an artificial set of discrete random<br/>variable that have, conviently, a very limited ranged, that are<br/>afterward &quot;shaked and baked&quot; to produce a very poor continuous<br/>&quot;normalized&quot; spectrum that is just a discrete spectrum in disguise.</p><p>So no, figure 3A reflect nothing in the nature of human speech. Taking<br/>Fm as<br/>   round (F1 / locutor height in centimeter)<br/>would have given very similar results.</p><p>Its another way to explain the error in the method</p><p>Again, as soon as you pinpoint where I am wrong/unclear, I am<br/>absolutely willing admit my error (it would not be the first time I<br/>make a fool of myself on this list) or explain until cristal-like<br/>clarity is reached (I am not that good at pedagogy, but I may try<br/>again). You are my guest Martin</p><p>&gt; Martin<br/>&gt; What then was this error? I can see no error in the FFT analysis.</p><p>No error in the FFT presented, as I explained, things goes bad just<br/>afterward.</p><p>Even more good will, I recap below, in three sentence as you asked<br/>(even though it is just a summary, not a clarication</p><p>1- The theoritical &quot;normalized&quot; spectrum is a spectrum of discrete<br/>values at some integer ratio. No surprize that actual normalized<br/>spectrum has peak.<br/>2- The physical relationship limits between f0 range and F1 range<br/>limits the number of significant peaks in interval [1,2]<br/>3- this lead to the trivial result (peak that are physically unlikely<br/>do not appear, those that are more likely due to the distribution of<br/>harmonic number of Fm as N/2, N/3, N/4 N/5 are proeminent).</p><p>All this is just due to acoustics, elementary algebrae and fairly<br/>simple stats concepts; it is totally neuroscience-free, otherwise, I<br/>would not have permit to challenge you Martin :).</p><p>All the detail is in my previous posts</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46665 href="#46665">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/1/2003 12:46:36 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt;&gt; My question was, did they put it into the<br/>&gt;&gt; paper by choosing a special trick to get it in.<br/>&gt;&gt; Fig 3A is an empirical one.<br/>&gt;&gt; It does reflect the nature of human speech,<br/>&gt;&gt; not a methods decision by the authors!!!</p><p>&gt; In this figure 3A, there is indeed a method decision, even though,<br/>&gt; probably not a malicious one. It is &quot;currently admitted&quot; (sorry, no<br/>&gt; bibliography) that there is no significant physical coupling between<br/>&gt; the vocal fold and the vocal tract, so there is no significant<br/>&gt; correlate between f0 (the pitch) and F1, F2 .. (formant freequencies).<br/>&gt; So as random variables, f0 and F1 can be considered continuous<br/>&gt; independent random variable (or slightly correlated, that is not<br/>&gt; important for the following).</p><p>OK, now I can have a cautious guess what your misunderstanding may be about.<br/>You seem to think that the study is about vowel formants. This, however, is<br/>not the case. The study deals with the ratios between the peaks in the voice<br/>spectrum, irrespective of any qualities that are related to vowels. Please<br/>have a look at that part of the paper where the sampling of the speech<br/>material is described.</p><p>&gt; On the other hand side Fm is, by definition, a multiple of f0, thus,<br/>&gt; it is a roundoff of the real physical value (F1). Fm has no physical<br/>&gt; meaning (numerology excluded).</p><p>This is wrong. The amplitudes of all peaks in the investigated sound spectra<br/>reflect the PHYSICAL resonance properties of the vocal tracts of the tested<br/>subjects.</p><p>Martin</p></div><h3><a id=46670 href="#46670">ðŸ”—</a>Jeff Olliff &#x3C;jolliff@dslnorthwest.net&#x3E;</h3><span>9/1/2003 10:15:36 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Martin, in every particular sample the energy peaks in the voice<br/>spectrum match those harmonics of the fundamental favored by the<br/>vowel formants.  Thank you for clairifying for us non-subscribers to<br/>the journal that the study deals with the ratios between the peaks<br/>in the voice spectrum.  The study does not find peaks at particular<br/>frequencies presumably (that would be so hard to believe), but<br/>rather the spectral peaks within every sample are taken as a data<br/>set of ratios and energies, and these are accumulated to get a<br/>picture of total energies at various ratios.  A simple pattern of<br/>harmonic ratios, within the limits Francois has described, seems<br/>like an expected result of measuring many harmonic ratios.  How can<br/>the researchers leap from this apparently trivial result to a<br/>speculation that habituation to spectral emphasis patterns in speech<br/>[by the grace of God, harmonic] causes, or biases melodic scale<br/>preferences?  Tuners of instruments use harmonic relationships to<br/>make scales, and probably have since gut harps, skin drums and bone<br/>flutes.  With no more basis at this time than the researchers, but<br/>neither any less, I suggest that our flair for perceiving vowel<br/>formants out of sketchy but harmonically related sensory input,<br/>underlies our interest in melody and harmony.  We judge harmonic<br/>relationships with extraordinary physical accuracy, down to<br/>multisecond beats, and other artifacts.  We seem to have generalized<br/>harmonic handlers in our wetwear, of the same order of physical<br/>accuracy as those underlying our spatial interpretations.  It is<br/>counterintuitive to suppose we need to have this facility beaten<br/>into us, or that some nonharmonic arrangement could have evolved, or<br/>could still be substituted.</p><p>Besides, if the peaks drop off at the 7-limit, so that 7/6 is not<br/>statistically significant, and so not reinforced in experience, how<br/>come I hear that harmony no sweat?  My analysis of Bach is that he<br/>heard it and used it.  Others on this list can hear higher limits<br/>that that.</p><p>Perhaps you can elucidate the methods and implications of the<br/>results, bearing in mind both the knowledge and limitations of your<br/>audience of tuning gurus and enthusiasts.  I appreciate Francois<br/>taking the trouble to educate us in these matters, not that I<br/>couldn&apos;t use further instruction.  I apologize in advance for any<br/>misrepresentations.</p><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; OK, now I can have a cautious guess what your misunderstanding may<br/>be about.<br/>&gt; You seem to think that the study is about vowel formants. This,<br/>however, is<br/>&gt; not the case. The study deals with the ratios between the peaks in<br/>the voice<br/>&gt; spectrum, irrespective of any qualities that are related to<br/>vowels. Please<br/>&gt; have a look at that part of the paper where the sampling of the<br/>speech<br/>&gt; material is described.<br/>&gt;<br/>&gt;<br/>&gt; &gt; On the other hand side Fm is, by definition, a multiple of f0,<br/>thus,<br/>&gt; &gt; it is a roundoff of the real physical value (F1). Fm has no<br/>physical<br/>&gt; &gt; meaning (numerology excluded).<br/>&gt;<br/>&gt; This is wrong. The amplitudes of all peaks in the investigated<br/>sound spectra<br/>&gt; reflect the PHYSICAL resonance properties of the vocal tracts of<br/>the tested<br/>&gt; subjects.</p><p>Jeff says:  I hate to be picky, but this second note appears to<br/>contradict the first, in that the PHYSICAL resonance properties of<br/>the vocal tracts are most accurately described by the vowel formants.<br/>&gt;<br/>&gt; Martin</p></div><h3><a id=46676 href="#46676">ðŸ”—</a>Bill Sethares &#x3C;sethares@ece.wisc.edu&#x3E;</h3><span>9/2/2003 7:17:37 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Here is an email I wrote when this article first appeared - which explains how they<br/>process the data to arrive at their results -- and why the method is problematical.<br/>I believe that this is similar to what Dominique was saying earlier.</p><p>I&apos;ve just finished reading the paper that Graham and Gene and<br/>others have been discussing:</p><p>The Statistical Structure of Human Speech Sounds Predicts<br/>Musical Universals</p><p>by David A. Schwartz, Catherine Q. Howe, and Dale Purves</p><p>The graphs are striking, and seem to present a new and powerful<br/>argument for JI (even though they claim its a case for 12-tet!).<br/>Before drawing conclusions on the importance of this work, it&apos;s<br/>maybe worth a look at what they actually did - and what assumptions<br/>they made, perhaps without even realizing it.</p><p>Basically they measure the spectrum of many speakers in a variety of<br/>languages, process the data, and come up with a graph that looks a lot like<br/>a consonance curve such as Plomp and Levelt, or the one-footed bride from<br/>Partch, or Helmholtz&apos;s graph of comparative consonance - with nice consonant<br/>peaks at the simple integer ratios and dissonant valleys between.  What&apos;s<br/>new is that, unlike all the above, who consider the interaction between pairs<br/>of harmonic sounds, they arrive at this curve by a statistical analysis<br/>that processes only one (harmonic) sound at a time - the isolated voice.<br/>Hence their claim about universality and etc.</p><p>Lets look carefully at where this curve comes from - look at their processing<br/>of the data.  Here&apos;s what they do: for each spectrum (graph of frequency<br/>vs magnitude) normalize both the magnitude (by the largest magnitude) and<br/>normalize the frequency (by the frequency of the partial with the greatest<br/>magnitude).  The result in every case is a spectrum for which the frequency<br/>of the largest partial is &quot;1&quot; and the magnitude of the largest partial is<br/>&quot;1&quot; (obviously not physical units, but normalized units).  The voice has<br/>the property that usually, the largest partial is not the fundamental - for<br/>exaample, maybe the voice has partials at</p><p>150 300 450 600 750 900 Hz<br/>with amplitudes normalized to<br/> 0.1 0.2 1.0 0.4 0.3</p><p>Doing the frequency normalization then gives the processed spectrum as</p><p>freq = .33 .66 1  1.33 1.66 2.0<br/>mag  =  0.1 0.2 1.0 0.4 0.3</p><p>Another example: a voice might have partials at:</p><p>250 500 750 1000 Hz<br/>.3   1   .6   .4</p><p>Normalized this becomes:</p><p>freq = .5 1 1.5 2<br/>mag = .3   1   .6   .4</p><p>So - now you take and average a few thousand of these together.<br/>What do you get?  Something with a lot of energy at 1 (of course)<br/>and smaller amounts of energy at - you guessed it - small integer<br/>ratios like 1.33, 1.5, 1.66, and 2!</p><p>In other words, their curve is a result of the way that they have<br/>processed/normalized (the frequencies of) the data!</p><p>So, why they might have chosen to normalize the data in just this way?</p><p>Well, if you believe that the ear focuses attention on the loudest partial<br/>in a sound, then this is a natural thing to do.  Indeed, these people are<br/>proud of the fact that they developed this technique for visual processes.<br/>In vision, it is quite a good assumption that (say) the brightest spot is<br/>the most salient.  But in audition there is no reason to think that the ear<br/>pays much attention to the loudest partial in a cluster - indeed, virtual<br/>pitch theory tells us that the ear focuses instead on a harmonic template<br/>and not on the individual partials themselves.</p><p>It appears to me that this argument is fundamentally based on a fallacious<br/>assumption about auditory perception.</p><p>--Bill Sethares</p></div><h3><a id=46680 href="#46680">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/2/2003 8:30:39 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Jeff:</p><p>&gt; Martin, in every particular sample the energy peaks in the voice<br/>&gt; spectrum match those harmonics of the fundamental favored by the<br/>&gt; vowel formants.</p><p>If you sample an ideal, steady-state vowel from a textbook, yes. But in real<br/>speech very little is ideal, and almost nothing is steady-state. Further,<br/>the authors of the study did not even focus on vowels. Their samples of 0.1<br/>sec were randomly selected. They only wrote an algorithm to avoid silent<br/>samples.</p><p>So what they sampled was actually a little bit of harmony with plenty of<br/>noise. But it was exactly that type of sound that is with us during much<br/>time of our lives. The interesting result was WHICH of all the theoretically<br/>possible harmony is actually sticking out of this noise.</p><p>&gt; A simple pattern of harmonic ratios, within the limits Francois has<br/>described, seems<br/>&gt; like an expected result of measuring many harmonic ratios.</p><p>Yes, but nobody could have guessed beforehand WHICH harmonic ratios would be<br/>sticking out of the noise.</p><p>&gt; How can the researchers leap from this apparently trivial result to a<br/>&gt; speculation that habituation to spectral emphasis patterns in speech<br/>&gt; [by the grace of God, harmonic] causes, or biases melodic scale<br/>&gt; preferences?</p><p>An important part of the paper is the auditory side of it. Evolution adapted<br/>our ears to that what is around them. We knew before this study was made<br/>that our ears are very well adapted to the ratios 4:3, 5:4, and 6:5, but<br/>less well to the ratios 7:6 and 9:7. The new speech data give us a strong<br/>clue why things might have gone that way, and not another way that was<br/>biologically possible.</p><p>&gt; Besides, if the peaks drop off at the 7-limit, so that 7/6 is not<br/>&gt; statistically significant,</p><p>It is not only statistically insignificant in the sampled data, it is not<br/>even visible as being different from the noise.</p><p>&gt; and so not reinforced in experience, how come I hear that harmony no<br/>sweat?</p><p>If you sample fine vowels from fine male voices, I would expect that 7:6<br/>clearly pops out of the noise.</p><p>&gt; My analysis of Bach is that he heard it and used it. Others on this list<br/>can hear higher &gt;limits that that.</p><p>Of course, but it&apos;s getting more difficult and you might need some practice.<br/>Much in the world is probabilistic, but there are also kind of breakpoints.<br/>In hearing there may well be one between 6:5 and 7:6.</p><p>&gt; .....the PHYSICAL resonance properties of<br/>&gt; the vocal tracts are most accurately described by the vowel formants.</p><p>Just to be sure, vowel formants are neither certain frequencies nor certain<br/>harmonics. They are frequency bands that cover several adjacent harmonics.<br/>Also, these bands are not fixed. Not even in one speaker. They vary so much<br/>that for the speech analysis of the brain not the absolute frequency range<br/>can be used but only the relations between different formants, that is<br/>between different frequency ranges, of the same speaker at one point in<br/>time.</p><p>Martin</p></div><h3><a id=46681 href="#46681">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/2/2003 8:33:35 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Martin</p><p>When you write:</p><p>&gt; Martin:<br/>&gt; This is wrong. The amplitudes of all peaks in the investigated<br/>sound spectra<br/>&gt; reflect the PHYSICAL resonance properties of the vocal tracts of<br/>the tested<br/>&gt; subjects.</p><p>I suppose that you means the peaks that appears in the spectra of<br/>figure 1C around 210, 415, 625, 830 Hz etc. (rounded to the precision<br/>I got from the PDF file) and that are then normalised to contribute<br/>to the normalized spectum of figure 2. If you think that those peaks<br/>are (or reflect) the physical resonance of the vocal tract you are<br/>wrong, sorry. Those peaks are the harmonics of the glottal source.</p><p>Back to the basic: f0 and Fn are independents<br/>----------------------------------------------</p><p>Human voice is made of two fairly independent components, namely the<br/>glottal source (vocal folds) and the vocal tract (the flexible fleshy<br/>tube above the glottis).</p><p>As discussed before in this group (furthermore, with you as far as I<br/>remember) the frequency of the glottal source depends on the vocal<br/>folds mechanical properties (tickness, tension etc.) and from the<br/>subglottal pressure. This mechanism can be described as a phase<br/>locking mechanism. As I measured by myself, human voice is harmonic<br/>(i.e. not measurably inharmonic on 16 bit precision PCM recording).</p><p>The vocal tract play the role of a passive resonator, or bandpass<br/>filters that amplify some broad bands named formants. The lowers of<br/>those band, F1 correspond to a wavelength which is 4 time the<br/>effective length of the vocal tract.</p><p>f0 and F1 are fairly independent. It is possible to change the pitch<br/>without (significantly) moving the formants (glissando on a vowel)<br/>and vice versa (one pitch, many vowels). Anybody can try it.</p><p>In summary, the glottal source is responsible for the harmonic comb-<br/>like structure of the spectrum, while the vocal tract is responsible<br/>for the bumpy spectral enveloppe with the formant structure.</p><p>More advanced topics: f0 and formants can be related , but not much<br/>------------------------------------------------------------------</p><p>In fact, it is possible, but not easy to change the pitch without<br/>moving at all the formants, simply because the whole glottis tend to<br/>move a little up and down when the pitch goes up and down. This<br/>movement is very limited compared to the length of the vocal tract,<br/>let say a variation of some 5%, while the pitch can move by more than<br/>two octave: 400%. 5% vs 400+%, that is not much coupling.</p><p>Trained signers learn to control glottis movement. Further, trained<br/>singers, especially sopranos, are capable of shaping formants so that<br/>the largest harmonics fall very near the center of formant, in order<br/>to ensure the most efficient energy transfer (but often at the<br/>expense of vowel inteligibility). That is certainly not natural, this<br/>require education, training and practice.</p><p>J. Sunberg has explored in deep those fascinating topics.</p><p>Then back to the JNS paper<br/>--------------------------<br/>It should be clear, now, that Fm as defined in the JNE paper is not a<br/>physical resonance value. It is a value that is near F1 more or less<br/>1/2 f0.</p><p>If Fm was corresponding the vocal tract resonance (a physical<br/>meaningfull value) that would means (among other more or les silly<br/>things) that the effective vocal tract lenght is constraint to have<br/>value in the set<br/>           c<br/>   L = -----------    n= 1, 2, 3...<br/>        4 * n * f0</p><p>In the case of the figure 1C, for a pitch of 210 Hz the length of the<br/>vocal tract is limited to values:</p><p>         38 cm<br/>    L =  ---     =&gt; 38, 19, 12, 9 ....cm<br/>          n</p><p>As common sense indicate, the effect, the effective lenght of a human<br/>vocal tract is not limited to discrete set (like for valve trumpet):<br/>it is a continuous variable.</p><p>mmmmm................</p><p>Ok let say I am wrong in the proof by the absurd above.</p><p>Let say that F1 is F1 and let say that Fm is a genuine-new-not-until-<br/>now-discovered property of the vocal tract. As Fm is an acoustic<br/>frequency, NECESSARILLY, there is a physical length associated to Fm.<br/>But there is no such thing as a length that is limited to a discrete<br/>set of value in the vocal tract. A way to get a discrete set of<br/>lengths is by an artificial integer rounding off process as described<br/>in the JNE paper.</p><p>Conclusion: The way Fm is derived is computation artefact, void of<br/>any meaning (numerology still excluded)</p><p>The more and more I get at it, the more and more I am deeply conviced<br/>that all this spectrum normalisation procedure is deeply fallacious,<br/>to say the less.</p><p>But if you are not yet conviced.... I think that give up</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46686 href="#46686">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/2/2003 10:14:43 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Bill:</p><p>&gt; The voice has the property that usually, the largest partial is not the<br/>&gt; fundamental - for exaample, maybe the voice has partials at</p><p>&gt; 150 300 450 600 750 900 Hz<br/>&gt; with amplitudes normalized to<br/>&gt; 0.1 0.2 1.0 0.4 0.3</p><p>&gt; Doing the frequency normalization then gives the processed spectrum as</p><p>&gt; freq = .33 .66 1 1.33 1.66 2.0<br/>&gt; mag = 0.1 0.2 1.0 0.4 0.3</p><p>&gt; Another example: a voice might have partials at:</p><p>&gt; 250 500 750 1000 Hz<br/>&gt; .3 1 .6 .4</p><p>&gt; Normalized this becomes:</p><p>&gt; freq = .5 1 1.5 2<br/>&gt; mag = .3 1 .6 .4</p><p>Thank you. This will certainly help many readers of the list.</p><p>&gt; So - now you take and average a few thousand of these together.<br/>&gt; What do you get? Something with a lot of energy at 1 (of course)<br/>&gt; and smaller amounts of energy at - you guessed it - small integer<br/>&gt; ratios like 1.33, 1.5, 1.66, and 2!</p><p>&gt; In other words, their curve is a result of the way that they have<br/>&gt; processed/normalized (the frequencies of) the data!</p><p>This, however, is not true. The author did NOT normalize data from a theory<br/>text book, but real speech data that was randomly sampled. Your examples<br/>suggest that they should have found an averaged ratio spectrum with vertical<br/>lines at the low-order ratios. In fact they found one in which most of the<br/>power is BETWEEN these ratios. See my previous post on this issue.</p><p>&gt; So, why they might have chosen to normalize the data in just this way?</p><p>&gt; Well, if you believe that the ear focuses attention on the loudest partial<br/>&gt; in a sound, then this is a natural thing to do. Indeed, these people are<br/>&gt; proud of the fact that they developed this technique for visual processes.<br/>&gt; In vision, it is quite a good assumption that (say) the brightest spot is<br/>&gt; the most salient. But in audition there is no reason to think that the ear<br/>&gt; pays much attention to the loudest partial in a cluster - indeed, virtual<br/>&gt; pitch theory tells us that the ear focuses instead on a harmonic template<br/>&gt; and not on the individual partials themselves.</p><p>Well ?? !!   Well, this is surprising. You based your criticism on a guess<br/>of the possible motives of the authors, even though the authors explicitly<br/>explained their decision why they did precisely this normalization and<br/>nothing else. On p. 7161 they write:</p><p>&quot;This method of normalization avoids any assumptions about the structure of<br/>human speech sounds, e.g., that such sounds should be conceptualized in<br/>terms of ideal harmonic series.&quot;</p><p>If you think about this, I bet you will admit that this is the best possible<br/>choice they could make to tackle the &quot;problem&quot; that the order of amplitudes<br/>of the harmonics in the real spectra does not fit any norm.</p><p>[By the way, the comparison with vision, which the authors discussed, deals<br/>with something different. They suggested that the &quot;statistical&quot; (others say<br/>&quot;probabilistic&quot;) learning in hearing may be similar to that in vision. The<br/>harmonic bias (&quot;harmonic template&quot;) you mentioned has now been shown to be a<br/>possible result of statistical learning from speech sounds during evolution<br/>and development.<br/>This had been suggested earlier, also by Terhardt. But nobody could have<br/>known beforehand how this would look like in detail with REAL, randomly<br/>sampled speech sounds.]</p><p>&gt; It appears to me that this argument is fundamentally based on a fallacious<br/>&gt; assumption about auditory perception.</p><p>It had not appeared like that, had you read the methods section of the<br/>paper.</p><p>Martin</p></div><h3><a id=46688 href="#46688">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/2/2003 10:30:13 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; If you think that those peaks<br/>&gt; are (or reflect) the physical resonance of the vocal tract you are<br/>&gt; wrong, sorry. Those peaks are the harmonics of the glottal source.</p><p>&quot;Those peaks&quot; each have a frequency AND an amplitude. The frequencies in<br/>ideal, steady-state vowels reflect nothing, if they are harmonics of the<br/>glottal one.</p><p>But the amplitudes reflect the physical resonance of the vocal tract. That&apos;s<br/>what the paper&apos;s data are all about. Perhaps Bill&apos;s examples could now make<br/>this clear to some more readers.</p><p>Martin</p></div><h3><a id=46689 href="#46689">ðŸ”—</a>Bill Sethares &#x3C;sethares@ece.wisc.edu&#x3E;</h3><span>9/2/2003 11:58:46 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt; It had not appeared like that, had you read the methods section of the<br/>paper.</p><p>I read every word of the paper...</p><p>The authors said:</p><p>&quot;This method of normalization avoids any assumptions about the structure of<br/>human speech sounds, e.g., that such sounds should be conceptualized in<br/>terms of ideal harmonic series.&quot;</p><p>True. However, what they DID was to normalize the sounds by the<br/>frequency of the largest partial.<br/>What does such an normalization mean? It means that<br/>in some way, the largest partial must be a meaningful entity.<br/>I know of no psychoacoustic study that has ever indicated that<br/>&quot;the largest partial&quot; in a sound is a particularly salient feature<br/>of the sound, human voice or other.<br/>Hence my criticsm is of their method,<br/>not on my guess as to their motives.</p><p>While the authors avoid assumptions about the<br/>human voice, they are *implicitely* assuming something<br/>about the importance of a particular aspect of the sound<br/>(the largest partial) to the<br/>auditory system. What I find disturbing about the article is that<br/>the authors sweep their actual assumptions under the rug,<br/>while proclaiming that they were avoiding (other) assumptions.</p></div><h3><a id=46700 href="#46700">ðŸ”—</a>Gene Ward Smith &#x3C;gwsmith@svpal.org&#x3E;</h3><span>9/2/2003 3:51:35 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:</p><p>&gt; Much in the world is probabilistic, but there are also kind of<br/>breakpoints.<br/>&gt; In hearing there may well be one between 6:5 and 7:6.</p><p>I hear one between 7/6 and 8/7, but not between 6/5 and 7/6.</p></div><h3><a id=46709 href="#46709">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/2/2003 4:25:48 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&lt;francois.laferriere@o...&gt; wrote:<br/>&gt; Hello Martin<br/>&gt;<br/>&gt; When you write:<br/>&gt;<br/>&gt; &gt; Martin:<br/>&gt; &gt; This is wrong. The amplitudes of all peaks in the investigated<br/>&gt; sound spectra<br/>&gt; &gt; reflect the PHYSICAL resonance properties of the vocal tracts of<br/>&gt; the tested<br/>&gt; &gt; subjects.<br/>&gt;<br/>&gt; I suppose that you means the peaks that appears in the spectra of<br/>&gt; figure 1C around 210, 415, 625, 830 Hz etc. (rounded to the<br/>precision<br/>&gt; I got from the PDF file) and that are then normalised to contribute<br/>&gt; to the normalized spectum of figure 2. If you think that those<br/>peaks<br/>&gt; are (or reflect) the physical resonance of the vocal tract you are<br/>&gt; wrong, sorry.</p><p>forgive me, francois, but who cares about the physical resonance of<br/>the vocal tract when there&apos;s no sound in there resonating? what&apos;s<br/>relevant to this paper are the properties of the sounds actually<br/>produced by the vocal tract. you correctly pointed out that the<br/>result of only finding peaks at integer ratios was completely<br/>predictable and fairly trivial. you also pointed out that the<br/>resonance peaks of the vocal tract are completely continuous, rather<br/>than discrete as a function of the glottal frequency. but had the<br/>authors had access to (or estimated, say by curve-fitting) the latter<br/>quantities, how should they have incorporated them into their study?<br/>one only hears the frequency components that are actually present, as<br/>shaped by the resonances of the vocal tract (which is what the<br/>authors measured), not the resonant frequencies themselves. so the<br/>latter are relevant how? this is what mystifies me about your latest<br/>post. especially considering the length you went to to make this<br/>seemingly irrelevant point.</p></div><h3><a id=46720 href="#46720">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/3/2003 2:58:59 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Bill</p><p>I am quite happy to read from you that you reached the same<br/>conclusion as mine in a certainly much clearer phrasing.</p><p>&gt; Bill:<br/>&gt; Indeed, these people are<br/>&gt; proud of the fact that they developed this technique for visual<br/>processes.<br/>&gt; In vision, it is quite a good assumption that (say) the brightest<br/>spot is<br/>&gt; the most salient. But in audition there is no reason to think that<br/>the ear<br/>&gt; pays much attention to the loudest partial in a cluster - indeed,<br/>virtual<br/>&gt; pitch theory tells us that the ear focuses instead on a harmonic<br/>template<br/>&gt; and not on the individual partials themselves.</p><p>You pinpoint that those researcher come from vision analysis, and it<br/>is reasonable to think that this kind of data gathering procedure may<br/>work on spacial frequencies of pictures, because normal pictures (for<br/>instance natural landscape but not a checker) do not feature periodic<br/>pattern that would lead to two dimensional harmonic pattern in<br/>frequency space. So this is probably a too direct transposition of<br/>image methodology to sound that create this unvolontary bias in the<br/>data gathering.</p><p>&gt; Bill:<br/>&gt; It appears to me that this argument is fundamentally based on a<br/>fallacious<br/>&gt; assumption about auditory perception.</p><p>I just go a little farther than you, I think that the flaw is due to<br/>the probability distribution of Fm that cannot be higher than 7 for<br/>quite dumb physical reason.</p><p>&gt; Jeff:<br/>&gt; A simple pattern of harmonic ratios, within the limits Francois has<br/>described, seems<br/>&gt; like an expected result of measuring many harmonic ratios.</p><p>Correct, the physical limits and probability distribution of f0 and<br/>F1 is bound to lead to a selection of simple ratios.</p><p>&gt; Martin:<br/>&gt; Yes, but nobody could have guessed beforehand WHICH harmonic ratios<br/>would<br/>&gt; be sticking out of the noise.</p><p>The exact amplitude could no be guessed, but I explained clearly how<br/>only a very limited number of ratio are likely, some other are<br/>possible but less likely, and much of them are clearly forbiden. I<br/>also explained clearly the discrpencies between men and women<br/>results. So, all in all, the results remains quite trivial.</p><p>&gt; Martin:<br/>&gt; Just to be sure, vowel formants are neither certain frequencies nor<br/>&gt; certain harmonics.</p><p>Good idea to ask.</p><p>vowel formants can be modeled from a physical model of the vocal<br/>tract, then computed from actual speech signal. A formant is defined<br/>by a (certain, precise) central frequency and a bandwith. Any basic<br/>textbook about speech processing describe that. The central frequency<br/>is unrelated to pitch, so it cannot be related to certain harmonics<br/>of the voice signal.</p><p>&gt; Martin:<br/>&gt; They are frequency bands that cover several adjacent harmonics.<br/>&gt; Also, these bands are not fixed. Not even in one speaker.</p><p>Correct, these bands move over time, but can be computed (central<br/>frequency and bandwith) with some accuracy at any given time.</p><p>&gt; Martin:<br/>&gt; They vary so much<br/>&gt; that for the speech analysis of the brain not the absolute<br/>frequency range<br/>&gt; can be used but only the relations between different formants, that<br/>is<br/>&gt; between different frequency ranges, of the same speaker at one<br/>point in</p><p>Correct, they vary much, F1 is &quot;typically&quot; between 500 and 750 for<br/>adults. It is admitted that F1 and F2 are sufficient for wowel<br/>recognition. I do not see your point.</p><p>&gt; Paul:<br/>&gt; forgive me, francois, but who cares about the physical resonance of<br/>&gt; the vocal tract when there&apos;s no sound in there resonating? what&apos;s<br/>&gt; relevant to this paper are the properties of the sounds actually<br/>&gt; produced by the vocal tract. you correctly pointed out that the<br/>&gt; result of only finding peaks at integer ratios was completely<br/>&gt; predictable and fairly trivial. you also pointed out that the<br/>&gt; resonance peaks of the vocal tract are completely continuous,<br/>rather<br/>&gt; than discrete as a function of the glottal frequency. but had the<br/>&gt; authors had access to (or estimated, say by curve-fitting) the<br/>latter<br/>&gt; quantities, how should they have incorporated them into their<br/>study?<br/>&gt; one only hears the frequency components that are actually present,<br/>as<br/>&gt; shaped by the resonances of the vocal tract (which is what the<br/>&gt; authors measured), not the resonant frequencies themselves. so the<br/>&gt; latter are relevant how? this is what mystifies me about your<br/>latest<br/>&gt; post. especially considering the length you went to to make this<br/>&gt; seemingly irrelevant point.</p><p>Thanks Paul for making me realise that as I tried to accumulate<br/>demonstrations of what I, in my mind, is an obvious flaw, I became<br/>more and more terse and obscure (while I hoped to progress the other<br/>way). So I should have gave up before :-).</p><p>I used the technique of &quot;dimensional analysis&quot; to try to<br/>show &quot;clearly&quot; that Fm is not a physical quantity but just a<br/>computation artefact, but failed miserably at being any much clearer.</p><p>The fact remain that Fm that is the foundation of the paper is<br/>computed and used in such a way that it generates artifical peaks in<br/>the &quot;normalized&quot; spectrum.</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46721 href="#46721">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/3/2003 3:15:53 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Gene:</p><p>&gt;&gt; Much in the world is probabilistic, but there are also kind of<br/>&gt;&gt; breakpoints. In hearing there may well be one between 6:5 and 7:6.</p><p>&gt; I hear one between 7/6 and 8/7, but not between 6/5 and 7/6.</p><p>Also this is plausible from what we know about hearing. Psychoacoustic<br/>experiments showed that for pure tones the borderline between consonance and<br/>dissonance lies around 260 Cent for frequencies in the music-relevant range.<br/>So, some may hear 267 as consonant, while others do less so.</p><p>There is another component that may also vary considerably between<br/>listeners. 7:6 deviates more from the ubiquitous 12-tone scales than 6:5<br/>does. (Probably you discussed such things at length in earlier years.)</p><p>Martin</p></div><h3><a id=46722 href="#46722">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/3/2003 2:58:29 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Bill:</p><p>&gt; True. However, what they DID was to normalize the sounds by the<br/>&gt; frequency of the largest partial.<br/>&gt; What does such an normalization mean? It means that<br/>&gt; in some way, the largest partial must be a meaningful entity.<br/>&gt; I know of no psychoacoustic study that has ever indicated that<br/>&gt; &quot;the largest partial&quot; in a sound is a particularly salient feature<br/>&gt; of the sound, human voice or other.<br/>&gt; Hence my criticsm is of their method,<br/>&gt; not on my guess as to their motives.</p><p>Again, I bet that all who think over this methods decision of the authors<br/>will come to the conclusion that they chose the best possible option.</p><p>When sampling data you have to make decisions. And the best decisions are<br/>those which are based on the smallest assumptions.</p><p>So, what could they do to collect data on the probability of frequency<br/>ratios between the peaks in randomly selected speech samples?</p><p>1) They could have taken the 10 peaks with the highest amplitudes. But why<br/>10? Why not 5 or 20? You would need masses of theory to &quot;justify&quot; such a<br/>decision.</p><p>2) They could have taken all peaks in a frequency range, say 100-2000 Hz.<br/>But why not 50-4000 Hz? You would need masses of theory to &quot;justify&quot; such a<br/>decision. If the range is too large, you also lose most possible information<br/>in noise.</p><p>3) They could have taken the peak at f0, instead of the highest peak, as a<br/>&quot;normalized center peak&quot;. But how to calculate f0? This has been a hot<br/>issue in speech research for decades. There are competing algorithms, and<br/>the discussion is still long from being settled.</p><p>I see no better way than that taken by the authors. If somebody else sees<br/>one, please let us know.</p><p>&gt; While the authors avoid assumptions about the<br/>&gt; human voice, they are *implicitely* assuming something<br/>&gt; about the importance of a particular aspect of the sound<br/>&gt; (the largest partial) to the auditory system.</p><p>This is a wrong conclusion. As shown above, their decision was the one that<br/>assumed least, compared to all other possible decisions. The extremely<br/>trivial assumption they may have made is that the highest peak and its<br/>neighbors (see Fig. 1C) have a good chance to enter the auditory system.<br/>This assumption is so self-evident that it cannot be a surprise that they<br/>did not mention it explicitly.</p><p>&gt; What I find disturbing about the article is that<br/>&gt; the authors sweep their actual assumptions under the rug,<br/>&gt; while proclaiming that they were avoiding (other) assumptions.</p><p>I hope you can now see what they &quot;swept under the rug&quot;. Perhaps you can now<br/>also see that something else may be considered as &quot;disturbing&quot;.</p><p>Martin</p></div><h3><a id=46727 href="#46727">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/3/2003 12:12:12 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; The fact remain that Fm that is the foundation of the paper is<br/>&gt; computed and used in such a way that it generates artifical peaks in<br/>&gt; the &quot;normalized&quot; spectrum.</p><p>1) Fm, the frequency of the maximum peak in the spectra of the sound samples<br/>is NOT the foundation of the paper. It is the frequency to which all other<br/>frequencies and amplitudes in a given sample are related to receive a &quot;power<br/>spectrum&quot; of ratios.</p><p>2) Fm is NOT computed in any way. It is simply read out from the FFT.</p><p>3) Artificial peaks are NOT generated. The peaks that appear in the grand<br/>average spectra appear due to an adequate methods decision of the authors.<br/>See my previous message.</p><p>Fran&iuml;&iquest;&half;ois, had you spent a small part of the time that went into writing your<br/>long messages in this thread on simply reading the methods section of the<br/>study, you would have done yourself and all other list members a great<br/>favor. Perhaps next time ;-)</p><p>Martin</p></div><h3><a id=46738 href="#46738">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/3/2003 2:02:39 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Fran&ccedil;ois:<br/>&gt;<br/>&gt; &gt; The fact remain that Fm that is the foundation of the paper is<br/>&gt; &gt; computed and used in such a way that it generates artifical peaks in<br/>&gt; &gt; the &quot;normalized&quot; spectrum.<br/>&gt;<br/>&gt; 1) Fm, the frequency of the maximum peak in the spectra of the sound samp=<br/>les<br/>&gt; is NOT the foundation of the paper. It is the frequency to which all othe=<br/>r<br/>&gt; frequencies and amplitudes in a given sample are related to receive a &quot;po=<br/>wer<br/>&gt; spectrum&quot; of ratios.<br/>&gt;<br/>&gt; 2) Fm is NOT computed in any way. It is simply read out from the FFT.<br/>&gt;<br/>&gt; 3) Artificial peaks are NOT generated. The peaks that appear in the grand=</p><p>&gt; average spectra appear due to an adequate methods decision of the authors=<br/>.<br/>&gt; See my previous message.</p><p>Fm is not computet, my mistake in this specific sentence.</p><p>&gt; Fran&ccedil;ois, had you spent a small part of the time that went into writing y=<br/>our<br/>&gt; long messages in this thread on simply reading the methods section of the=</p><p>&gt; study, you would have done yourself and all other list members a great<br/>&gt; favor. Perhaps next time ;-)<br/>&gt;<br/>&gt; Martin</p><p>Martin, you didn&apos;t read or understood a word of my very first post on<br/>this topic (46560) which was clear enough for anybody with a minimal<br/>math/science culture. You were unable to produce a single sensible<br/>counter-argument on my demonstration.</p><p>The conclusion of the paper may be correct or interrsting or worth<br/>discussion, at this point, I really do not care. Conclusion is just<br/>based on false premices and that is bothering me.</p><p>What is starting to bother me as well is your arrogant attitude, not<br/>especially toward me, but toward anybody who disagree with you.</p><p>Have you ever admited you were wrong, once in your life?</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46740 href="#46740">ðŸ”—</a>Bill Sethares &#x3C;sethares@ece.wisc.edu&#x3E;</h3><span>9/3/2003 8:51:22 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Martin and Francois,</p><p>Despite appearences, it does seem that we have reached<br/>agreement about the crucial nature of the normalization-by-largest<br/>partial method used in the paper.</p><p>Where we disagree is in whether this normalization is sensible.</p><p>My view (and Francois, as well, assuming I read you right)<br/>is that this procedure is unjustified -<br/>while Martin feels that it is a sensible.</p><p>My reasoning is as follows:<br/>the normalization places a very important role<br/>on the perception of the largest partial in a sound -<br/>indeed, the largest partial plays a pivotal role.<br/>There are many things about sounds that are commonly considered<br/>perceptually relevant - amplitude/pitch/virtual pitch/modulations/<br/>etc, but a perception of &quot;the largest partial&quot; is not one of them.<br/>Hence, a computational method in which the largest partial plays<br/>a pivotal role needs to be looked at carefully.</p><p>Martin queried: I see no better way than that<br/>taken by the authors. If somebody else sees<br/>one, please let us know.</p><p>There are many possible paths the authors could have taken.<br/>They could have normalized by the spectral center<br/>(the center of mass of the power spectrum).<br/>They could have normalized by the expectation of the<br/>power spectrum.<br/>They could have normalized by the spectral center of the<br/>log of the power spectrum (since pitch is generally<br/>perceived in a log (dB) scale).<br/>I see no a priori reason to prefer any one of these methods over<br/>any other one - is the largest partial more perceptually relevant<br/>than the center of the spectrum? Than the expectation?</p><p>This is why I do not find the argument &quot;what else could<br/>they do?&quot; convincing, because they had many choices.<br/>Moreover, they make no argument in favor of the choice they made.</p><p>In any case, I hope it is clearer now why I find the normalization-by-<br/>largest partial an unconvincing method.</p><p>--Bill Sethares</p></div><h3><a id=46747 href="#46747">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/4/2003 6:36:45 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; Martin, you didn&apos;t read or understood a word of my very first post on<br/>&gt; this topic (46560) which was clear enough for anybody with a minimal<br/>&gt; math/science culture. You were unable to produce a single sensible<br/>&gt; counter-argument on my demonstration.</p><p>This was hard to read. If you still want a public criticism, here it is:</p><p>Message 46560:</p><p>&gt; Let see how it work.<br/>&gt; The normalized value is defined (in the paper) as<br/>&gt; Fn = F/Fm<br/>&gt; As the signal is harmonic F and Fm are restricted to values in the<br/>&gt; serie f0, 2f0, 3f0 ...</p><p>1) Only some of the selected samples are harmonic (see my previous post).</p><p>2) Even when a sample was harmonic, ONLY &quot;Fm&quot; was a multiple f0. &quot;F&quot; was<br/>taken from ALL frequencies of the FFT spectrum, not just from the multiples<br/>of f0. So all relevant ratios &quot;had a chance&quot; (in your words), even 11/9 or<br/>17/16.</p><p>[By the way, all this is written down in the methods section of the paper.]</p><p>Martin</p></div><h3><a id=46748 href="#46748">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/4/2003 6:15:47 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Bill:</p><p>&gt; Despite appearences, it does seem that we have reached<br/>&gt; agreement about the crucial nature of the normalization-by-largest<br/>&gt; partial method used in the paper.</p><p>The normalization was to the largest &quot;peak&quot; in the spectra of the samples,<br/>not to the largest &quot;partial&quot;. As I said before, the authors did NOT filter<br/>their speech material for quasi-steady-state vowels. As they selected the<br/>samples randomly, the amount of periodicity in the samples varied greatly.<br/>Not all spectra had partials, but all had a highest peak.</p><p>&gt; My reasoning is as follows:<br/>&gt; the normalization places a very important role<br/>&gt; on the perception of the largest partial in a sound -<br/>&gt; indeed, the largest partial plays a pivotal role.<br/>&gt; There are many things about sounds that are commonly considered<br/>&gt; perceptually relevant - amplitude/pitch/virtual pitch/modulations/<br/>&gt; etc, but a perception of &quot;the largest partial&quot; is not one of them.<br/>&gt; Hence, a computational method in which the largest partial plays<br/>&gt; a pivotal role needs to be looked at carefully.</p><p>OK. Looking at things carefully is good. The question here is, could this<br/>decision have done any damage to the study? Could it have biased the results<br/>in an inappropriate way?</p><p>So, let&apos;s assume the sampling algorithm has hit a fairly clean vowel and the<br/>highest peak in the FFT spectrum is the highest partial. I fully agree that<br/>the highest partial per se has no particular importance in hearing. In the<br/>frequency range of speech and music the partials that are most weighed by<br/>the brain for pitch extraction are the partials 3 to 6. Partials 1 and 2 are<br/>also weighed, in particular if they have a high amplitude. Partials 7 and<br/>higher have less or no importance, because their frequencies are poorly, or<br/>not at all, resolved in ear and brain (neighboring partials that are too<br/>close to each other mask each other). Now, in vowels the highest partial<br/>usually is among numbers 1-6. So it is among those that are weighed for<br/>pitch extraction, and it is weighed stronger than its neighbors, because it<br/>has a higher amplitude.</p><p>In conclusion the decision of the authors agrees well with what the ear does<br/>with the highest partials in vowel sounds.</p><p>&gt; Martin queried: I see no better way than that<br/>&gt; taken by the authors. If somebody else sees<br/>&gt; one, please let us know.</p><p>&gt; There are many possible paths the authors could have taken.<br/>&gt; They could have normalized by the spectral center<br/>&gt; (the center of mass of the power spectrum).</p><p>This would have been meaningless considering the research aims. Aim of the<br/>study was to see how the distribution of ratios was between outstanding<br/>spectral lines. For a ratio you need two points in the spectrum, and at<br/>least one of them has to be a peak. The &quot;center of mass of the power<br/>spectrum&quot; could fall anywhere, even in a valley of the spectrum. Therefore<br/>it cannot be used for the purpose of the study.</p><p>&gt; They could have normalized by the expectation of the power spectrum.</p><p>Same as above, plus problem to justify the &quot;expectation&quot;.</p><p>&gt; They could have normalized by the spectral center of the<br/>&gt; log of the power spectrum (since pitch is generally<br/>&gt; perceived in a log (dB) scale).</p><p>Same as first suggestion.</p><p>&gt; In any case, I hope it is clearer now why I find the normalization-by-<br/>&gt; largest partial an unconvincing method.</p><p>I hope you can now reconsider this view.</p><p>Martin</p></div><h3><a id=46750 href="#46750">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/4/2003 8:20:38 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello</p><p>&gt;<br/>&gt; Message 46560:<br/>&gt;<br/>&gt; &gt; Let see how it work.<br/>&gt; &gt; The normalized value is defined (in the paper) as<br/>&gt; &gt; Fn = F/Fm<br/>&gt; &gt; As the signal is harmonic F and Fm are restricted to values in the<br/>&gt; &gt; serie f0, 2f0, 3f0 ...<br/>&gt;<br/>&gt; 1) Only some of the selected samples are harmonic (see my previous<br/>post).<br/>&gt;<br/>&gt; 2) Even when a sample was harmonic, ONLY &quot;Fm&quot; was a multiple f0. &quot;F&quot; was<br/>&gt; taken from ALL frequencies of the FFT spectrum, not just from the<br/>multiples<br/>&gt; of f0. So all relevant ratios &quot;had a chance&quot; (in your words), even<br/>11/9 or<br/>&gt; 17/16.</p><p>We need to go back to articulatory phonetics again.</p><p>A phoneme can be voiced (with harmonic spectrum due to vocal fold<br/>vibration) or unvoiced. When voiced, the harmonic spectrum carry the<br/>bulk of energy and then Fm is a multiple of f0. This is true for all<br/>vowels and most consonants. What does that left? Two category of<br/>consonant, the unvoiced plosive (P, T, K) and the unvoiced fricative<br/>(in english F like in Fourier, S like in spectrum, CH like in sheep).<br/>Unvoiced plosive are basically... silence, so are rightfully discarded<br/>by the protocol.</p><p>That left us with unvoiced fricatives, which are basically, broadband<br/>  white noise that shall contribute to noise in the averaged spectrum.<br/>Further, what is the probability weight of F,S,CH against the rest of<br/>the phonological system (all the other vowels and consonants)? not<br/>much, certainly.</p><p>The contribution of unharmonic segments is altogether irrelevant (it<br/>is noise!) and insignificant (there are very few of them).</p><p>Except if the database contains a huge amount (more than 50%) of<br/>whispered voice (which is very unlikely for what I know of speech<br/>database) the data is clearly dominated by voiced, harmonic segment,<br/>so my demonstration is still correct, until otherwhise debunked</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46755 href="#46755">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/4/2003 12:13:46 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; ....the data is clearly dominated by voiced, harmonic segment,<br/>&gt; so my demonstration is still correct, until otherwhise debunked</p><p>Then I would like to invite you to debunk it yourself. If your statement was<br/>correct, we would see spectral lines, or at least very steep peaks, in Fig.<br/>1C. But we don&apos;t. The power distribution is mainly flat with some small<br/>peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot; is OUTSIDE<br/>the low-order ratios. This is due to the inharmonicity of the speech<br/>samples, and nothing else. You can also see that the ratio 11/9 is well<br/>represented in the distribution. The same applies to 11.5/9.5. These ratios<br/>have only slightly less power than 5/4 and 6/5. [But they are irrelevant, of<br/>course, because they do not stick out of the inharmonicity noise.]</p><p>Martin</p></div><h3><a id=46759 href="#46759">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/4/2003 12:46:09 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Fran&ccedil;ois:<br/>&gt;<br/>&gt; &gt; ....the data is clearly dominated by voiced, harmonic segment,<br/>&gt; &gt; so my demonstration is still correct, until otherwhise debunked<br/>&gt;<br/>&gt; Then I would like to invite you to debunk it yourself. If your<br/>statement was<br/>&gt; correct, we would see spectral lines, or at least very steep peaks,<br/>in Fig.<br/>&gt; 1C. But we don&apos;t. The power distribution is mainly flat with some<br/>small<br/>&gt; peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot; is<br/>OUTSIDE<br/>&gt; the low-order ratios. This is due to the inharmonicity of the speech<br/>&gt; samples, and nothing else.</p><p>is inharmonicity really the right word, or should it be<br/>merely &quot;noise&quot;? to me, inharmonicity implies a discrete spectrum that<br/>deviates from the harmonic pattern, and i challenge anyone to show me<br/>evidence of a human voice producing *that*.</p><p>&gt; inharmonicity noise.]</p><p>now i&apos;m extra confused -- i thought i knew what &quot;inharmonicity&quot;<br/>and &quot;noise&quot; were, but what is &quot;inharmonicity noise&quot;??</p></div><h3><a id=46760 href="#46760">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/4/2003 1:04:36 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Jeff Olliff&quot; &lt;jolliff@d...&gt; wrote:</p><p>&gt; Besides, if the peaks drop off at the 7-limit, so that 7/6 is not<br/>&gt; statistically significant,</p><p>but 7/4 and 7/5 are.</p><p>&gt; and so not reinforced in experience, how<br/>&gt; come I hear that harmony no sweat?  My analysis of Bach is that he<br/>&gt; heard it and used it.</p><p>well, i&apos;ve spent enough words on this list disputing that, i&apos;ll hold<br/>off on going back into that unless there&apos;s a strong desire on<br/>someone&apos;s part . . . for now, interested parties can listen to john<br/>delaubenfels&apos; adaptively tuned versions of the bach chaconne as<br/>arranged by busoni, with versions which target 5-limit vertical<br/>sonorities as well as versions which target 7-limit vertical<br/>sonorities (for dominant seventh chords and the like):</p><p><a href="http://bellsouthpwp.net/j/d/jdelaub/jstudio.htm">http://bellsouthpwp.net/j/d/jdelaub/jstudio.htm</a></p><p>download <a href="http://b-b-bj.zip">b-b-bj.zip</a></p></div><h3><a id=46770 href="#46770">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/5/2003 1:58:39 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I must apologize for the typo in my previous message. Instead of &quot;Fig. 1C&quot;,<br/>it must be &quot;Fig. 2C&quot;, of course. Sorry.</p><p>Martin</p></div><h3><a id=46771 href="#46771">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/5/2003 2:58:01 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:</p><p>&gt;&gt; The power distribution is mainly flat with some small<br/>&gt;&gt; peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot; is<br/>&gt;&gt; OUTSIDE the low-order ratios. This is due to the inharmonicity of the<br/>&gt;&gt; speech samples, and nothing else.</p><p>&gt; is inharmonicity really the right word, or should it be merely &quot;noise&quot;?</p><p>Then you would have to call the vast majority of speech signals &quot;noise&quot;.<br/>This would be against all conventions. You might be right perhaps when<br/>considering the contents of much speech. But that would be another question<br/>;-)</p><p>&quot;Inharmonicity&quot; would be the right word, when dealing with the difference<br/>between harmonic (low-order) ratios and other (non-harmonic) ratios in the<br/>distribution of ratios in speech spectra. And this is what the study was<br/>about.</p><p>&gt; to me, inharmonicity implies a discrete spectrum that<br/>&gt; deviates from the harmonic pattern, and i challenge anyone to show me<br/>&gt; evidence of a human voice producing *that*.</p><p>You mean discrete spectral lines with non-harmonic frequency ratios? These,<br/>of course, do not exist. But non-harmonic ratios in the spectrum represent<br/>the greater part of spectral power in human speech, as seen in Fig. 2C of<br/>the study.</p><p>&gt;&gt; inharmonicity noise.]</p><p>&gt; now i&apos;m extra confused -- i thought i knew what &quot;inharmonicity&quot;<br/>&gt; and &quot;noise&quot; were, but what is &quot;inharmonicity noise&quot;??</p><p>&quot;Noise&quot; in science means the opposite of signal or information. In the study<br/>the information at issue was harmonic (low-order) ratios. So other ratios<br/>were &quot;noise&quot; compared to this &quot;signal&quot; background. Because these other<br/>ratios are inharmonic (high-order) ratios, the results, as in Fig. 2C, were<br/>&quot;harmonicity information&quot; plus &quot;inharmonicity noise&quot;.</p><p>Martin</p></div><h3><a id=46772 href="#46772">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/5/2003 4:08:54 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Martin and Paul<br/>&gt; Martin:</p><p>&gt; &gt;&gt; The power distribution is mainly flat with some small<br/>&gt; &gt;&gt; peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot; is<br/>&gt; &gt;&gt; OUTSIDE the low-order ratios. This is due to the inharmonicity of the<br/>&gt; &gt;&gt; speech samples, and nothing else.</p><p>Paul answered:<br/>&gt; is inharmonicity really the right word, or should it be merely &quot;noise&quot;?</p><p>The flat background is due to<br/>  - harmonic peaks spreading due to FFT window width (0.1 sec is quite<br/>short in this respect)<br/>  - pitch movement within each sample that blurr the peaks (0.1 is<br/>quite long then....)<br/>  - possible contribution of turbulent noise (frication noise)</p><p>All this introduces &quot;noise&quot; (information theory acceptance) in the<br/>results, I understand Paul answer this way. This noise masks the basic<br/>discrete nature of the spectrum by filling the &quot;forbiden band&quot; with noise.</p><p>inharmonicity is definitly not the right word.</p><p>&gt; Then you would have to call the vast majority of speech signals &quot;noise&quot;.<br/>&gt; This would be against all conventions. You might be right perhaps when<br/>&gt; considering the contents of much speech. But that would be another<br/>question<br/>&gt; ;-)</p><p>&gt; Martin:<br/>&gt; &quot;Inharmonicity&quot; would be the right word, when dealing with the<br/>difference<br/>&gt; between harmonic (low-order) ratios and other (non-harmonic) ratios<br/>in the<br/>&gt; distribution of ratios in speech spectra. And this is what the study was<br/>&gt; about.<br/>&gt;<br/>&gt;<br/>&gt; &gt; to me, inharmonicity implies a discrete spectrum that<br/>&gt; &gt; deviates from the harmonic pattern, and i challenge anyone to show me<br/>&gt; &gt; evidence of a human voice producing *that*.<br/>&gt;<br/>&gt; You mean discrete spectral lines with non-harmonic frequency ratios?<br/>These,<br/>&gt; of course, do not exist. But non-harmonic ratios in the spectrum<br/>represent<br/>&gt; the greater part of spectral power in human speech, as seen in Fig.<br/>2C of<br/>&gt; the study.<br/>&gt;<br/>&gt;<br/>&gt; &gt;&gt; inharmonicity noise.]<br/>&gt;<br/>&gt; &gt; now i&apos;m extra confused -- i thought i knew what &quot;inharmonicity&quot;<br/>&gt; &gt; and &quot;noise&quot; were, but what is &quot;inharmonicity noise&quot;??<br/>&gt;<br/>&gt; &quot;Noise&quot; in science means the opposite of signal or information. In<br/>the study<br/>&gt; the information at issue was harmonic (low-order) ratios. So other<br/>ratios<br/>&gt; were &quot;noise&quot; compared to this &quot;signal&quot; background. Because these other<br/>&gt; ratios are inharmonic (high-order) ratios, the results, as in Fig.<br/>2C, were<br/>&gt; &quot;harmonicity information&quot; plus &quot;inharmonicity noise&quot;.<br/>&gt;</p><p>You seems to come to an agreement on that: there is no such thing as<br/>anharmonic speech signal. So the background is due to measurement<br/>dispersion or &quot;noise&quot; but in the information theory acceptanceof thge<br/>word &quot;noise&quot;, not the signal theory acceptance.</p><p>I hope that this clarify the point</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46782 href="#46782">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/5/2003 3:58:03 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Paul:<br/>&gt;<br/>&gt; &gt;&gt; The power distribution is mainly flat with some small<br/>&gt; &gt;&gt; peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot;<br/>is<br/>&gt; &gt;&gt; OUTSIDE the low-order ratios. This is due to the inharmonicity<br/>of the<br/>&gt; &gt;&gt; speech samples, and nothing else.<br/>&gt;<br/>&gt; &gt; is inharmonicity really the right word, or should it be<br/>merely &quot;noise&quot;?<br/>&gt;<br/>&gt; Then you would have to call the vast majority of speech<br/>&gt;signals &quot;noise&quot;.</p><p>not necessarily. a finite fft will show a continuous spectrum, but<br/>part of that is merely the classical uncertainty principle at work.<br/>fourier transforms are only valid up to a certain level of<br/>uncertainty, if you allow for full generality in the sample in terms<br/>of both frequency and time.</p><p>&gt; This would be against all conventions. You might be right perhaps<br/>when<br/>&gt; considering the contents of much speech. But that would be another<br/>question<br/>&gt; ;-)</p><p>well, what i had in mind was that most speech sounds have a noise<br/>(continuous) component that one could in theory subtract out, and<br/>then it is the discrete spectrum that remains which could be<br/>described as either harmonic or inharmonic. my claim is that<br/>inharmonic it would never be, though things would get hairy when the<br/>vocal dynamics become chaotic (basically vocal multiphonics).</p><p>&gt; &quot;Inharmonicity&quot; would be the right word, when dealing with the<br/>difference<br/>&gt; between harmonic (low-order) ratios and other (non-harmonic) ratios<br/>in the<br/>&gt; distribution of ratios in speech spectra. And this is what the<br/>study was<br/>&gt; about.</p><p>hmm . . .</p><p>&gt; &gt; to me, inharmonicity implies a discrete spectrum that<br/>&gt; &gt; deviates from the harmonic pattern, and i challenge anyone to<br/>show me<br/>&gt; &gt; evidence of a human voice producing *that*.<br/>&gt;<br/>&gt; You mean discrete spectral lines with non-harmonic frequency<br/>ratios? These,<br/>&gt; of course, do not exist.</p><p>well, if you synthesize a bell sound, and make it sustain<br/>indefinitely, the longer the sample you fft, the more closely the<br/>result will approach this condition.</p><p>&gt; But non-harmonic ratios in the spectrum represent<br/>&gt; the greater part of spectral power in human speech, as seen in Fig.<br/>2C of<br/>&gt; the study.</p><p>in the form of noise, correct?</p><p>&gt; &gt;&gt; inharmonicity noise.]<br/>&gt;<br/>&gt; &gt; now i&apos;m extra confused -- i thought i knew what &quot;inharmonicity&quot;<br/>&gt; &gt; and &quot;noise&quot; were, but what is &quot;inharmonicity noise&quot;??<br/>&gt;<br/>&gt; &quot;Noise&quot; in science means the opposite of signal or information. In<br/>the study<br/>&gt; the information at issue was harmonic (low-order) ratios. So other<br/>ratios<br/>&gt; were &quot;noise&quot; compared to this &quot;signal&quot; background. Because these<br/>other<br/>&gt; ratios are inharmonic (high-order) ratios, the results, as in Fig.<br/>2C, were<br/>&gt; &quot;harmonicity information&quot; plus &quot;inharmonicity noise&quot;.<br/>&gt;<br/>&gt; Martin</p><p>so &quot;inharmonicity&quot; means nothing more than to distinguish the noise<br/>to which it applies from the harmonic-spectum component, yes?</p></div><h3><a id=46795 href="#46795">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/7/2003 6:00:02 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:</p><p>&gt;&gt; &quot;Noise&quot; in science means the opposite of signal or information. In<br/>&gt;&gt; the study the information at issue was harmonic (low-order) ratios. So<br/>&gt;&gt; other ratios were &quot;noise&quot; compared to this &quot;signal&quot; background. Because<br/>these<br/>&gt;&gt; other ratios are inharmonic (high-order) ratios, the results, as in<br/>Fig.2C,<br/>&gt;&gt; were &quot;harmonicity information&quot; plus &quot;inharmonicity noise&quot;.</p><p>&gt; so &quot;inharmonicity&quot; means nothing more than to distinguish the noise<br/>&gt; to which it applies from the harmonic-spectum component, yes?</p><p>Yes, in this particular case. I also agree that in the vocal tract, as<br/>opposed to bells, there are no inharmonic vibration modes. The difference<br/>between us may be that we have different views on the transient status of<br/>all speech sounds. Well over 90 % of all sounds of phoneme length are<br/>glissandi or portamenti. In such transient sounds the harmonic spectrum is<br/>stretched or compressed, that is, &quot;disharmonized&quot;. The reason is that the<br/>various vibration modes have different latencies. This in turn is due to the<br/>different masses that vibrate. Usually, smaller masses (faster vibrations,<br/>higher frequencies) have shorter latencies.</p><p>So, the reason why 7:5 sticks out in the data, but 7:6 doesn&apos;t, may be that<br/>the &quot;signal&quot; at 7:6 is so small that it is blurred out by the ubiquitous<br/>transient disharmonization. For me at least this would be an interesting<br/>finding. We knew about these blurring effects, but we did not know their<br/>extent in the statistics of a large speech corpus.</p><p>Martin</p></div><h3><a id=46796 href="#46796">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/7/2003 6:27:26 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; &gt;&gt; The power distribution is mainly flat with some small<br/>&gt; &gt;&gt; peaks ontop of it. Most of the power in this ratio &quot;spectrogram&quot; is<br/>&gt; &gt;&gt; OUTSIDE the low-order ratios. This is due to the inharmonicity of the<br/>&gt; &gt;&gt; speech samples, and nothing else.</p><p>&gt; The flat background is due to<br/>&gt; - harmonic peaks spreading due to FFT window width (0.1 sec is quite<br/>&gt; short in this respect)<br/>&gt; - pitch movement within each sample that blurr the peaks (0.1 is<br/>&gt; quite long then....)</p><p>Well, if the sampling window was too short in one respect, and too long in<br/>another respect, perhaps the authors chose an appropriate length in the<br/>middle. This is what we all have to do. Washing the hands too little does<br/>not clean them. Washing them too much is bad for the skin.</p><p>Now that we got this long in the discussion, what about telling us what the<br/>authors should have done instead? If you think that their methods<br/>predetermined their results, you can perhaps say which methods would have<br/>lead to &quot;meaningful&quot; results (in your view). For example, which method would<br/>have given the ratio 11:9 &quot;a chance&quot; (as you expressed it)?</p><p>Martin</p></div><h3><a id=46814 href="#46814">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/9/2003 12:53:37 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello,</p><p>&gt; Martin (refering to Paul comment):<br/>&gt; Yes, in this particular case. I also agree that in the vocal tract, as<br/>&gt; opposed to bells, there are no inharmonic vibration modes. The<br/>difference<br/>&gt; between us may be that we have different views on the transient<br/>status of<br/>&gt; all speech sounds. Well over 90 % of all sounds of phoneme length are<br/>&gt; glissandi or portamenti. In such transient sounds the harmonic<br/>spectrum is<br/>&gt; stretched or compressed, that is, &quot;disharmonized&quot;. The reason is<br/>that the<br/>&gt; various vibration modes have different latencies. This in turn is<br/>due to the<br/>&gt; different masses that vibrate. Usually, smaller masses (faster<br/>vibrations,<br/>&gt; higher frequencies) have shorter latencies.</p><p>Once of all, the vocal tract has NOTHING to do with the harmonic structure<br/>of the human voice! The harmonic structure of the spectrum depends on the<br/>glottis phase lock mechanisms. The vocal tract has broad band resonances<br/>but which is only very remotely like a set of harmonics.</p><p>Then, in &quot;transient parts&quot; of the signal, the harmonic spectrum is<br/>evenly compressed or stretched AS FAR AS CAN BE MEASURED. In transient<br/>parts, infinite time window approximation (i.e. the signal is stable<br/>in a long window) is less and less true and the frequency analysis is<br/>less and less reliable. Thas does no means that the underlying<br/>physical mechanisms that produces harmonic structure ceases to exist:<br/>that just means that its effect cannot be reliably measured from<br/>frequency analysis (uncertainty principle is back). No such thing as<br/>physical &quot;disharmonisation&quot; can be measured from speech signal.<br/>Nevertheless, we have to admit that when the signal is transient, the<br/>precision to which harmonicity (or inharmonicity) can be assessed is<br/>limited by the uncertainty principle. This limitation may sometime be<br/>erroneously interpreted as &quot;disharmonisation&quot; but in fact, it just a<br/>measurement artifact, not a physical phenomena.</p><p>&gt; So, the reason why 7:5 sticks out in the data, but 7:6 doesn&apos;t, may<br/>be that<br/>&gt; the &quot;signal&quot; at 7:6 is so small that it is blurred out by the ubiquitous<br/>&gt; transient disharmonization. For me at least this would be an interesting<br/>&gt; finding. We knew about these blurring effects, but we did not know their<br/>&gt; extent in the statistics of a large speech corpus.</p><p>Fran?ois:</p><p>&gt; The flat background is due to<br/>&gt; - harmonic peaks spreading due to FFT window width (0.1 sec is quite<br/>&gt; short in this respect)<br/>&gt; - pitch movement within each sample that blurr the peaks (0.1 is<br/>&gt; quite long then....)</p><p>&gt; Martin<br/>&gt; Well, if the sampling window was too short in one respect, and too<br/>long in<br/>&gt; another respect, perhaps the authors chose an appropriate length in the<br/>&gt; middle. This is what we all have to do. Washing the hands too little<br/>does<br/>&gt; not clean them. Washing them too much is bad for the skin.<br/>&gt;<br/>&gt; Now that we got this long in the discussion, what about telling us<br/>what the<br/>&gt; authors should have done instead?</p><p>Have a good walk :-).</p><p>Seriously: the way data is gathered is not the issue, not at all. My<br/>disgression was just to explain why there is a &quot;floor&quot; to the<br/>normalized spectrum (and not only discrete peaks at simple ratio values)</p><p>The problem is not in the data gathering (that seems fair enough), but<br/>in the spectrum normalisation process. I have explained this with<br/>sufficient detail already.</p><p>&gt; If you think that their methods<br/>&gt; predetermined their results, you can perhaps say which methods would<br/>have<br/>&gt; lead to &quot;meaningful&quot; results (in your view). For example, which<br/>method would<br/>&gt; have given the ratio 11:9 &quot;a chance&quot; (as you expressed it)?</p><p>Select extraterrestrial speakers with vocal tract of 5cm and an<br/>average pitch of 20 Hz.<br/>Select those who speak veeeeeeeeeeeeeryyyyyyyyyyyyy<br/>sloooooooooooooooolyyyyyyyyyyyyyy and monotonously (to avoid pitch and<br/>spectrum instability).<br/>Fiddle sample length until peaks appears the more clearly possible<br/>(that was probably already done for the paper to find 0.1 sec as a<br/>good compromise).</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46817 href="#46817">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/9/2003 6:48:53 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; Once of all, the vocal tract has NOTHING to do with the harmonic structure<br/>&gt; of the human voice!</p><p>Not with the frequencies of the partials, but with the amplitudes at these<br/>frequencies!!!!!!<br/>That&apos;s what the study was all about.</p><p>&gt; No such thing as<br/>&gt; physical &quot;disharmonisation&quot; can be measured from speech signal.</p><p>If partial 5 leads partial 4 in a glissando you have a physical<br/>disharmonization!!!!!!</p><p>&gt; Nevertheless, we have to admit that when the signal is transient, the<br/>&gt; precision to which harmonicity (or inharmonicity) can be assessed is<br/>&gt; limited by the uncertainty principle. This limitation may sometime be<br/>&gt; erroneously interpreted as &quot;disharmonisation&quot; but in fact, it just a<br/>&gt; measurement artifact, not a physical phenomena.</p><p>This view is in error. The physical disharmonization, as described above is<br/>real. I can also be measured. For example, take a time window in which<br/>partial 5 has reached 60 % of its frequency shift whereas partial 4 has only<br/>reached 30 % of its frequency shift.</p><p>&gt;&gt; Now that we got this long in the discussion, what about telling us<br/>&gt;&gt; what the authors should have done instead?</p><p>&gt; The problem is not in the data gathering (that seems fair enough), but<br/>&gt; in the spectrum normalisation process. I have explained this with<br/>&gt; sufficient detail already.</p><p>What then should the authors have done to get the &quot;problem&quot; out of the<br/>&quot;spectrum normalization process&quot;?</p><p>&gt;&gt; If you think that their methods<br/>&gt;&gt; predetermined their results, you can perhaps say which methods would<br/>&gt;&gt; have lead to &quot;meaningful&quot; results (in your view). For example, which<br/>&gt;&gt; method would have given the ratio 11:9 &quot;a chance&quot; (as you expressed it)?</p><p>&gt; Select extraterrestrial speakers with vocal tract of 5cm and an<br/>&gt; average pitch of 20 Hz.<br/>&gt; Select those who speak veeeeeeeeeeeeeryyyyyyyyyyyyy<br/>&gt; sloooooooooooooooolyyyyyyyyyyyyyy and monotonously (to avoid pitch and<br/>&gt; spectrum instability).</p><p>Are you saying now that the results of the study reflect the nature of human<br/>speech?</p><p>Martin</p></div><h3><a id=46832 href="#46832">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/10/2003 12:38:59 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Martin</p><p>&gt; Fran&ccedil;ois:<br/>&gt; No such thing as<br/>&gt; physical &quot;disharmonisation&quot; can be measured from speech signal.</p><p>&gt; Martin<br/>&gt; If partial 5 leads partial 4 in a glissando you have a physical<br/>&gt; disharmonization!!!!!!</p><p>If... but this does not exists</p><p>1- There is no such thing as &quot;physical disharmonization&quot; of human voice.<br/>2- If such thing exists it is not measureable.<br/>3- Even if it exists and is measureable, it would be insignificant in<br/>random speech sample of 0.1 sec</p><p>In other words, all harmonics moves togethers when pitch moves<br/>reasonably slowly. When pitch moves very quickly, the precision that<br/>would gives clue to disharmonisation cannot be reached. Finally, 0.1<br/>sec. of speech has good chance of being fairly stable in term of pitch<br/>and formant (there is no huge pitch jump like in singing voice).</p><p>I have checked manually thousands of speech and signing voice using<br/>FFTs and other analysis method and never observed such thing.</p><p>If you can provide me with  a single sample of speech (or even singing<br/>voice) that features disharmonisations or even provide me with a<br/>single serious paper that describes this phenomena, I will be too<br/>happy to get educated.</p><p>Nevertheless, let us investigate the consequence of &quot;disharmonisation&quot;<br/>(even though it does not exist).</p><p>Let suppose that f5 change ahead of f4 in an upward pitch movement<br/>(this is exagerated due to limitaion of ascii art)</p><p>............................<br/>...600.|................----<br/>.......|............----....<br/>.......|........----........<br/>.......|....----............<br/>f5 500.|----................<br/>.......|....................<br/>.......|....................<br/>.......|....................<br/>.......|....................<br/>f4 400 |--------------------<br/>............................</p><p>In this window, average value of f4 is 400 while average value of f5<br/>is 550. So instead of contributing at 5:4 1.2500, (if Fm is f4) it<br/>contribute to a &quot;slightly&quot; higher value 1.3750.</p><p>It is resonable to think that if higher frequencies move faster<br/>upward, they move faster downward as well. For instance the contrary<br/>movement of above shall be</p><p>............................<br/>f5.600.|----................<br/>.......|....----.............<br/>.......|........---- ........<br/>.......|............---- ....<br/>...500.|................----.<br/>f4 480.|--------------------<br/>.......|....................<br/>.......|....................<br/>.......|....................<br/>...400 |--------------------</p><p>So instead of contributing at 1.250 (if Fm is f4), this sample<br/>contribute to a &quot;slighly&quot; lower value of 1.145.</p><p>On the average, there should be roughly the same (small) amont of<br/>upward and downward pitch. So ther should be roughly the same amount<br/>of contribution at right and at left of each average peak of the<br/>average normalize spectrum.</p><p>If it is the case (time symetry of &quot;disharmonisation&quot;)<br/>disharmonisation does not changes peak locations on average normalized<br/>spectrum, it just spread them out a little bit.</p><p>so</p><p>4 - if physical disharmonization exist (it don&apos;t) and is measurable<br/>(it is not) and is significant in 0.1 sec. sample (unlikely), its<br/>upward and downward contribution shall spread out peak locations but<br/>not change peak centers.</p><p>Ok let suppose that I am wrong all the way, disharmonisation occurs<br/>and occurs more strongly on upward (downward) pitch movement, this<br/>should rises (lowers) every peak above 1 in the average normaliuzed<br/>spectrum. So the 1.5 peak should be slighly higher (lower) than 1.5, 2<br/>slighly higher (lower) that 2.0 and so on.</p><p>so finally.</p><p>5 - existence of significant physical disharmonisation would<br/>contradict the average spectrum presented in the paper.</p><p>&gt; Fran&ccedil;ois:<br/>&gt; The problem is not in the data gathering (that seems fair enough), but<br/>&gt; in the spectrum normalisation process. I have explained this with<br/>&gt; sufficient detail already.<br/>&gt; Martin:<br/>&gt; What then should the authors have done to get the &quot;problem&quot; out of the<br/>&gt; &quot;spectrum normalization process&quot;?</p><p>The variable analysed is roughly Fn F/Fm ~= F/(round (F1/f0) where F<br/>has proeminet values at f0 2f0, 3f0 etc.</p><p>Suppose that I take a population, I define a variable as ShoeSize /<br/>round (IQ / ShoeSize) and analyse stats on this variable. I discover a<br/>lot of interesting properties of this variable. What is the problem?<br/>How can I get it out?</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46836 href="#46836">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/10/2003 10:32:19 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;francois_laferriere&quot;<br/>&lt;francois.laferriere@o...&gt; wrote:</p><p>&gt; 1- There is no such thing as &quot;physical disharmonization&quot; of human<br/>voice.</p><p>if i recall correctly, the only instrument on which judith brown<br/>found &quot;disharmonization&quot; during pitch changes was the violin. here&apos;s<br/>her paper again:</p><p><a href="http://www.wellesley.edu/Physics/brown/pubs/freqRatV99P1210-P1218.djvu">http://www.wellesley.edu/Physics/brown/pubs/freqRatV99P1210-P1218.djvu</a></p><p>unfortunately i have no idea how i was able to read this paper in the<br/>first place, my system certainly doesn&apos;t know how to open it now!</p><p>&gt; The variable analysed is roughly Fn F/Fm ~= F/(round (F1/f0) where F<br/>&gt; has proeminet values at f0 2f0, 3f0 etc.<br/>&gt;<br/>&gt; Suppose that I take a population, I define a variable as ShoeSize /<br/>&gt; round (IQ / ShoeSize)</p><p>francois, i think we may be getting back to the objection i already<br/>debunked (or so i thought). it seems you&apos;re putting &quot;round&quot; in the<br/>first formula because you&apos;d like the formula to begin with the vocal<br/>tract (simplified as a single-resonator system?) resonant frequency.<br/>if so, i&apos;ll say again, this is unfair. the authors are only concerned<br/>with the frequencies actually present in the sound, since this is the<br/>only aspect of vocalization relevant to their hypothesis. therefore,<br/>this &quot;rounding&quot; occurs naturally -- or not at all in the case of<br/>unvoiced phonemes. either way, the authors simply use the frequencies<br/>present in the signal. am i misinterpreting you?</p></div><h3><a id=46841 href="#46841">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/10/2003 1:19:28 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:</p><p>&gt; francois wrote:<br/>&gt; 1- There is no such thing as &quot;physical disharmonization&quot; of human<br/>&gt; voice.</p><p>&gt; if i recall correctly, the only instrument on which judith brown<br/>&gt; found &quot;disharmonization&quot; during pitch changes was the violin. here&apos;s<br/>&gt; her paper again:</p><p><a href="http://www.wellesley.edu/Physics/brown/pubs/freqRatV99P1210-P1218.djvu">http://www.wellesley.edu/Physics/brown/pubs/freqRatV99P1210-P1218.djvu</a></p><p>This is correct. But she only considered vibratos, not glissandi. For<br/>vibratos the frequency shifts are very small in string instruments and<br/>extremely small in wind instruments. So the disharmonization due to latency<br/>differences between the partials is negligible in her experiments.</p><p>&gt; unfortunately i have no idea how i was able to read this paper in the<br/>&gt; first place, my system certainly doesn&apos;t know how to open it now!</p><p>You need to download a plug-in for djvu files. These are fairly short and<br/>freely available on the web. Just google for &quot;djvu&quot; and pick the most<br/>convenient download offer.</p><p>Martin</p></div><h3><a id=46842 href="#46842">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/10/2003 1:05:34 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt;&gt; Fran&iuml;&iquest;&half;ois:<br/>&gt;&gt; No such thing as<br/>&gt;&gt; physical &quot;disharmonisation&quot; can be measured from speech signal.</p><p>&gt;&gt; Martin:<br/>&gt;&gt; If partial 5 leads partial 4 in a glissando you have a physical<br/>&gt;&gt; disharmonization!!!!!!</p><p>&gt; If... but this does not exists</p><p>In wind instruments, low tones have longer latencies than high tones. And<br/>within the spectrum of one tone of these instruments, the latencies vary<br/>between the partials. There is no reason to assume that this can be<br/>different with the human voice.</p><p>&gt; If you can provide me with a single sample of speech (or even singing<br/>&gt; voice) that features disharmonisations or even provide me with a<br/>&gt; single serious paper that describes this phenomena, I will be too<br/>&gt; happy to get educated.</p><p>The latency behavior of partials in wind instruments is described in good<br/>books on the acoustics of wind instruments.</p><p>&gt; If it is the case (time symetry of &quot;disharmonisation&quot;)<br/>&gt; disharmonisation does not changes peak locations on average normalized<br/>&gt; spectrum, it just spread them out a little bit.</p><p>Well, &quot;a little bit&quot; is quite a joke here. In your example the ratio<br/>fluctuations around 5/4 (1.25) went down below 6:5 (1.2) and up above 4:3<br/>(1.33) !!!!!!!  This is indeed a terrible &quot;spreading&quot;. It would be blurring<br/>the whole range between the minor third and the fourth !!!!!  But thanks for<br/>demonstrating to the list, then, WHY the ratio distribution curve has such a<br/>strong flat component (high plateau)  ;-))</p><p>&gt;&gt; Fran&iuml;&iquest;&half;ois:<br/>&gt;&gt; The problem is not in the data gathering (that seems fair enough), but<br/>&gt;&gt; in the spectrum normalisation process. I have explained this with<br/>&gt;&gt; sufficient detail already.<br/>&gt;&gt; Martin:<br/>&gt;&gt; What then should the authors have done to get the &quot;problem&quot; out of the<br/>&gt;&gt; &quot;spectrum normalization process&quot;?</p><p>&gt;The variable analysed is roughly Fn F/Fm ~= F/(round (F1/f0) where F<br/>&gt;has proeminet values at f0 2f0, 3f0 etc.</p><p>&gt;Suppose that I take a population, I define a variable as ShoeSize /<br/>&gt;round (IQ / ShoeSize) and analyse stats on this variable. I discover a<br/>&gt;lot of interesting properties of this variable. What is the problem?<br/>&gt;How can I get it out?</p><p>Could you please say what the authors should have done to avoid the problem<br/>which you claim they have in their methods?</p><p>Martin</p></div><h3><a id=46852 href="#46852">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/11/2003 3:01:38 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello,</p><p>&gt; Francois:<br/>&gt; The variable analysed is roughly Fn F/Fm ~= F/(round (F1/f0) where F<br/>&gt; has proeminet values at f0 2f0, 3f0 etc.<br/>&gt;<br/>&gt; Suppose that I take a population, I define a variable as ShoeSize /<br/>&gt; round (IQ / ShoeSize)</p><p>&gt; Paul:<br/>&gt; francois, i think we may be getting back to the objection i already<br/>&gt; debunked (or so i thought). it seems you&apos;re putting &quot;round&quot; in the<br/>&gt; first formula because you&apos;d like the formula to begin with the vocal<br/>&gt; tract (simplified as a single-resonator system?) resonant frequency.<br/>&gt; if so, i&apos;ll say again, this is unfair. the authors are only concerned<br/>&gt; with the frequencies actually present in the sound, since this is the<br/>&gt; only aspect of vocalization relevant to their hypothesis. therefore,<br/>&gt; this &quot;rounding&quot; occurs naturally -- or not at all in the case of<br/>&gt; unvoiced phonemes. either way, the authors simply use the frequencies<br/>&gt; present in the signal. am i misinterpreting you?</p><p>Okay, I probably skipped an important step, in order to get quickly to<br/>the explanation of the physical limits contraining N (the wavenumber<br/>of Fm) and thus indirectly constraining the values of (a/b).</p><p>In fact it is not necessary to introduce F1 or any rounding to<br/>understand why the spectrum is discrete.<br/>It is not necessary to introduce any knowledge of acoustic. A simple<br/>phenomenological viewpoint is necessary.</p><p>Let assume that the actual FFT spectrum of 1-B is representative of<br/>human speech (and indeed it is): a spectrum made of evenly spaced<br/>peaks, at integer multiple of a frequency f0. This comb-like spectrum<br/>is modulated by a specral envelope, from which we know nothing except<br/>that it has some maximum. We know nothing of it the enveloppe, but we<br/>can nevertheless deduce that there is a maximum of it that indeed<br/>occurs very likely at multiple of f0, so instead of expressing Fm in<br/>term of frequency distribution, it is possible to express it in term<br/>of wavenumber distribution, as it is fairly done in figure 3A.</p><p>If we know nothing of the spectra, it is nevertheless possible, from<br/>the probablity distrib. of figure 3A as far as I can read them</p><p>P(1) = 0.035<br/>p(2) = 0.18<br/>p(3) = 0.24<br/>p(4) = 0.26<br/>p(5) = 0.17<br/>p(6) = 0.08<br/>p(7) = 0.03<br/>p(8) = 0.01<br/>p(9) = 0.003<br/>p(10) = 0.002<br/>p(n)  = 0 for n &gt; 10</p><p>to deduce some&quot;results&quot;.</p><p>The sum is not exacly 1 (1.01), but I dont seek high precision, just<br/>figures.</p><p>So made from those values, a simple, brute force computation using excel.<br/>A created a complete version of table 3b, with all values up to 10,<br/>and weighted each ratio by p(n), n being the denominator to compute a<br/>rough spectral value.</p><p>For instance A(1.33333) = p(4:3) + p(8:6) + p(12:9)<br/>assuming that we know nothing of the spectrum but the wavenumber of<br/>Fm, let say that numerator are equiprobable).</p><p>That lead to the extremely raw approximation<br/>A(1.33333) = p(3) + p(6) + p(9) = 0.323.</p><p>I computed all the ratio weights like this and then converted linear<br/>results in dB (then A(1) = 0 dB )</p><p>I got</p><p>F/Fm          A(F/Fm)<br/>1,000000000    0<br/>1,100000000  -27<br/>1,111111111  -25<br/>1,125000000  -20<br/>1,142857143  -15<br/>1,166666667  -11<br/>1,200000000   -8<br/>1,250000000   -6<br/>1,300000000  -27<br/>1,333333333   -5<br/>1,375000000  -20<br/>1,400000000   -8<br/>1,428571429  -15<br/>1,444444444  -25<br/>1,500000000   -3<br/>1,555555556  -25<br/>1,571428571  -15<br/>1,600000000   -8<br/>1,666666667   -5<br/>1,700000000  -26<br/>1,714285714  -15<br/>1,750000000   -6<br/>1,777777778  -25<br/>1,800000000   -8<br/>1,833333333  -11<br/>1,857142857  -15<br/>1,875000000  -20<br/>1,888888889  -25<br/>1,900000000  -27<br/>2,000000000    0</p><p>Obviously, this is raw, it does not feature the spectral decay of<br/>actual data, but it shows the most salient feature of normalised spectrum:<br/> - high amplitudes at simple ratios with large peaks at 1/2 and 2<br/> - rough symetry around 1.5 (I didn&apos;t notice that before!)</p><p>AND THIS IS COMPUTED FROM DATA OF FIGURE 3A ALONE.</p><p>It misses a few things that can be explained fairly well, but some<br/>phonation acoustics is needed (formant structure etc.), I wont get<br/>into that.</p><p>&gt;&gt; Fran?ois:<br/>&gt;&gt; No such thing as<br/>&gt;&gt; physical &quot;disharmonisation&quot; can be measured from speech signal.</p><p>&gt;&gt; Martin:<br/>&gt;&gt; If partial 5 leads partial 4 in a glissando you have a physical<br/>&gt;&gt; disharmonization!!!!!!</p><p>&gt; If... but this does not exists</p><p>I should have added &quot;in speech signal&quot;</p><p>&gt; Martin<br/>&gt; In wind instruments, low tones have longer latencies than high<br/>tones. And<br/>&gt; within the spectrum of one tone of these instruments, the latencies vary<br/>&gt; between the partials. There is no reason to assume that this can be<br/>&gt; different with the human voice.</p><p>Yes there is reason, but that will send us back to a discussion we had<br/>earlier on this forum about the acoustical coupling between vocal fold<br/>and vocal tract.</p><p>You still confused the sound production of woodwind (and brass) that<br/>rely on a strong coupling between the reed (or whistle or lips) with<br/>the bore. In a woodwind, when a hole is open, a high frequency mode,<br/>having a high pressure near the hole, may die out instantenously,<br/>before the reed is &quot;aware&quot; (due to bore length and finite value of<br/>sound velocity (or something like that).</p><p>There is no significant coupling between vocal fold an the rest of the<br/>vocal tract. Try to make a woodwind out of a rolled slice of steak :).<br/>Vocal fold frequency depends on its own physical properties (mass<br/>distribution, tension) and subglottic pressure. Any change of this<br/>parameters propagates &quot;instantanously&quot; in the vocal fold (being much<br/>smaller than a wavelength, in particular because sound velocity is<br/>much higher in &quot;steak&quot; than in air, ask your butcher :-)). So</p><p>- there is no reason to think that disharmonisation exist in human voice<br/>- it has, until proven otherwise, never been observed or documented</p><p>&gt; Martin:<br/>&gt; The latency behavior of partials in wind instruments is described in<br/>good<br/>&gt; books on the acoustics of wind instruments.</p><p>No doubt. I will try to get some more info, thank.</p><p>&gt; Frano?=ois:<br/>&gt; If it is the case (time symetry of &quot;disharmonisation&quot;)<br/>&gt; disharmonisation does not changes peak locations on average normalized<br/>&gt; spectrum, it just spread them out a little bit.</p><p>&gt; Martin:<br/>&gt; Well, &quot;a little bit&quot; is quite a joke here. In your example the ratio<br/>&gt; fluctuations around 5/4 (1.25) went down below 6:5 (1.2) and up<br/>above 4:3<br/>&gt; (1.33) !!!!!!! This is indeed a terrible &quot;spreading&quot;. It would be<br/>blurring<br/>&gt; the whole range between the minor third and the fourth !!!!! But<br/>thanks for<br/>&gt; demonstrating to the list, then, WHY the ratio distribution curve<br/>has such a<br/>&gt; strong flat component (high plateau) ;-))</p><p>Obviously, those numbers are ridiculous !!! It just a tought<br/>experiment to show that even for extreme values of disharmonisation,<br/>observed peak shall not move (exactly as observed).</p><p>&gt; Martin<br/>&gt; Could you please say what the authors should have done to avoid the<br/>problem<br/>&gt; which you claim they have in their methods?</p><p>Plenty of thing in fact, but I think that none of them would produces<br/>&quot;interresting&quot; results: exemple</p><p>1 - use only whispered voice.<br/>Using only whispered voice would suppress the bias due to the harmonic<br/>structure.<br/>Fm would not be forced to be (most of the time) a small integer<br/>multiple of f0.<br/>Would give an idea of average spectral decay after Fm.</p><p>2 - select f0 instead of Fm<br/>pitch extraction algorithm are fairly reliable after all. That would<br/>lead to peak only at integer values (instead of simple ratio) an<br/>background noise</p><p>3 - compute and use F1 instead of Fm<br/>That would also suppress the bias to harmonic structure. There are<br/>rubust algorithms to do so. Same result as 1.</p><p>4 - select longer, spectraly stable sample<br/>Would give the same result, with sharper peaks and much less noise.<br/>much less trouble to debunk :)</p><p>etc..</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46854 href="#46854">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/11/2003 7:32:29 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Francois:</p><p>&gt;&gt; Martin<br/>&gt;&gt; In wind instruments, low tones have longer latencies than high<br/>&gt;&gt; tones. And<br/>&gt;&gt; within the spectrum of one tone of these instruments, the latencies vary<br/>&gt;&gt; between the partials. There is no reason to assume that this can be<br/>&gt;&gt; different with the human voice.</p><p>&gt; Yes there is reason, but that will send us back to a discussion we had<br/>&gt; earlier on this forum about the acoustical coupling between vocal fold<br/>&gt; and vocal tract.<br/>........</p><p>&gt; Vocal fold frequency depends on its own physical properties (mass<br/>&gt; distribution, tension) and subglottic pressure.</p><p>You mean &quot;vocal fold frequenciES&quot; (f0 plus partials) change in synchrony<br/>without relevant latencies between the partials. This is plausible. But<br/>again, not the FREQUENCIES at the source (vocal folds) determine the power<br/>spectra of speech. The AMPLITUDES at each of these frequencies do it. And<br/>the amplitudes at the various partial frequencies do NOT change in<br/>synchrony. This is due to the different sizes of vibrating air masses that<br/>are involved in these amplitudes. In pitch shifts, phase leads across the<br/>amplitude changes of partials have to occur, on simple physical grounds.</p><p>&gt; Obviously, those numbers are ridiculous !!! It just a tought<br/>&gt; experiment to show that even for extreme values of disharmonisation,<br/>&gt; observed peak shall not move (exactly as observed).</p><p>The peaks are not shifted, of course, but the majority of ratio probability<br/>is FLAT, that is BETWEEN the low-order-ratio peaks. This is what the figures<br/>of the study show.</p><p>&gt;&gt; Martin<br/>&gt;&gt; Could you please say what the authors should have done to avoid the<br/>&gt;&gt; problem which you claim they have in their methods?</p><p>&gt; Plenty of thing in fact, but I think that none of them would produces<br/>&gt; &quot;interresting&quot; results: exemple</p><p>&gt; 1 - use only whispered voice.</p><p>Not relevant for an influence of speech on the evolution of hearing.</p><p>&gt; 2 - select f0 instead of Fm<br/>&gt; pitch extraction algorithm are fairly reliable after all. That would<br/>&gt; lead to peak only at integer values (instead of simple ratio) an<br/>&gt; background noise</p><p>This would not change the results in the least. Again the ratios 1:2:3:4:5:6<br/>would be standing out !!!!!!!!!!</p><p>&gt; 3 - compute and use F1 instead of Fm<br/>&gt; That would also suppress the bias to harmonic structure. There are<br/>&gt; rubust algorithms to do so. Same result as 1.</p><p>You would need a PEAK as a point of reference in each of the sample spectra,<br/>if you want to show the distribution of ratios between peaks. F1 can be<br/>anywhere, even in a dip of a sample spectrum.</p><p>&gt; 4 - select longer, spectraly stable sample<br/>&gt; Would give the same result, with sharper peaks and much less noise.<br/>&gt; much less trouble to debunk :)</p><p>In natural speech, longer samples are in NO WAY spectrally more stable.</p><p>Fran&iuml;&iquest;&half;ois, none of your suggestions would be of help in finding answers to<br/>the questions of the research project. It seems to me you misunderstood<br/>what the authors tried to investigate.</p><p>Martin</p></div><h3><a id=46858 href="#46858">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/11/2003 11:07:26 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Francois:</p><p>&gt; &gt; 2 - select f0 instead of Fm<br/>&gt; &gt; pitch extraction algorithm are fairly reliable after all. That<br/>would<br/>&gt; &gt; lead to peak only at integer values (instead of simple ratio) an<br/>&gt; &gt; background noise<br/>&gt;<br/>&gt; This would not change the results in the least.</p><p>obviously it would change the results, because you&apos;d only see peaks<br/>at 1/1, 2/1, 3/1, 4/1, 5/1 . . . but no longer at 3/2, 4/3, 5/3,<br/>5/4 . . .</p></div><h3><a id=46862 href="#46862">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/11/2003 12:31:58 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Paul:</p><p>&gt; &gt; &gt; 2 - select f0 instead of Fm<br/>&gt; &gt; &gt; pitch extraction algorithm are fairly reliable after all. That<br/>&gt; would<br/>&gt; &gt; &gt; lead to peak only at integer values (instead of simple ratio) an<br/>&gt; &gt; &gt; background noise<br/>&gt; &gt;<br/>&gt; &gt; This would not change the results in the least.<br/>&gt;<br/>&gt; obviously it would change the results, because you&apos;d only see peaks<br/>&gt; at 1/1, 2/1, 3/1, 4/1, 5/1 . . . but no longer at 3/2, 4/3, 5/3,<br/>&gt; 5/4 . . .</p><p>But Paul, what would the ratios between these peaks be?<br/>Between 2/1 and 3/1 we would get 2/3,<br/>Between 3/1 and 4/1 we would get 3/4,<br/>Between 4/1 and 5/1 we would get 4/5,<br/>etc.<br/>We would end up with the same increased probability of the low-order<br/>ratios, as when using the authors&apos; methods. And we would end up with<br/>the same limit of visible ratios, because this limit is determined by<br/>the relation of f0 to the first formant range of the human vocal<br/>tract.</p><p>Martin</p></div><h3><a id=46864 href="#46864">ðŸ”—</a>Paul Erlich &#x3C;perlich@aya.yale.edu&#x3E;</h3><span>9/11/2003 12:54:11 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; Paul:<br/>&gt;<br/>&gt; &gt; &gt; &gt; 2 - select f0 instead of Fm<br/>&gt; &gt; &gt; &gt; pitch extraction algorithm are fairly reliable after all.<br/>That<br/>&gt; &gt; would<br/>&gt; &gt; &gt; &gt; lead to peak only at integer values (instead of simple ratio)<br/>an<br/>&gt; &gt; &gt; &gt; background noise<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; This would not change the results in the least.<br/>&gt; &gt;<br/>&gt; &gt; obviously it would change the results, because you&apos;d only see<br/>peaks<br/>&gt; &gt; at 1/1, 2/1, 3/1, 4/1, 5/1 . . . but no longer at 3/2, 4/3, 5/3,<br/>&gt; &gt; 5/4 . . .<br/>&gt;<br/>&gt; But Paul, what would the ratios between these peaks be?<br/>&gt; Between 2/1 and 3/1 we would get 2/3,<br/>&gt; Between 3/1 and 4/1 we would get 3/4,<br/>&gt; Between 4/1 and 5/1 we would get 4/5,</p><p>right, but instead of seeing these ratios among the original peaks,<br/>you&apos;re having to take ratios of peaks to get them. that&apos;s a change.</p></div><h3><a id=46888 href="#46888">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/12/2003 2:06:17 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt; Frano?=ois<br/>&gt; Vocal fold frequency depends on its own physical properties (mass<br/>&gt; distribution, tension) and subglottic pressure.</p><p>&gt; Martin:<br/>&gt; You mean &quot;vocal fold frequenciES&quot; (f0 plus partials) change in synchrony<br/>&gt; without relevant latencies between the partials. This is plausible. But<br/>&gt; again, not the FREQUENCIES at the source (vocal folds) determine the<br/>power<br/>&gt; spectra of speech. The AMPLITUDES at each of these frequencies do<br/>it. And<br/>&gt; the amplitudes at the various partial frequencies do NOT change in<br/>&gt; synchrony.</p><p>Up to that we are in total agreement</p><p>&gt; This is due to the different sizes of vibrating air masses that<br/>&gt; are involved in these amplitudes. In pitch shifts, phase leads<br/>across the<br/>&gt; amplitude changes of partials have to occur, on simple physical grounds.</p><p>Whatever is your physical interpretation right or wrong, there is no<br/>such thing as measurable phase lead (except possibly due to poor<br/>recording hardware<br/>as was for instance early digital recording AD/DA) or &quot;disharmonisation&quot;</p><p>Neverteless, I was puzzled in my early attempt at &quot;high precision<br/>pitch analysis&quot; by the fact that in pitch movement, amplitude ratio<br/>between partial may change. For instance (asci art again)</p><p>............................<br/>...600.|................====<br/>.......|............====....<br/>.......|........----........<br/>.......|....----............<br/>f5 500.|----................<br/>.......|.................---<br/>.......|.............----...<br/>.......|.........----.......<br/>.......|.....====...........<br/>f4 400 |=====---------------<br/>............................</p><p>where === denote high intensity peak, while --- is lower intensity<br/>(again this is exagerated due to limitation of ascii art). f4 moves<br/>from 400 Hz to 480 Hz while f5 moves from 500 Hz to 600 Hz so that at<br/>any given time f5/f4 = 5/4. But as f4 is more intense aroud 410 Hz,<br/>its average on the whole window will sho a peak around 410 and not<br/>around 440. For the same reason f5 will not average around 550 but<br/>around 485 Hz. So, on the window average, f5/f4 seems to be inharmonic<br/>(1.18 instead of 1.25).</p><p>To get rid of this annoying effect, you have to use shorter window,<br/>but for short windows, the uncertainty principle kicks in. So<br/>compromise must be made on window size to get best spectral estimate.</p><p>This computational inharmonicity do exist (and in fact occur all the<br/>time for .1 sec window)</p><p>But again:</p><p>- This inharmonicity is not physical<br/>- It shall spread the peaks but not shift them on the average as they<br/>are as likely to contribute at right or at left of each peak.</p><p>again we seem to agree</p><p>&gt; The peaks are not shifted, of course, but the majority of ratio<br/>probability<br/>&gt; is FLAT, that is BETWEEN the low-order-ratio peaks. This is what the<br/>figures<br/>&gt; of the study show.</p><p>The paper rightfully focus on the peaks, not on the background.<br/>Secondly, I see no flat floor but gentle slope on each side of each<br/>peak that eventually merge with the neighbouring peaks. I provided<br/>plenty of plausible causes for peak spreading and background noise:</p><p>- pitch instability<br/>- uncertainty principle<br/>- turbulent white noise<br/>- computational inharmonicity</p><p>I do not understand why you focus on what occur BETWEEN the peaks;<br/>this is very secondary.</p><p>&gt; Francois:<br/>&gt; 2 - select f0 instead of Fm<br/>&gt; pitch extraction algorithm are fairly reliable after all. That<br/>would<br/>&gt; lead to peak only at integer values (instead of simple ratio) an<br/>&gt; background noise<br/>&gt;<br/>&gt; This would not change the results in the least.</p><p>&gt; Paul:<br/>&gt; obviously it would change the results, because you&apos;d only see peaks<br/>&gt; at 1/1, 2/1, 3/1, 4/1, 5/1 . . . but no longer at 3/2, 4/3, 5/3,<br/>&gt; 5/4 . . .</p><p>absolutely exact</p><p>&gt; Martin:<br/>&gt; But Paul, what would the ratios between these peaks be?<br/>&gt; Between 2/1 and 3/1 we would get 2/3,<br/>&gt; Between 3/1 and 4/1 we would get 3/4,<br/>&gt; Between 4/1 and 5/1 we would get 4/5,<br/>&gt; etc.<br/>&gt; We would end up with the same increased probability of the low-order<br/>&gt; ratios, as when using the authors&apos; methods. And we would end up with<br/>&gt; the same limit of visible ratios, because this limit is determined by<br/>&gt; the relation of f0 to the first formant range of the human vocal<br/>&gt; tract.</p><p>Not a all, think about it<br/>Using pitch instead of Fm to normalise, and taking only harmonic<br/>contribution (that are the most significant) sample contribution are:</p><p>A(1) = A(f1)/A(f1) = 1 by definition<br/>A(2) = sum (A(f2)/A(f1))<br/>A(3) = sum (A(f3)/A(f1))<br/>...</p><p>with noise and error in between.</p><p>In fact this would demangle the current presentation where results are<br/>highly convulated where</p><p>A(1) = 1 ;<br/>A(a/b) = sum (A(fa)/A(fb)) when N = b) + sum (A(f2a)/A(f2b) when N =<br/>2b) + ...</p><p>In other words, taking the pitch (N=1 always) instead of highest Fm<br/>(prob(N) distributed around 4) would not produce peak anywhere else<br/>than at integer values.</p><p>But that is not my point at all.</p><p>My point is not to advocate any of my dubious ;-) alternate protocol..</p><p>*****************<br/>** MY POINT IS **<br/>*****************</p><p>If one is not very careful about the analysis process, you can fiddle<br/>with unrelated data until you get results fitting one preconception.<br/>In my mind, this paper is honest (otherwhise detail to understand the<br/>mistake would not have been clearly presented), but basicaly flawed.</p><p>** END OF MY POINT **</p><p>So Martin, when you write:</p><p>&gt; Frano?=ois, none of your suggestions would be of help in finding<br/>answers to<br/>&gt; the questions of the research project. It seems to me you misunderstood<br/>&gt; what the authors tried to investigate.</p><p>you are absolutely correct all the way:</p><p>- none of my suggestion would produce valuable results<br/>- I do not understand what they try to investigate<br/>- I see, but hardly, what are the questions of the research project</p><p>But I in some ways, don&apos;t care. I never questionned their conclusions,<br/>perhaps they are correct by some chance. The idea that there are<br/>traces of musical scale in spoken speech worth discussion (I do like it).</p><p>I just say that their protocol is wrong from the beginning, and<br/>produces trivial, predictable and otherwise uninteresting results,<br/>that&apos;s all.</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div><h3><a id=46891 href="#46891">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/12/2003 4:58:30 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Paul Erlich&quot; &lt;perlich@a...&gt; wrote:<br/>&gt; --- In <a href="mailto:tuning@yahoogroups.com">tuning@yahoogroups.com</a>, &quot;Martin Braun&quot; &lt;nombraun@t...&gt; wrote:<br/>&gt; &gt; Paul:<br/>&gt; &gt;<br/>&gt; &gt; &gt; &gt; &gt; 2 - select f0 instead of Fm<br/>&gt; &gt; &gt; &gt; &gt; pitch extraction algorithm are fairly reliable after all.<br/>&gt; That<br/>&gt; &gt; &gt; would<br/>&gt; &gt; &gt; &gt; &gt; lead to peak only at integer values (instead of simple<br/>ratio)<br/>&gt; an<br/>&gt; &gt; &gt; &gt; &gt; background noise<br/>&gt; &gt; &gt; &gt;<br/>&gt; &gt; &gt; &gt; This would not change the results in the least.<br/>&gt; &gt; &gt;<br/>&gt; &gt; &gt; obviously it would change the results, because you&apos;d only see<br/>&gt; peaks<br/>&gt; &gt; &gt; at 1/1, 2/1, 3/1, 4/1, 5/1 . . . but no longer at 3/2, 4/3,<br/>5/3,<br/>&gt; &gt; &gt; 5/4 . . .<br/>&gt; &gt;<br/>&gt; &gt; But Paul, what would the ratios between these peaks be?<br/>&gt; &gt; Between 2/1 and 3/1 we would get 2/3,<br/>&gt; &gt; Between 3/1 and 4/1 we would get 3/4,<br/>&gt; &gt; Between 4/1 and 5/1 we would get 4/5,<br/>&gt;<br/>&gt; right, but instead of seeing these ratios among the original peaks,<br/>&gt; you&apos;re having to take ratios of peaks to get them. that&apos;s a change.</p><p>OK. But it&apos;s a change in the looking, not a change in the results.<br/>And the change in the looking is not bigger than the change between<br/>opening your eyes once and opening your eyes twice.</p><p>Have a good day!</p><p>Martin</p></div><h3><a id=46894 href="#46894">ðŸ”—</a>Martin Braun &#x3C;nombraun@telia.com&#x3E;</h3><span>9/12/2003 6:41:02 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Fran&iuml;&iquest;&half;ois:</p><p>&gt; So, on the window average, f5/f4 seems to be inharmonic<br/>&gt; (1.18 instead of 1.25).</p><p>&gt; To get rid of this annoying effect, you have to use shorter window,<br/>&gt; but for short windows, the uncertainty principle kicks in. So<br/>&gt; compromise must be made on window size to get best spectral estimate.</p><p>&gt; This computational inharmonicity do exist (and in fact occur all the<br/>&gt; time for .1 sec window)</p><p>I am glad that we agree on the description of the phenomenon of<br/>disharmonization in pitch shifts. It&apos;s also fine that you saw in empirical<br/>data what I expected by only thinking through the physics.</p><p>&gt; But again:<br/>&gt; - This inharmonicity is not physical</p><p>Here we still disagree. You are right that the ratio 5/4 (of your example)<br/>does not disappear. But what happens, if your time windows are so short that<br/>the &quot;errors&quot; through averaging disappear? You&apos;ll see this: in some windows<br/>there is no power at the 4th partial and in other windows there is no power<br/>at the 5th partials. So you have a 5/4 ratio all the time, but one that is<br/>not real. What you have in reality is a ratio between real peaks (those that<br/>have power) which deviates from 5/4.</p><p>&gt; - It shall spread the peaks but not shift them on the average as they<br/>&gt; are as likely to contribute at right or at left of each peak.</p><p>&gt; again we seem to agree</p><p>exactly</p><p>&gt;&gt; The peaks are not shifted, of course, but the majority of ratio<br/>&gt;&gt; probability<br/>&gt;&gt; is FLAT, that is BETWEEN the low-order-ratio peaks. This is what the<br/>&gt;&gt; figures of the study show.</p><p>&gt; The paper rightfully focus on the peaks, not on the background.</p><p>The interpretation of the authors focuses on the peaks. But they displayed<br/>the complete spectra for anybody to see.</p><p>&gt; Secondly, I see no flat floor but gentle slope on each side of each<br/>&gt; peak that eventually merge with the neighbouring peaks.</p><p>Fran&iuml;&iquest;&half;ois, below the slopes - in fact: below the dips - there is a HIGH<br/>plateau of noise !!!<br/>For example in Fig.2C the noise floor is 20 times (!) as high as the<br/>difference between the peak at 5/4 and the valley between 5/4 and 6/5.</p><p>&gt; I do not understand why you focus on what occur BETWEEN the peaks;<br/>&gt; this is very secondary.</p><p>Well, in the example above there is much more between the peaks than at the<br/>peaks. This is important to note, because it shows the big difference<br/>between &quot;clean&quot; theoretically derived textbook spectra and real speech<br/>spectra. The value of the study is not to have replicated simple general<br/>wisdom on the general harmonicity of speech. It&apos;s value is to have shown<br/>what the harmonicity looks like in REAL data.</p><p>&gt; So Martin, when you write:</p><p>&gt;&gt; Fran&iuml;&iquest;&half;ois, none of your suggestions would be of help in finding<br/>&gt;&gt;answers to<br/>&gt;&gt; the questions of the research project. It seems to me you misunderstood<br/>&gt;&gt; what the authors tried to investigate.</p><p>&gt; you are absolutely correct all the way:</p><p>&gt; - none of my suggestion would produce valuable results<br/>&gt; - I do not understand what they try to investigate<br/>&gt; - I see, but hardly, what are the questions of the research project</p><p>You could have read that what you did not understand or see in the first<br/>section of the paper, which is called &quot;introduction&quot;.</p><p>&gt; But I in some ways, don&apos;t care. I never questionned their conclusions,<br/>&gt; perhaps they are correct by some chance. The idea that there are<br/>&gt; traces of musical scale in spoken speech worth discussion (I do like it).</p><p>If the idea is worth discussing, and you even like it, then the same should<br/>also apply to the details of the results. That is, which ratios stick out of<br/>the noise and which don&apos;t.</p><p>&gt; I just say that their protocol is wrong from the beginning, and<br/>&gt; produces trivial, predictable and otherwise uninteresting results,<br/>&gt; that&apos;s all.</p><p>Had the authors asked you, before starting this study, you had grossly<br/>mispredicted the amount of noise between the peaks. And you also would not<br/>have been able to predict the limit beyond which simple ratios disappear in<br/>the noise. You might have predicted sex differences, but not their exact<br/>values.</p><p>Martin</p></div><h3><a id=46985 href="#46985">ðŸ”—</a>francois_laferriere &#x3C;francois.laferriere@oxymel.com&#x3E;</h3><span>9/15/2003 1:44:26 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Hello Martin,</p><p>A final post (as far as I am concerned) on this discussion that<br/>becomes very technical, more and more distant to the list concerns and<br/>somehow, tedious.</p><p>&gt; Frano?=ois:<br/>&gt; So, on the window average, f5/f4 seems to be inharmonic<br/>&gt; (1.18 instead of 1.25).</p><p>&gt; To get rid of this annoying effect, you have to use shorter window,<br/>&gt; but for short windows, the uncertainty principle kicks in. So<br/>&gt; compromise must be made on window size to get best spectral estimate.</p><p>&gt; This computational inharmonicity do exist (and in fact occur all the<br/>&gt; time for .1 sec window)</p><p>&gt; Martin:<br/>&gt; I am glad that we agree on the description of the phenomenon of<br/>&gt; disharmonization in pitch shifts. It&apos;s also fine that you saw in<br/>empirical<br/>&gt; data what I expected by only thinking through the physics.</p><p>&gt; Frano?=ois:<br/>&gt; But again:<br/>&gt; - This inharmonicity is not physical</p><p>&gt; Martin:<br/>&gt; Here we still disagree. You are right that the ratio 5/4 (of your<br/>example)<br/>&gt; does not disappear. But what happens, if your time windows are so<br/>short that<br/>&gt; the &quot;errors&quot; through averaging disappear? You&apos;ll see this: in some<br/>windows<br/>&gt; there is no power at the 4th partial and in other windows there is<br/>no power<br/>&gt; at the 5th partials.</p><p>Not at all, what appears is:<br/>  - as window size get smaller, harmonic peaks get broader, until they<br/>merge<br/>    (and no peak extraction algorith is of any help).<br/>  - As they get broader, the assesment of harmonicity is less and less<br/>accurate</p><p>&gt; So you have a 5/4 ratio all the time, but one that is<br/>&gt; not real. What you have in reality is a ratio between real peaks<br/>(those that<br/>&gt; have power) which deviates from 5/4.</p><p>Even if that occur (an harmonic is so small that a given peak vanishes<br/>in noise) that does not mean that it cease to exist, no more that<br/>vanishing stars cease to exist in daylight. In other words, that some<br/>peaks cease to be measurable is more a signal-to-noise problem than<br/>anything related to a modification of the underlying physical process.</p><p>&gt; Frano?=ois<br/>&gt; - It shall spread the peaks but not shift them on the average as they<br/>&gt; are as likely to contribute at right or at left of each peak.</p><p>&gt; again we seem to agree</p><p>&gt; Martin:<br/>&gt; exactly</p><p>We never agree on so much before, great!</p><p>&gt; Martin<br/>&gt; The peaks are not shifted, of course, but the majority of ratio<br/>&gt; probability<br/>&gt; is FLAT, that is BETWEEN the low-order-ratio peaks. This is what the<br/>&gt; figures of the study show.</p><p>&gt; Fran&ccedil;ois:<br/>&gt; The paper rightfully focus on the peaks, not on the background.</p><p>&gt; Martin:<br/>&gt; The interpretation of the authors focuses on the peaks. But they<br/>displayed<br/>&gt; the complete spectra for anybody to see.</p><p>&gt; Fran&ccedil;ois:<br/>&gt; Secondly, I see no flat floor but gentle slope on each side of each<br/>&gt; peak that eventually merge with the neighbouring peaks.</p><p>&gt; Martin:<br/>&gt; Fran&ccedil;ois, below the slopes - in fact: below the dips - there is a HIGH<br/>&gt; plateau of noise !!!<br/>&gt; For example in Fig.2C the noise floor is 20 times (!) as high as the<br/>&gt; difference between the peak at 5/4 and the valley between 5/4 and 6/5.</p><p>20 time? what do you mean? 13 dB? where do you see a &quot;floor&quot; (at which<br/>ratio value)?</p><p>&gt; Frano?=ois:<br/>&gt; I do not understand why you focus on what occur BETWEEN the peaks;<br/>&gt; this is very secondary.</p><p>There is no floor, but an steepy slope of around -20db per octave<br/>between 1 and 2.<br/>To have a clearer picture, we should remove or &quot;normalise&quot; this slope.</p><p>But to so, we ought to know where does it come from.</p><p>Where does it come from? On normal speech, there is around -6dB from<br/>the glottal source (may vary much) and -6dB from the lips radiation<br/>caracterists, so roughly -12dB per Octave. The missing -8dB come from<br/>the fact that most data gatered for ratio between 1 and 2 are done on<br/>the right hand side of a formant. But is certainly not a straight,<br/>simple -8dB / octave, because near 1, the more complex N+1/N ratio<br/>such as 7:6 (compared to less complex 3:2) contribute more due to<br/>their average proximity to formant top. I see no way to &quot;predict&quot; this<br/>value of -20 dB because it mixes up contributions of formant bandwith,<br/>mde even more complex by the normalisation process, and glottis and<br/>lips radiation. Instead of isolating variables contributing to the<br/>background, the normalisation make them impossible to disentangle.</p><p>Not being able to modelize properly this high frequency decay hamper<br/>any attempt to interpret the background. But that is not the topic of<br/>the paper anyways.</p><p>&gt; Martin:<br/>&gt; Well, in the example above there is much more between the peaks than<br/>at the<br/>&gt; peaks. This is important to note, because it shows the big difference<br/>&gt; between &quot;clean&quot; theoretically derived textbook spectra and real speech<br/>&gt; spectra. The value of the study is not to have replicated simple general<br/>&gt; wisdom on the general harmonicity of speech. It&apos;s value is to have shown<br/>&gt; what the harmonicity looks like in REAL data.</p><p>An this has nothing to do with harmonicity/inharmonicity anayways and<br/>inharmonicity has nothing to do the topic of the paper.</p><p>&gt; So Martin, when you write:</p><p>&gt;&gt; Frano?=ois, none of your suggestions would be of help in finding<br/>&gt;&gt;answers to<br/>&gt;&gt; the questions of the research project. It seems to me you misunderstood<br/>&gt;&gt; what the authors tried to investigate.</p><p>&gt; Francois:<br/>&gt; you are absolutely correct all the way:</p><p>&gt; - none of my suggestion would produce valuable results<br/>&gt; - I do not understand what they try to investigate<br/>&gt; - I see, but hardly, what are the questions of the research project</p><p>&gt; Martin<br/>&gt; You could have read that what you did not understand or see in the first<br/>&gt; section of the paper, which is called &quot;introduction&quot;.</p><p>I was only half kidding. The paper goes from a rather broad hypothesis<br/>to a not less broad conclusion through the very small bottleneck of a<br/>debatable (to say the less) process.</p><p>&gt; Martin:<br/>&gt; If the idea is worth discussing, and you even like it, then the same<br/>should<br/>&gt; also apply to the details of the results. That is, which ratios<br/>stick out of<br/>&gt; the noise and which don&apos;t.</p><p>I explained clearly and quantitatively enough, which ration stick out<br/>and maintain what I said:</p><p>&gt; I just say that their protocol is wrong from the beginning, and<br/>&gt; produces trivial, predictable and otherwise uninteresting results,<br/>&gt; that&apos;s all.</p><p>&gt; Martin:<br/>&gt; Had the authors asked you, before starting this study, you had grossly<br/>&gt; mispredicted the amount of noise between the peaks. And you also<br/>would not<br/>&gt; have been able to predict the limit beyond which simple ratios<br/>disappear in<br/>&gt; the noise.</p><p>Well, in experimental science, who care about &quot;predicting&quot; the noise.<br/>Understanding the source of the noise in order to reduce it in the<br/>data gathering process is the only useful issue.</p><p>&gt; Martin:<br/>&gt; You might have predicted sex differences, but not their exact<br/>&gt; values.</p><p>I have been able to &quot;predict&quot; :-) peak locations and rough relatives<br/>amplitudes from harmonic number distribution alone, explain sex<br/>difference, describe roughly different contribution to background<br/>noise and spectral decay.</p><p>I think that it is not bad for a dilettante.</p><p>I must go back to the work I am paid for</p><p>yours truly</p><p>Fran&ccedil;ois Laferri&egrave;re</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            