<a href="/tuning">back to list</a><h1>Calculating Harmonic Entropy</h1><h3><a id=12557 href="#12557">ðŸ”—</a>John A. deLaubenfels &#x3C;jdl@adaptune.com&#x3E;</h3><span>9/9/2000 5:12:59 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[Carl Lumma:]<br/>&gt;&gt;&gt;Sorry for not being more clear. Could you show the 20 ratios, from<br/>&gt;&gt;&gt;the farey series used, with the lowest harmonic entropy?</p><p>[Paul Erlich:]<br/>&gt;&gt;I&apos;ve only calculated the harmonic entropy at cents values.</p><p>[Carl:]<br/>&gt;Can you ask it about the cents values represented by the ratios of a<br/>&gt;given farey order?</p><p>I&apos;ve just gotten a C++ program up and running; it calculates the<br/>following values:</p><p>   For   551.32, entropy is     4.234279<br/>   For   617.49, entropy is     4.179904<br/>   For   782.49, entropy is     4.080664<br/>   For   231.17, entropy is     4.383823<br/>   For  1071.70, entropy is     3.940651<br/>   For   435.08, entropy is     4.270048<br/>   For   933.13, entropy is     4.015730<br/>   For   266.87, entropy is     4.341114<br/>   For  1049.36, entropy is     3.922407<br/>   For   813.69, entropy is     4.028784<br/>   For  1017.60, entropy is     3.912540<br/>   For   315.64, entropy is     4.257082<br/>   For   582.51, entropy is     4.119130<br/>   For   968.83, entropy is     3.875105<br/>   For   386.31, entropy is     4.141619<br/>   For   884.36, entropy is     3.742208<br/>   For   498.04, entropy is     3.905651<br/>   For   701.96, entropy is     3.405798<br/>   For  1200.00, entropy is     1.840298<br/>   For     0.00, entropy is     2.023188</p><p>(these are your 20 points, plus 1/1, in reverse order).  This is for<br/>Farey N=100, s=.01 (internally changed to .01/log(2)).</p><p>[Paul:]<br/>&gt;&gt;&gt;&gt;right (although what I call s=1% becomes, in your units,<br/>&gt;&gt;&gt;&gt;s=1%/log(2)=1.4427.</p><p>[Carl:]<br/>&gt;&gt;&gt;Eh? How does s apply to what I did?</p><p>[Paul:]<br/>&gt;&gt;It doesn&apos;t, but if you&apos;re going to proceed to calculate harmonic<br/>&gt;&gt;entropy, it&apos;ll be good to have this straight.</p><p>[Carl:]<br/>&gt;Ah, thanks. I think I&apos;ll need to go over to something a little<br/>&gt;higher-level than LISP before I calculate harmonic entropy. I have<br/>&gt;Maple V, but perhaps I should start with Mathematica or Matlab...</p><p>Carl, I don&apos;t know LISP, but I&apos;m sure it has plenty of power!  You&apos;ve<br/>already done the hard part, sorting the Farey series and figuring the<br/>mediant widths.  Here&apos;s all you&apos;ve got to do to finish the job (Paul E,<br/>correct me if I&apos;m wrong about anything here!):</p><p>   . You&apos;ve already got a list of ratios representing the Farey series<br/>     for N=100 (natch, N can be anything else as well!).  You&apos;ve already<br/>     sorted the list and figured out the logarithmic &quot;width&quot; of each<br/>     interval using the mediant method; thus, for example, 3/2 has width<br/>     0.01414622 octaves, as you&apos;ve calculated already.</p><p>   . Pick any interval for which you want to calculate harmonic entropy.<br/>     Paul traverses even cent values, but the entropy curve is a<br/>     continuous function.  Express the interval as number of octaves, to<br/>     align with the widths above.  Example: 3/2, is approx 701.96 cents,<br/>     or 0.5849625 octaves.</p><p>   . Do a summation of the interval you&apos;ve picked over all the ratios<br/>     in the Farey series.  For each ratio, calculate a &quot;raw probability&quot;<br/>     as follows:</p><p>        dist = (pickedInterval - fareyInterval)<br/>        sdev = dist / s;         // s may be .01/log(2), etc.<br/>        prob = exp(-sdev * sdev / 2) * intervalWidth;</p><p>     Suppose, for example, you are calculating the entropy at 699 cents.<br/>     the &quot;raw probability&quot; that this will be heard as 3/2 is:</p><p>        pickedInterval =  .5825000 octaves<br/>        fareyInterval  =  .5849625 octaves<br/>        dist           = -.0024625 octaves<br/>        s              =  .0144270 octaves<br/>        sdev           = -.1706875<br/>        prob           =  exp(-0.0145671) * 0.01414622<br/>                       =  0.9855385       * 0.01414622<br/>                       =  0.0139416</p><p>     So, you&apos;ll calculate a raw probability that the 699 cents will be<br/>     heard as every one of the Farey series ratios; as you might expect,<br/>     the values get very small as you compare, say, 699 cents to 1/1!</p><p>   . Sum all the &quot;raw probabilities&quot;; they should add up to 1, but of<br/>     course they won&apos;t!  Save the sum, then make a second probability<br/>     pass and calculate the &quot;true probability&quot; that the interval will<br/>     be heard as a particular just ratio.</p><p>        prob2          = prob / probSum;  // sigma(prob2) = 1.0</p><p>   . In this second pass you&apos;re going to build the actual entropy value<br/>     using each of the individual probabilities, by summing:</p><p>        entropy        = entropy - (prob2 * log(prob2));</p><p>That&apos;s all there is to it!  For dyads, that is...</p><p>Note that entropy is positive because each probability is less than<br/>one and therefore prob*log(prob) is negative.</p><p>JdL</p></div><h3><a id=12596 href="#12596">ðŸ”—</a>Carl Lumma &#x3C;CLUMMA@NNI.COM&#x3E;</h3><span>9/10/2000 11:04:55 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt;&gt;Can you ask it about the cents values represented by the ratios of a<br/>&gt;&gt;given farey order?<br/>&gt;<br/>&gt;I&apos;ve just gotten a C++ program up and running; it calculates the<br/>&gt;following values:<br/>//<br/>&gt;(these are your 20 points, plus 1/1, in reverse order).  This is for<br/>&gt;Farey N=100, s=.01 (internally changed to .01/log(2)).</p><p>Cool, but I was asking for the 20 ratios from Farey order 100 with<br/>the lowest harmonic entropy, as opposed to the h.e. values for the<br/>&quot;widest&quot; 20 ratios from Farey order 100.</p><p>&gt;Carl, I don&apos;t know LISP, but I&apos;m sure it has plenty of power!</p><p>Well, you can code any computable function with it, so in theory,<br/>it can do everything.  And, it&apos;s very fast and efficient for an<br/>interpreted language.  But it only has as much power as you give<br/>it -- data types, even basic math, must all be built from simple<br/>list-processing routines.  I even had to write a procedure to do<br/>base-2 logs (I&apos;m actually using a striped-down dialect of LISP<br/>known as Scheme).</p><p>&gt;        sdev = dist / s;         // s may be .01/log(2), etc.</p><p>?  Isn&apos;t it pickedInterval * s?</p><p>&gt;        prob = exp(-sdev * sdev / 2) * intervalWidth;</p><p>What&apos;s intervalWidth?  The width of the fareyInterval?</p><p>I&apos;m not sure what&apos;s going on here, but I&apos;m to believe it&apos;s equivalent<br/>to finding the area of a standard distribution centered on pickedInterval<br/>that&apos;s sectioned off by the width of fareyInterval?</p><p>&gt;   . Sum all the &quot;raw probabilities&quot;; they should add up to 1, but of<br/>&gt;     course they won&apos;t!  Save the sum, then make a second probability<br/>&gt;     pass and calculate the &quot;true probability&quot; that the interval will<br/>&gt;     be heard as a particular just ratio.<br/>&gt;<br/>&gt;        prob2          = prob / probSum;  // sigma(prob2) = 1.0</p><p>Okay, we&apos;re finding what portion of the total probability each raw<br/>probability is.  This finishes the job I doubted the step above did?</p><p>&gt;   . In this second pass you&apos;re going to build the actual entropy value<br/>&gt;     using each of the individual probabilities, by summing:<br/>&gt;<br/>&gt;        entropy        = entropy - (prob2 * log(prob2));<br/>&gt;<br/>&gt;Note that entropy is positive because each probability is less than<br/>&gt;one and therefore prob*log(prob) is negative.</p><p>Right.  Zee familiar formula.</p><p>&gt;That&apos;s all there is to it!</p><p>Thanks dude.  Maybe I can do this in Scheme.</p><p>-Carl</p></div><h3><a id=12663 href="#12663">ðŸ”—</a>John A. deLaubenfels &#x3C;jdl@adaptune.com&#x3E;</h3><span>9/12/2000 6:35:39 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[I wrote:]<br/>&gt;&gt;(these are your 20 points, plus 1/1, in reverse order).  This is for<br/>&gt;&gt;Farey N=100, s=.01 (internally changed to .01/log(2)).</p><p>[Carl Lumma:]<br/>&gt;Cool, but I was asking for the 20 ratios from Farey order 100 with<br/>&gt;the lowest harmonic entropy, as opposed to the h.e. values for the<br/>&gt;&quot;widest&quot; 20 ratios from Farey order 100.</p><p>Oops!  Sorry, Carl.  I don&apos;t have a minimum-finding wrapper routine<br/>written yet.</p><p>[JdL:]<br/>&gt;&gt;Carl, I don&apos;t know LISP, but I&apos;m sure it has plenty of power!</p><p>[Carl:]<br/>&gt;Well, you can code any computable function with it, so in theory,<br/>&gt;it can do everything.  And, it&apos;s very fast and efficient for an<br/>&gt;interpreted language.  But it only has as much power as you give<br/>&gt;it -- data types, even basic math, must all be built from simple<br/>&gt;list-processing routines.  I even had to write a procedure to do<br/>&gt;base-2 logs (I&apos;m actually using a striped-down dialect of LISP<br/>&gt;known as Scheme).</p><p>Woah - had to write your own log function?  I&apos;m impressed!  I do happen<br/>to remember the infinite series for the exp() function, if you&apos;re<br/>interested.</p><p>[Carl:]<br/>&gt;What&apos;s intervalWidth?  The width of the fareyInterval?</p><p>Yes.</p><p>[Carl:]<br/>&gt;I&apos;m not sure what&apos;s going on here, but I&apos;m to believe it&apos;s equivalent<br/>&gt;to finding the area of a standard distribution centered on<br/>&gt;pickedInterval that&apos;s sectioned off by the width of fareyInterval?</p><p>Yes, the bell curve is centered around pickedInterval, and crosses each<br/>of the fareyInterval&apos;s.</p><p>&gt;&gt;   . Sum all the &quot;raw probabilities&quot;; they should add up to 1, but of<br/>&gt;&gt;     course they won&apos;t!  Save the sum, then make a second probability<br/>&gt;&gt;     pass and calculate the &quot;true probability&quot; that the interval will<br/>&gt;&gt;     be heard as a particular just ratio.<br/>&gt;&gt;<br/>&gt;&gt;        prob2          = prob / probSum;  // sigma(prob2) = 1.0<br/>&gt;<br/>&gt;Okay, we&apos;re finding what portion of the total probability each raw<br/>&gt;probability is.  This finishes the job I doubted the step above did?</p><p>If I&apos;m understanding your question, yes.</p><p>[Paul E:]<br/>&gt;These are pretty close to my values, except for the most consonant<br/>&gt;ratios, which are farther off (I get 2.2122 for 0.00 cents).</p><p>[JdL:]<br/>&gt;&gt;. Sum all the &quot;raw probabilities&quot;; they should add up to 1, but of<br/>&gt;&gt;  course they won&apos;t!</p><p>[Paul:]<br/>&gt;They should sum to sqrt(2*pi*s).</p><p>OK.  Glad to have that number; it provides a check for the original<br/>sum!  When I said &quot;should&quot;, I meant, &quot;should be normalized to, before<br/>entropy is calculated&quot;</p><p>[Paul:]<br/>&gt;I think I figured out why my results differ from John deLaubenfels&apos; --<br/>&gt;I&apos;m actually integrating over the bell curve from mediant to mediant<br/>&gt;(by subtracting the two corresponding values of the Error Function),<br/>&gt;while John is approximating the integral with the area of a rectangle.</p><p>That would fit the observation that the biggest discrepancies are at<br/>locations like 1/1!  OK, Paul, help me out: the &quot;Error Function&quot; is<br/>the integral of the bell curve function, exp(-x^2/2), yes?  If memory<br/>serves, that&apos;s one of those nasty function for which there is no nice<br/>expression to represent the integral, true?  So, how do I write the<br/>&quot;Error Function&quot;?  I can, of course, do it by brute force: integrate<br/>exp(-x^2/2) in tiny slices, save some of the values in a compile-time<br/>array, and interpolate.  Got any better ideas?</p><p>JdL</p></div><h3><a id=12669 href="#12669">ðŸ”—</a>John A. deLaubenfels &#x3C;jdl@adaptune.com&#x3E;</h3><span>9/12/2000 8:28:59 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[I wrote:]<br/>&gt;&gt;OK, Paul, help me out: the &quot;Error Function&quot; is the integral of the<br/>&gt;&gt;bell curve function, exp(-x^2/2), yes?  If memory serves, that&apos;s one<br/>&gt;&gt;of those nasty function for which there is no nice expression to<br/>&gt;&gt;represent the integral, true?  So, how do I write the &quot;Error<br/>&gt;&gt;Function&quot;?</p><p>[Manuel Op de Coul:]<br/>&gt;Yeah, use the approximation that I used.  I get nearly the same entropy<br/>&gt;values as Paul.</p><p>Thanks!  Do my eyes deceive me, or is that Pascal?  Haven&apos;t seen that<br/>in awhile...  But Pascal is easy to translate into C, much easier than<br/>Matlab!</p><p>JdL</p></div><h3><a id=12699 href="#12699">ðŸ”—</a>Ed Borasky &#x3C;znmeb@teleport.com&#x3E;</h3><span>9/12/2000 8:34:42 PM</span><div style='margin: 0px 20px; padding: 20px; background-color: #ddd'><b>Attachments</b></div><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&gt; -----Original Message-----<br/>&gt; From: John A. deLaubenfels [<a href="mailto:jdl@adaptune.com">mailto:jdl@adaptune.com</a>]<br/>&gt; Sent: Tuesday, September 12, 2000 6:36 AM<br/>&gt; To: <a href="mailto:tuning@egroups.com">tuning@egroups.com</a><br/>&gt; Subject: [tuning] Re: Calculating Harmonic Entropy<br/>&gt; That would fit the observation that the biggest discrepancies are at<br/>&gt; locations like 1/1!  OK, Paul, help me out: the &quot;Error Function&quot; is<br/>&gt; the integral of the bell curve function, exp(-x^2/2), yes?  If memory<br/>&gt; serves, that&apos;s one of those nasty function for which there is no nice<br/>&gt; expression to represent the integral, true?  So, how do I write the<br/>&gt; &quot;Error Function&quot;?  I can, of course, do it by brute force: integrate<br/>&gt; exp(-x^2/2) in tiny slices, save some of the values in a compile-time<br/>&gt; array, and interpolate.  Got any better ideas?</p><p>The error function (erf) and the probability integral (integral under the<br/>bell curve) are similar but not identical; I don&apos;t have my tables handy but<br/>I believe they differ only by a scaling factor. No, there is not a closed<br/>form expression for either of them. However, there are good approximations<br/>which do not require numerical integration. If you tell me which one it is<br/>you want (error function or integral under the bell curve) I will look up<br/>the approximations for you. Me, I just do it in Derive, where they&apos;re both<br/>built in :-).</p><p>Speaking of which, I&apos;ve figured out how to do Sethares&apos; contour plots of<br/>dissonance surfaces in Derive, and will post an example to the list in the<br/>near future. How do I post a &quot;JPEG&quot;??<br/>--<br/>M. Edward (Ed) Borasky<br/><a href="mailto:znmeb@teleport.com">znmeb@teleport.com</a><br/><a href="http://www.borasky-research.com/">http://www.borasky-research.com/</a></p></div><h3><a id=12700 href="#12700">ðŸ”—</a>Monz &#x3C;MONZ@JUNO.COM&#x3E;</h3><span>9/12/2000 9:48:05 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@egroups.com">tuning@egroups.com</a>, &quot;Ed Borasky&quot; &lt;znmeb@t...&gt; wrote:<br/>&gt; <a href="http://www.egroups.com/message/tuning/12699">http://www.egroups.com/message/tuning/12699</a><br/>&gt;<br/>&gt; ... I&apos;ve figured out how to do Sethares&apos; contour plots of<br/>&gt; dissonance surfaces in Derive, and will post an example to the<br/>&gt; list in the near future. How do I post a &quot;JPEG&quot;??</p><p>The best way to do this is to create a folder for your files<br/>in the tuning &apos;files&apos; section that egroups provides on the<br/>Tuning List website</p><p><a href="http://www.egroups.com/files/tuning/">http://www.egroups.com/files/tuning/</a></p><p>Click on &apos;Add folder&apos;, then name it and give a brief<br/>description.  Then click &apos;Upload file&apos; and it will<br/>open a browser dialog-box where you select the file<br/>from your hard-drive.  That&apos;s all there is to it.</p><p>Don&apos;t try to send it to the List with your posting<br/>as an attachment - most subscribers won&apos;t get it,<br/>and neither will the archives.</p><p>-monz<br/><a href="http://www.ixpres.com/interval/monzo/homepage.html">http://www.ixpres.com/interval/monzo/homepage.html</a></p></div><h3><a id=12705 href="#12705">ðŸ”—</a>John A. deLaubenfels &#x3C;jdl@adaptune.com&#x3E;</h3><span>9/13/2000 6:56:23 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[I wrote:]<br/>&gt;&gt; Do my eyes deceive me, or is that Pascal?</p><p>[Manuel Op de Coul:]<br/>&gt;Nope, its big sister Ada.</p><p>Oops!  I shoulda looked more closely!  So you have access to an actual<br/>Ada compiler, eh?</p><p>[I wrote:]<br/>&gt;&gt;OK, Paul, help me out: the &quot;Error Function&quot; is<br/>&gt;&gt;the integral of the bell curve function, exp(-x^2/2), yes?</p><p>[Paul E:]<br/>&gt;Basically, yes -- it&apos;s actually 2/sqrt(pi) * integral from 0 to x of<br/>&gt;exp(-t^2) dt.</p><p>OK, kyool...</p><p>[JdL:]<br/>&gt;&gt;If memory<br/>&gt;&gt;serves, that&apos;s one of those nasty function for which there is no nice<br/>&gt;&gt;expression to represent the integral, true?</p><p>[Paul:]<br/>&gt;Right -- it&apos;s a &quot;special function&quot;.</p><p>I hate it when that happens!  It&apos;s SO much easier to integrate, say,<br/>x^2.</p><p>[JdL:]<br/>&gt;&gt;So, how do I write the<br/>&gt;&gt;&quot;Error Function&quot;?  I can, of course, do it by brute force: integrate<br/>&gt;&gt;exp(-x^2/2) in tiny slices, save some of the values in a compile-time<br/>&gt;&gt;array, and interpolate.  Got any better ideas?</p><p>[Paul:]<br/>&gt;Matlab uses an approximation algorithm, published in &quot;Rational<br/>&gt;Chebyshev approximations for the error function&quot;  by W. J. Cody, Math.<br/>&gt;Comp., 1969, PP. 631-638. See if you can dig that up.</p><p>Well, thanks for the ref; next time I&apos;m near a good reference library<br/>I&apos;ll hafta look it up!  Meanwhile...</p><p>[Ed Borasky:]<br/>&gt;The error function (erf) and the probability integral (integral under<br/>&gt;the bell curve) are similar but not identical; I don&apos;t have my tables<br/>&gt;handy but I believe they differ only by a scaling factor. No, there is<br/>&gt;not a closed form expression for either of them. However, there are<br/>&gt;good approximations which do not require numerical integration. If you<br/>&gt;tell me which one it is you want (error function or integral under the<br/>&gt;bell curve) I will look up the approximations for you. Me, I just do it<br/>&gt;in Derive, where they&apos;re both built in :-).</p><p>If they&apos;re factors of each other, either is fine - I can figure out how<br/>to scale the input &amp; output.  Thanks!</p><p>I just love those mathematical terms, &quot;special function&quot; and &quot;no closed<br/>form expression&quot;.  Both my brothers were math majors, but, as will be<br/>obvious, I only dabble.</p><p>BTW!  Ed, I never did hear back from you on the Shostakovich Preludes<br/>and Fugues, Opus 87.  Didja like the retunings or no?  I love the music,<br/>and find it nicely retuned, but my ear is often very different from<br/>others&apos;!  You directed me to download the complete MIDI (in 12-tET)<br/>from:</p><p>   <a href="http://www.geocities.com/Vienna/5619/">http://www.geocities.com/Vienna/5619/</a></p><p>The web owner (whose name is not in front of me at the moment) did not<br/>respond to my request to post tunings, which is a shame, IMO.</p><p>JdL</p></div>