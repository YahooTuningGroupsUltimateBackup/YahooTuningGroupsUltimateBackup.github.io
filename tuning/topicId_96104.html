<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning Diatonic hearing modeled as a feature space</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning">back to list</a><h1>Diatonic hearing modeled as a feature space</h1><h3><a id=96104 href="#96104">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@...&#x3E;</h3><span>2/4/2011 4:57:45 AM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I think that there is more to &quot;diatonic&quot; hearing than 5L2s or the<br/>positions of the pitches in 12-tet.</p><p>I think that diatonic hearing is made up of a collection of different,<br/>random psychoacoustic phenomena that we have adapted to prioritize,<br/>search for, and extricate from the incoming signal. An analogous<br/>process of adaptations takes place when we process language: when you<br/>learn another language, you have to start listening for different<br/>inflections that you equated together in your old language, or else<br/>you&apos;ll have a terrible accent and not be able to understand what<br/>anyone&apos;s saying. Hence you move towards an internal coding of the<br/>signal that gets closer and closer to the actual entropy of the signal<br/>itself. And I mean entropy from an information theory standpoint, not<br/>just harmonic entropy. Whether language adaptations and music hearing<br/>adaptations are actually the same thing is neither here nor there, but<br/>I wouldn&apos;t be surprised if so.</p><p>This process has to do with pattern recognition. In a sense, we&apos;re<br/>taking the signal and mapping it into a feature space. Feature spaces<br/>are currently a hot topic in DSP, because people are all about them<br/>for building music search engines and auto-categorizing music and<br/>stuff. If you don&apos;t know what a feature space is, here&apos;s a good place<br/>to start: <a href="http://en.wikipedia.org/wiki/Features_%28pattern_recognition%29">http://en.wikipedia.org/wiki/Features_%28pattern_recognition%29</a></p><p>Probably better: <a href="http://en.wikipedia.org/wiki/Feature_extraction">http://en.wikipedia.org/wiki/Feature_extraction</a></p><p>Either way, certain other tunings will probably create lots of<br/>activity in different subspaces of a western listener&apos;s feature space<br/>right off the bat. This is to say that different subsets of the<br/>diatonic feature vectors, ones that we&apos;ve become ultra-sensitive to,<br/>might still jump out at you from other tunings. Pajara, for example,<br/>tends to excite similar features as does the diatonic scale. In<br/>Graham&apos;s Hardy piece in miracle, you might also hear some diatonic<br/>stuff. Blackwood excites the diatonic feature space quite a bit. Etc.</p><p>Now, some diatonic features activate when you listen to porcupine[7],<br/>but sometimes there&apos;s noise too - you sometimes can&apos;t tell if a note<br/>is a &quot;major third&quot; or a &quot;minor third,&quot; expressed in whatever those<br/>terms personally mean to you. Sooner or later you&apos;ll have to adapt to<br/>learn that the SNR for this feature, applied this tuning, is just a<br/>bit too low. So you&apos;ll have to either ditch the concept entirely, or<br/>improve your algorithm for figuring it out. Either way, once the<br/>problem is resolved, porcupine should suddenly become a lot more<br/>&quot;intelligible&quot; to you.</p><p>So there are two interesting things that could happen from this perspective:</p><p>The first is when a feature that correlated well to the input signal<br/>for a diatonic scale fails entirely for some other scale, e.g.<br/>porcupine. This interferes with the intelligibility of the signal, and<br/>the signal will just sound ambiguous and weird to you; further<br/>adaptation is necessary. Neutral thirds and major thirds are similar<br/>enough in size to group them together in some cases, but they&apos;re also<br/>different enough in size to distinguish them in different cases,<br/>depending on how your personalized feature extraction algorithm is<br/>running.</p><p>The second is that sometimes a feature that maps well to a diatonic<br/>scale ALSO maps well to another scale. Sometimes people tend to think<br/>that this is because they have been &quot;tainted&quot; with the curse of a<br/>lifetime of diatonic hearing, but in this perspective these things are<br/>still valid features of the signal, and meantone doesn&apos;t really have a<br/>claim on them. The feature of a &quot;leading tone resolving,&quot; for<br/>instance, can apply whether or not you&apos;re in meantone at all.</p><p>This is a very sketchy and preliminary analysis, and to be honest<br/>feature extraction is something I know more about rather than<br/>understanding in depth. But I think this paradigm might be useful for<br/>analyzing what&apos;s going on, because all of our disagreements in this<br/>regard can be represented as concrete statements about the feature<br/>space. If you believe that the only way we can adapt, after a lifetime<br/>of diatonic hearing, to understand porcupine temperament is to diverge<br/>from &quot;diatonic&quot; hearing entirely - then you have formed two isolated<br/>feature spaces and you believe that this is the only possible setup<br/>that could exist. If, you on the other hand, feel that it&apos;s possible<br/>to adapt in such a way that diatonic and porcupine hearing merge into<br/>one generalized structure of hearing, then you have now set up a<br/>higher-dimensional feature space that the other two form a subset of.</p><p>If you believe that all hearing is related to JI, and you now enjoy an<br/>ultra-fine ability to discriminate between similar intervals, and you<br/>hate something like 12-tet because it equates 81/64 and 5/4, then you<br/>have somehow set up a feature space in which 81/64 and 5/4 map to<br/>completely different features (which I don&apos;t think is possible, but<br/>that&apos;s just me), and you do not enjoy the loss of dimensionality as<br/>you start equating things again. You also feel that this is the only<br/>positive adaptation possible; e.g. that developing a feature space<br/>which could extract information out of deliberately tempered intervals<br/>is in some sense maladaptative and &quot;lossy.&quot;</p><p>If you believe that all of music derives from JI, then you believe<br/>that a lot of the features are fixed from birth and correspond to JI<br/>intervals (or at least the ones we can discern). If you believe that<br/>all of music derives from general psychoacoustics, and the HE is a<br/>more apt model, then you believe that the features are still fixed<br/>from birth, but correspond to a handful of JI intervals and an<br/>additional &quot;mistuning&quot; feature. If you believe that the latter is<br/>involved, but that the JI intervals tend to be clustered into<br/>different perceptual groups (like how the diatonic scale equates 7/6<br/>and 6/5 in terms of functionality), then you believe that a lot of the<br/>features are still fixed from birth, correspond to a handful of JI<br/>intervals and an additional &quot;mistuning&quot; feature, and that the feature<br/>vectors are dynamic and changing - different ones can be prioritized,<br/>different ones can be equated and differentiated, etc.</p><p>If you believe that all of music derives from cognitive things outside<br/>of psychoacoustics, sort of an extremist-Rothenberg perspective, then<br/>you believe that the features have more to do with how different notes<br/>match up to one another in size. Rothenberg&apos;s specific claim that an<br/>improper scale is useless for melody is equivalent to the claim that<br/>interval-width ordering cues are such a fundamental and predominant<br/>feature of the signal that it is extremely difficult to set up a<br/>feature space without it. Rothenberg&apos;s claim that subsets of improper<br/>scales can work for melody is equivalent to the claim that it is<br/>possible to play subsets of the scale such that the SNR for that<br/>feature is still set up to be low Paul&apos;s claim that he thinks that<br/>improper scales are fine for melody is equivalent to the claim that he<br/>has set up a feature space that has adapted to handle the sometimes<br/>low SNR for this vector.</p><p>The claim that MOS&apos;s are easier to grasp is equivalent to the claim<br/>that these scales are easier to perform feature extraction on, in<br/>general, due to some predominant feature that we all share. Whether or<br/>not this is inborn has to do with the mutability of the vectors.</p><p>The claim that atonal people can hear &quot;inversions&quot; of chords as being<br/>equivalent to the actual chords themselves is equal to the claim that<br/>they can attenuate periodicity feature extraction, and have built an<br/>additional &quot;symmetry&quot; feature. The claim that they&apos;re full of it and<br/>just being pretentious is equal to the claim that this feature space<br/>is somehow biologically unattainable.</p><p>The following are some possible things that could be extracted as<br/>&quot;features&quot; from a signal<br/>- That a sequence of notes fleshes out a certain background structure<br/>(e.g. that they&apos;re part of a scale)<br/>- That these scales repeat at the octave (or something else)<br/>- That a certain chord/structure has high or low tonalness<br/>- That a certain structure has high or low roughness<br/>- That a certain structure tends to be followed by another structure<br/>- That a certain structure is an &quot;upside-down&quot; version of another structure<br/>- That the roots often move by an approximate 3/2<br/>- That melodies tend to move in terms of whole and half steps in an<br/>approximate 2:1 ratio<br/>- That motion by half step tends to be used to move from a<br/>high-entropy chord to a low-entropy chord<br/>- That a certain structure makes you think of a certain color<br/>- That different structures make you think of different colors</p><p>Here are some features that we haven&apos;t seen activated much in music:<br/>- That different structures resemble different phonemes<br/>- That structures beating at certain rates are followed by structures<br/>beating at other rates<br/>- That detuned timbres are followed by perfectly tuned timbres</p><p>etc. Perhaps some people here might have more insight into this.</p><p>-MIke</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            