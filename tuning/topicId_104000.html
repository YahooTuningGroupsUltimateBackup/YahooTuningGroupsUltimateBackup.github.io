<a href="/tuning">back to list</a><h1>In-browser HE calculator</h1><h3><a id=104000 href="#104000">ðŸ”—</a>Mike Battaglia &#x3C;battaglia01@...&#x3E;</h3><span>3/4/2012 8:13:32 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Check it out:<br/><a href="http://www.mikebattagliamusic.com/HE-JS/HE.html">http://www.mikebattagliamusic.com/HE-JS/HE.html</a></p><p>This calculates Harmonic Renyi Entropy in O(nlogn) time, which I<br/>explained pretty thoroughly on XA some time ago. You&apos;ll all know what<br/>most of this is, so play around. So far it&apos;s been confirmed to work on<br/>Chrome and Firefox, and will probably not work in Internet Explorer.</p><p>There are a few things you may not know. The first may be Renyi<br/>entropy itself, given by Ha(d) for some dyad d. As a refresher, the<br/>&quot;a&quot; parameter is the thing that Renyi entropy adds to generalize<br/>Shannon entropy. For those who weren&apos;t following, the point to this is<br/>that it can be interpreted as the extent to which we believe the brain<br/>is &quot;actively guessing&quot; at ratios.</p><p>A value of a=1, which corresponds to the usual Shannon entropy, means<br/>that you&apos;re more or less content to assume that the incoming signal<br/>remains in and is perceived as being in some kind of &quot;ambiguous&quot; state<br/>that is in some meaningful sense a superposition of ratios. You might<br/>assume this means that the pitches in the dyad are &quot;noisy&quot; and<br/>fluctuate, or that phenomena from more than one ratio are occurring at<br/>the same time, or something like that. A value of a=1 means that<br/>you&apos;re not assuming the existence of any sort of intelligent process<br/>that attempts to analyze the ambiguity to guess at the actual ratio:<br/>you&apos;re simply measuring the extent of the uncertainty or ambiguity is<br/>measured by the Shannon entropy. You might interpret this as an<br/>affirmation that the signal remains perceptually in this sort of<br/>state.</p><p>A value of a=Infinity, on the other hand, means that you&apos;re more or<br/>less assuming that the incoming signal doesn&apos;t remain perceptually<br/>ambiguous but is actively trying to be &quot;resolved&quot; or &quot;guessed at,&quot; and<br/>that ratios are in a competition in which one can &quot;win.&quot; This type of<br/>entropy is called &quot;min-entropy,&quot; and is commonly used rather than<br/>Shannon entropy to measure the security of cryptographic keys, because<br/>it measures the worst-case susceptibility of a secret key to being<br/>correctly cracked by an intelligent hacker with a priori knowledge of<br/>the probability distribution. In our case, we might say it&apos;s measuring<br/>the best-case susceptibility of an ambiguous ratio to being correctly<br/>resolved by an intelligent brain with the same sort of a priori<br/>knowledge. One need not make any assumptions about the sort of<br/>algorithm that might be taking place, only that it exists as some sort<br/>of black box, and is &quot;as good as possible&quot;: min-entropy as a result<br/>measures how surprising it would be if random key/ratio also turned<br/>out to be the most common one in the distribution.</p><p>Intermediate values, such as a=2 (the &quot;collision&quot; entropy) demonstrate<br/>behavior intermediate to a=1 and a=Inf. Paul liked a=2 a lot, because<br/>it can be thought of as the probability that two random variables are<br/>equal to one another, and so can be said to measure the uncertainty in<br/>a &quot;confirmation&quot; of the incoming signal. &quot;a&quot; can thus be interpreted<br/>as some kind of &quot;activeness&quot; or &quot;adaptiveness&quot; parameter. It would be<br/>interesting to see if lots of musical training with complex ratios<br/>correlates well with an increase in a (note that for the same s, an<br/>increase in a can turn a maximum like 11/9 into a minimum).</p><p>Other than that, only thing you might not know is the &quot;normalize by<br/>Hartley entropy&quot; option. The special case a=0 is the &quot;Hartley<br/>entropy,&quot; which is like an infinitely dumb guesser; it always treats<br/>any probability distribution as a uniform distribution. This isn&apos;t all<br/>that notable by itself, but it&apos;s notable for trying to get the HE<br/>curve to converge as N increases. It&apos;s a well-known result that, in<br/>general, the Renyi entropy of order a is greater than or equal to the<br/>Renyi entropy of order b if a &lt; b. A proof of this can be found in<br/>proposition 2.4 here:<br/>ftp://ftp.inf.ethz.ch/doc/dissertations/th12187.ps.gz - and an<br/>implication of this is that the value a=0, called the &quot;Hartley<br/>entropy&quot; or sometimes &quot;max entropy,&quot; will always be the greatest Renyi<br/>entropy possible and will never be exceeded by larger values of a.</p><p>The Hartley entropy is simply log(size of sample space). Therefore, by<br/>simply looking at Ha(d)/H0(d), we can normalize the curve regardless<br/>of N. H0(d) will always be log(size of sample space), or in our case<br/>log(number of ratios in the set), so this is pretty simple.</p><p>-Mike</p></div>