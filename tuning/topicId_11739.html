<!DOCTYPE html>
            <html>
            <head>
            <meta charset="utf-8">
                <meta name="viewport"
            content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no">
                <meta http-equiv="x-ua-compatible" content="ie=edge">
                <title>Yahoo Tuning Groups Ultimate Backup tuning RE: [tuning] Concordance Ex Nihilo [tuning experiment]</title>
                <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
            </head>
            <body>
            </body>
            </html>
        <a href="/tuning">back to list</a><h1>RE: [tuning] Concordance Ex Nihilo [tuning experiment]</h1><h3><a id=11739 href="#11739">ðŸ”—</a>Paul H. Erlich &#x3C;PERLICH@ACADIAN-ASSET.COM&#x3E;</h3><span>8/23/2000 1:19:19 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Joseph Pehrson wrote,</p><p>&gt;If I&apos;m understanding the concept of harmonic entropy at all, when<br/>&gt;higher values of N are used... i.e. larger numbers for the ratios and<br/>&gt;more included pitches, there is not so much &quot;entropy,&quot; since there is<br/>&gt;more choice and the smaller values don&apos;t &quot;stand out&quot; so much as when<br/>&gt;there is less choice and N is smaller (??)</p><p>I think you have the right insight as to what is going on, though you&apos;ve got<br/>the definition of &quot;entropy&quot; upside-down (as the graph should show) -- there<br/>is more entropy when there is more choice and the simpler ratios don&apos;t<br/>&quot;stand out&quot; so much as when there is less choice and N is smaller . . .</p><p>&gt;But, it will be complete if I could kindly post some of the results<br/>&gt;as AUDIBLE .mp3 files on the little &quot;Tuning Lab&quot; site...  I&apos;m anxious<br/>&gt;to do that...</p><p>When I get a chance, I&apos;ll make these files for you . . .</p></div><h3><a id=11743 href="#11743">ðŸ”—</a>Joseph Pehrson &#x3C;pehrson@pubmedia.com&#x3E;</h3><span>8/23/2000 1:36:54 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>--- In <a href="mailto:tuning@egroups.com">tuning@egroups.com</a>, &quot;Paul H. Erlich&quot; &lt;PERLICH@A...&gt; wrote:</p><p>&gt; I think you have the right insight as to what is going on, though<br/>&gt;you&apos;ve got the definition of &quot;entropy&quot; upside-down (as the graph<br/>&gt;should show)</p><p>Whoops.  Just forgot what the term &quot;entropy&quot; meant exactly.  You can<br/>see it&apos;s been a while since I&apos;ve taken a physics class.  Hopefully,<br/>this list will make up for it a little bit!</p><p>[JP]:<br/>&gt; &gt;But, it will be complete if I could kindly post some of the results<br/>&gt; &gt;as AUDIBLE .mp3 files on the little &quot;Tuning Lab&quot; site...  I&apos;m<br/>&gt;anxious to do that...<br/>&gt;<br/>&gt; When I get a chance, I&apos;ll make these files for you . . .</p><p>GREAT!!!!!!!!!</p><p>________ ____ ___ __ _<br/>Joseph Pehrson</p></div><h3><a id=11746 href="#11746">ðŸ”—</a>jacky_ekstasis@yahoo.com</h3><span>8/23/2000 2:22:07 PM</span><button style="float: right; margin-right: 20px">toggle monospace</button><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>&quot;Joseph Pehrson&quot; &lt;pehrson@p...&gt; wrote:</p><p>&gt; &gt; I think you have the right insight as to what is going on, though<br/>&gt; &gt;you&apos;ve got the definition of &quot;entropy&quot; upside-down (as the graph<br/>&gt; &gt;should show)<br/>&gt;<br/>&gt; Whoops.  Just forgot what the term &quot;entropy&quot; meant exactly.</p><p>I too must admit that I&apos;ve been thinking the whole time we were<br/>speaking of the variety of &quot;Entropy&quot; used in Communication Theory:</p><p>&quot;A measure of the efficiency of a system (as a code or a language) in<br/>transmitting information, being equal to the logarithm of the number<br/>of different messages that can be sent by selection from the same set<br/>of symbols and thus indicating the degree of inital uncertainty that<br/>can be resolved by any one message.&quot;</p><p>Are we in fact talking about the physics variety of entropy? Guess<br/>I&apos;m a little slow on this. Paul would you be so kind as to guide me<br/>to a past post or paper that will clarify this for me and I will<br/>endeavor to follow more closely. Forgive my naivete.</p><p>Thanks,</p><p>Jacky</p></div>
                <script>
                    let monospace = false
                    $('button').on('click', function () {
                      if (monospace) {
                        $('p').css("font-family", "")
                      } else {
                        $('p').css("font-family", "monospace")
                      }
                      monospace = !monospace
                    })
                </script>
            