<a href="/tuning">back to list</a><h1>Complexity formulae</h1><h3>Dave Keenan &#x3C;d.keenan@xx.xxx.xxx&#x3E;</h3><span>3/8/1999 9:57:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[Keenan:]<br/>&gt;&gt;Certainly the next step is to get the complexity of a ratio from its<br/>&gt;&gt;numerator and denominator, but I&apos;m unclear whether to multiply them, or<br/>&gt;&gt;take the maximum value (as is normally done with odd or prime limits).</p><p>[Lumma:]<br/>&gt;In my experience their product can be very useful.  I learned this from<br/>&gt;Denny Genovese, who calls it the DF.  The idea is based on Partch&apos;s<br/>&gt;(perhaps dubious) assumption that the length of the period of the<br/>&gt;composite wave determines its consonance. ...<br/>&gt;I&apos;d like to learn more about how this approach overlaps or contradicts<br/>&gt;with the maximum value approach.</p><p>If linear complexity measures are multiplied, logarithmic ones should add.<br/>Max value can be done for both, but clearly it throws away some<br/>information. Just another point on the accuracy vs. user-friendliness<br/>tradeoff.</p><p>[Keenan:]<br/>&gt;&gt;If you multiply them you should then take the square-root (i.e. you should<br/>&gt;&gt;find the geometric mean). This is to keep them commensurate with the<br/>&gt;&gt;odd-&gt;limit or prime-limit where the max value is taken.</p><p>[Lumma:]<br/>&gt;I don&apos;t see how taking the square root hurts anything, but I don&apos;t see what<br/>&gt;it adds either.  What do you mean by &quot;commensurate&quot; with the max value<br/>&gt;approaches?</p><p>No it doesn&apos;t hurt or add, but instead of being a linear measure it becomes<br/>a quadratic one. &quot;Commensurate&quot; means literally &quot;belonging to the same<br/>system of measurement&quot;. Hertz are not comensurate with cents, but semitones<br/>or schismas are. More specifically I mean that you can directly compare two<br/>measures (metrics) with at most a constant multiplier required to bring<br/>them into some kind of agreement.</p><p>[Monzo:]<br/>&gt;Since the ratio can be represented by the<br/>&gt;same series you use in your formula (the prime<br/>&gt;factors), it seems to me that you should just<br/>&gt;use negative exponents for the factors of the<br/>&gt;denominator, and multiply all the factors together.</p><p>Sure. Good idea. Just have to take absolute values of exponents when<br/>calculating complexities.</p><p>[Wolf:]<br/>&gt;In chapter 5 of his _Divisions of the Tetrachord_ (Frog Peak 1993), John<br/>&gt;Chalmers gives an excellent summary of methods for the analysis of<br/>&gt;tetrachords which might be usefully applicable to individual intervals and<br/>&gt;systems other than tetrachords. The presentation of Klarenz Barlow&apos;s<br/>&gt;indigestability formula is, however, not so useful out of the context of<br/>&gt;Bawrlough&apos;s landmark  _Bus Journey to Parametron_ (Feedback Papers 21-23,<br/>&gt;available in the states from Frog Peak).</p><p>I&apos;m not sure what&apos;s fact and what&apos;s fiction here :-) but thanks for the<br/>formulae.</p><p>[Wolf:]<br/>&gt;(5) Euler&apos;s _gradus suavatis_ function:<br/>&gt;<br/>&gt;        (a) for prime numbers: the prime number itself<br/>&gt;<br/>&gt;        (b) for composite numbers:<br/>&gt;&#x9;&#x9;the sum of the prime factors minus one less than<br/>&gt;&#x9;&#x9;the number of factors.<br/>&gt;<br/>&gt;        (c) for ratios:<br/>&gt;&#x9;&#x9;convert a ratio to a segment of the harmonic series, then<br/>&gt;&#x9;&#x9;compute the least common multiple of the terms.</p><p>I don&apos;t understand part (c) above. What does it mean to &quot;convert a ratio to<br/>a segment of the harmonic series&quot;? I just think of the harmonic series as<br/>the series of whole numbers (positive integers).</p><p>[Keenan:]<br/>&gt;&gt; I&apos;ve switched to referring<br/>&gt;&gt; to them as &quot;prime exponent weights&quot; now.<br/>&gt;&gt; I suppose they represent something about<br/>&gt;&gt; how the human brain processes combinations<br/>&gt;&gt; of tones. The relative importance human evolution<br/>&gt;&gt; has given to the various primes. ...</p><p>[Monzo:]<br/>&gt;This is very interesting to me.  I&apos;ve already concluded<br/>&gt;regarding the study of meters that we break all meters<br/>&gt;down into simpler and simpler subdivisions, until<br/>&gt;ultimately everything can be expressed by combinations<br/>&gt;of 2s and 3s.<br/>...<br/>&gt;I&apos;m certain that our difficulty of understanding<br/>&gt;more complex meters as anything but 2s and 3s<br/>&gt;has a lot to do with &quot;The relative importance human<br/>&gt;evolution has given to the various primes&quot;.  We<br/>&gt;can comprehend 1, 2 or 3 of anything *right away*,<br/>&gt;and from my knowledge of how evolution works,<br/>&gt;speed of recognition or comprehension ranks<br/>&gt;near the top of the list of importance.</p><p>I wish to make it clear that I don&apos;t expect evolution to have given the<br/>same relative weights to the primes in relation to ryhthm (or cake cutting)<br/>as it did in regard to frequency ratios.</p><p>[Monzo:]<br/>&gt;With your original approximated weights<br/>&gt;(0.3, 0.8, 0.9, 1.0, 1.0, ...), which I thought were<br/>&gt;OK at first, it&apos;s evident that there is a sharp<br/>&gt;increase in complexity after 2.  Perhaps with<br/>&gt;a different weighting your formula is a mathematical<br/>&gt;validation and explanation of my idea.</p><p>Since you are free to choose the weights to fit your own judgement it can&apos;t<br/>be a validation or explanation of that judgement, merely a description of<br/>it. But tell us what weights you favour and we can compare and maybe home<br/>in on a rough agreement.</p><p>Please use the generalised/parameterised Barlow-type formula I mentioned in<br/>my previous post to this thread (i.e. multiply weights by absolute<br/>exponents then add them up), or at least make it clear which version the<br/>weights are for (including whether log or linear).</p><p>Actually, this could more simply be called a parameterised Wilson&apos;s<br/>harmonic complexity (rather than &quot;the absolute value of the reciprocal of a<br/>parameterised Barlow&apos;s harmonicity&quot;). Wilson&apos;s choice of weights<br/>corresponds to the primes themselves, except for 2 whose weight is zero.</p><p>&gt;&gt; Give me a better term. [Keenan]<br/>&gt;<br/>&gt;&quot;Prime importance&quot;? [Monzo]</p><p>&quot;Prime importance&quot; could be used instead of &quot;prime exponent weight&quot;, but I<br/>don&apos;t think it could be used instead of &quot;harmonic complexity&quot; (of a ratio).<br/>I now favour &quot;harmonic complexity&quot; over &quot;musical complexity&quot;. It reminds us<br/>it only works for (near) harmonic timbres and &quot;musical&quot; was too broad since<br/>it could apply to rhythm as well. (Thanks for mentioning rhythm).</p><p>[Monzo:]<br/>&gt;I gave it a weight of as low as .05 before it looked<br/>&gt;like something I agreed with, but the weight of 3<br/>&gt;was much lower also - I don&apos;t remember what now.<br/>&gt;<br/>&gt;As I said above, my inclination would be to have<br/>&gt;low weights for 2 and 3 and then increase sharply<br/>&gt;after that.  What do you think?</p><p>I might well agree with such a weighting. Try me.</p><p>[Monzo:]<br/>&gt;How about the<br/>&gt;question of 15? I&apos;ve always thought that 15/8<br/>&gt;is a pretty consonance in a chord - is it more,<br/>&gt;or less, consonant than 7/4?</p><p>Ah but we&apos;re not (yet) talking about &quot;in a chord&quot;. I agree with Paul Erlich<br/>that the 15/8 just happens to appear in a chord with other highly consonant<br/>intervals, but anyway, on its own I think 15/8 is more dissonant than 7/4.</p><p>&gt;[Monzo (replying to Erlich):]<br/>&gt;OK, Paul, you got me there.  So apparently the<br/>&gt;effect of octave-equivalence has more to do with<br/>&gt;prime-base 2 have an extremely low weight<br/>&gt;in Keenan&apos;s &quot;musical complexity&quot; prime-weights<br/>&gt;formula than anything else.  But to account<br/>&gt;for the fact of octave-equivalency when no<br/>&gt;other prime equivalency occurs anywhere near<br/>&gt;as strongly, the curve of the weights must be<br/>&gt;very low at 2 then rise sharply for 3 and above.</p><p>I agree. You might say that the second harmonic so often accompanied a<br/>fundamental that it carried very little additional information. It has very<br/>low importance. But why *prime* importance. Why is 4 (and 8 and 16) also so<br/>unimportant? Maybe it was just an accident of economical &quot;wiring&quot; that<br/>treats four as a power of two. Or maybe it is just that the 4th harmonic is<br/>just treated as the 2nd harmonic of the 2nd harmonic.</p><p>[Wolf:]<br/>&gt;Although I cannot invite the list to dinner in Cologne, I propose that we<br/>&gt;duplicate the experiment. Since most of us on the list are into higher<br/>&gt;primes, let&apos;s try all divisions through 19.  Please send me, off list ,<br/>&gt;your own ranking of the difficulties in slicing tortes (cakes, pies) from 2<br/>&gt;to 19 equal parts.  I&apos;ll compile the results and report back.</p><p>As I said, I don&apos;t think one can draw any musical conclusions from this,<br/>but it sounds like fun and will be interesting to compare.</p><p>[Morrison:]<br/>&gt;   Am I correct in inferring that, by how much they &quot;cost&quot;, you mean how<br/>much taking it out affects the high-level harmonic character?</p><p>Yes. Or equivalently how much we over or underestimate the dissonance of an<br/>interval by ignoring *how many* factors of 2 (or 3 etc.) it has.</p><p>Regards,</p><p>-- Dave Keenan<br/><a href="http://uq.net.au/~zzdkeena">http://uq.net.au/~zzdkeena</a></p></div><h3>Daniel Wolf &#x3C;DJWOLF_MATERIAL@xxxxxxxxxx.xxxx&#x3E;</h3><span>3/9/1999 1:40:12 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Message text written by Dave Keenan<br/>&gt;<br/>I&apos;m not sure what&apos;s fact and what&apos;s fiction here :-) but thanks for the<br/>formulae.&lt;</p><p>In case you&apos;re wondering, the spelling of Barlow&apos;s name is highly variable.<br/>Although he has lived in Europe for the past 30 years, he comes from the<br/>Anglophone minority community in Calcutta, and I suspect that he intends<br/>this as a humorous slant on both Indian English orthography and the<br/>decaying remains of colonialism. To the best of my knowlege, everthing in<br/>the passage you cited was true.</p></div><h3>Dave Keenan &#x3C;d.keenan@xx.xxx.xxx&#x3E;</h3><span>3/10/1999 11:41:08 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[Erlich:]<br/>&gt;P.S. Dave: I don&apos;t like it if 15/8 is given the same complexity as 6/5,<br/>&gt;assuming factors of 2 are ignored.</p><p>No. Nor do I. But first notice that I&apos;m specifically saying that factors of<br/>2 should *not* be ignored, just given a low weight. But no matter what the<br/>weighting of 2, 15/4 would have the same complexity as 12/5. I certainly<br/>don&apos;t thing they have the same dissonance (15/4 is greater). So maybe<br/>you&apos;ve just blown away all formulae where we ignore whether various factors<br/>are on the same or different sides of the vinculum (the line between<br/>numerator and denominator). If so, good work.</p><p>Maybe the tolerance function can take care of it. But it seems unlikely.<br/>Can you establish that it won&apos;t?</p><p>More cases which would have the same complexity under any such scheme (same<br/>complexity on the same line).<br/> 6/1   3/2<br/>10/1   5/2<br/>12/1   4/3<br/>14/1   7/2<br/>15/1   5/3 *<br/>18/1   9/2<br/>20/1   5/4<br/>21/1   7/3 *<br/>22/1  11/2<br/>24/1   8/3<br/>26/1  13/2<br/>28/1   7/4<br/>30/1  15/2  10/3   6/5 *<br/>33/1  11/3 *<br/>34/1  17/2<br/>35/1   7/5 *<br/>36/1   9/4<br/>38/1  19/2<br/>39/1  13/3<br/>40/1   8/5<br/>42/1  21/2  14/3   7/6 *<br/>44/1  11/4<br/>45/1   9/5 *<br/>46/1  23/2<br/>48/1  16/3<br/>50/1  25/2<br/>51/1  17/3<br/>52/1  13/4<br/>54/1  27/2<br/>55/1  11/5 *<br/>56/1   8/7 *<br/>58/1  29/2<br/>60/1  15/4  20/3  12/5 *<br/>:<br/>120/1 15/8  40/3  24/5 *</p><p>Those where only powers-of-two change sides, are not too objectionable. Nor<br/>are those that involve primes higher than 11, since the tolerance function<br/>should take care of them. I&apos;ve flagged the remaining ones with *.</p><p>Surely oo/1 is as consonant as 1/1. (&quot;oo&quot; is meant to look like the lazy-8<br/>infinity symbol). I&apos;d even say n/1, where n&gt;16, are almost as consonant as<br/>1/1. Can we arrange for the tolerance function (e.g. Harmonic Entropy) to<br/>give that result?</p><p>But it doesn&apos;t blow away all formulae based on separate prime<br/>factorisations of numerator and denominator. There must be ways of<br/>combining them that avoid this problem. So Paul (or anyone), how might we<br/>modify odd-limit to include some consideration of 2&apos;s? e.g. so 6/5 is not<br/>forced to have the same complexity as 5/3 (or 3/2 same as 4/3). (I think<br/>Dan Wolf asked you the same question).</p><p>Do we agree that the superparticular series 2/1, 3/2, 4/3, 5/4, 6/5, 7/6,<br/>8/7, 9/8, 10/9, 11/10 ... is progressively more dissonant until it gets<br/>close enough to 1/1 for the tolerance function to take over? If someone<br/>were to ask me &quot;What is dissonance?&quot;. I don&apos;t think I could do better than<br/>to say &quot;It is that quality of the sound (independent of pitch and loudness)<br/>which you hear increasing as you go up this series of just intervals<br/>(assuming a near harmonic timbre)&quot;.</p><p>Here&apos;s are more series of intervals which IMHO increase in dissonance<br/>(until getting too near some low complexity interval). Since the tolerance<br/>function can take care of the &quot;until too near ...&quot; bit, these can be taken<br/>as unlimited series of increasing *complexity*. I&apos;ve arbitrarily stayed<br/>within 2 octaves.</p><p>2/1, 3/2, 4/3, 5/4, 6/5, 7/6, 8/7, 9/8, 10/9, 11/10 ... until too near 1/1<br/>3/2, 5/3, 7/4, 9/5, 11/6, 13/7, 15/8 ... until too near 2/1<br/>3/1, 5/2, 7/3, 9/4, 11/5, 13/6, 15/7 ... until too near 2/1<br/>2/1, 5/2, 8/3, 11/4, 13/5, ... until too near 3/1<br/>4/1, 7/2, 10/3, 13/4, 14/5, ... until too near 3/1<br/>3/1, 7/2, 11/3, 15/4, ... until too near 4/1<br/>1/1, 4/3, 7/5, 10/7, 13/8, ... until too near 3/2<br/>2/1, 5/3, 8/5, 11/7, 14/9, ... until too near 3/2<br/>2/1, 7/3, 12/5, ... until too near 5/2<br/>3/1, 8/3, 13/5, ... until too near 5/2<br/>3/2, 7/5, 11/8, ... until too near 4/3<br/>1/1, 5/4, 9/7, ... until too near 4/3<br/>3/2, 8/5, 13/8, ... until too near 5/3<br/>2/1, 7/4, 12/7, ... until too near 5/3<br/>4/3, 9/7, ... until too near 5/4<br/>1/1, 6/5, 11/9, ... until too near 5/4<br/>3/1, 10/3, ... until too near 7/2<br/>4/1, 11/3, ... until too near 7/2</p><p>Anyone disagree with any of these? Next we need to decide how they interleave.</p><p>[Rosati:]<br/>&gt; During a discussion with Paul Erlich awhile back, I mentioned that 13/8<br/>&gt; sounded more consonant to me than 11/8. I attributed this to 11/8 falling in<br/>&gt; the tritone &quot;hump&quot;, while 13/8 sits between 5/3 and 8/5. After Paul gave us<br/>&gt; David Canright&apos;s new web address I was reading his &quot;Tour up the Harmonic<br/>&gt; Series&quot; (<a href="http://www.mbay.net/~anne/david/harmser/index.htm">http://www.mbay.net/~anne/david/harmser/index.htm</a>) and noticed that<br/>&gt; he also hears 13/8 as more consonant than 11/8. So, I was wondering how<br/>&gt; other distinguished ears in this forum might weigh in on this and what<br/>&gt; implications it might have for theories of odd-limit (or prime limit, for<br/>&gt; that matter) consonance indexing.</p><p>I don&apos;t think it has any implications for any *complexity* measure, since<br/>11/8 and 13/8 are in regions where the lower complexity of notes either<br/>side will dominate, i.e. this result will be taken care of by the tolerance<br/>or blurring function. I agree totally with what you said. The dissonance<br/>hump between 4/3 and 7/5 (where 11/8 is) is higher than the one between 8/5<br/>and 5/3 (where 13/8 is).</p><p>[Keenan:]<br/>&gt;&gt;The second function has been called tolerance. This is some kind of<br/>&gt;&gt;blurring function where the dissonance of a complex ratio will depend more<br/>&gt;&gt;on its proximity to nearby simpler ratios, thus limiting the significance<br/>&gt;&gt;of higher primes.</p><p>[Erlich replied:]<br/>&gt;Have you noticed that this argument is far more valid if you replace<br/>&gt;&quot;primes&quot; with &quot;odds&quot; at the end of the sentence,</p><p>Actually, I&apos;d prefer to just say &quot;limiting the significance of higher<br/>*numbers*&quot;, whether they be prime, odd or otherwise.</p><p>[Erlich:]<br/>&gt;showing that a strict<br/>&gt;lattice approach like all those we&apos;ve been discussing is much more<br/>&gt;likely to be meaningful if we restrict ourselves to, say, the<br/>&gt;11-odd-limit, than if we restrict ourselves to the 11-prime-limit, the<br/>&gt;7-prime-limit, the 5-prime-limit, or even the 3-prime-limit?</p><p>I didn&apos;t think we needed to limit the complexity measure in this way<br/>because the tolerance function should take care of it in a less arbitrary<br/>way. Isn&apos;t what-you-are-talking-about a more user-friendly but less<br/>accurate way of modelling tolerance? Certainly a useful point in that<br/>tradeoff, and I agree odd-limit is much better than prime-limit for this<br/>purpose of &quot;limiting the limit&quot;.</p><p>[Erlich:]<br/>&gt;The 11-odd-limit seems about right given ideal conditions for pitch<br/>&gt;discrimination.</p><p>I agree. And before anyone jumps on us, remember we&apos;re talking about bare<br/>intervals (not in chords).</p><p>[Erlich:]<br/>&gt;And again, a triangular lattice would be more appropriate.</p><p>So what does the corresponding equation (or algorithm) look like, to get<br/>complexity from n/d, without ignoring factors of 2.</p><p>I&apos;m convinced that n+d isn&apos;t a good enough complexity measure *in chords*,<br/>but remind me what is wrong with it for intervals?</p><p>Regards,<br/>-- Dave Keenan<br/><a href="http://uq.net.au/~zzdkeena">http://uq.net.au/~zzdkeena</a></p></div><h3>Paul H. Erlich &#x3C;PErlich@xxxxxxxxxxxxx.xxxx&#x3E;</h3><span>3/11/1999 3:45:42 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Dave Keenan wrote,</p><p>&gt;So maybe<br/>&gt;you&apos;ve just blown away all formulae where we ignore whether various<br/>factors<br/>&gt;are on the same or different sides of the vinculum (the line between<br/>&gt;numerator and denominator). If so, good work.</p><p>Thanks. I like to blow things away.</p><p>&gt;Surely oo/1 is as consonant as 1/1. (&quot;oo&quot; is meant to look like the<br/>lazy-8<br/>&gt;infinity symbol). I&apos;d even say n/1, where n&gt;16, are almost as consonant<br/>as<br/>&gt;1/1. Can we arrange for the tolerance function (e.g. Harmonic Entropy)<br/>to<br/>&gt;give that result?</p><p>Actually, Harmonic Entropy already works that way! Some time ago I<br/>posted an old derivation of mine that the certainty with which an<br/>interval is perceived as a just ratio is inversely proportional to the<br/>DENOMINATOR of the ratio (when ratios are expressed with a larger<br/>numerator than denominator). This derivation is a corrected version of<br/>one originally done by Van Eck and is based on the exact same model I&apos;ve<br/>used for all the harmonic entropy work I&apos;ve described so far (including<br/>some graphs I just set to Joe Monzo). Now notice that the odd limit is<br/>an upper bound for the denominator, and you&apos;ll see why I find the odd<br/>limit useful even in octave-specific situations!</p><p>&gt;But it doesn&apos;t blow away all formulae based on separate prime<br/>&gt;factorisations of numerator and denominator. There must be ways of<br/>&gt;combining them that avoid this problem. So Paul (or anyone), how might<br/>we<br/>&gt;modify odd-limit to include some consideration of 2&apos;s? e.g. so 6/5 is<br/>not<br/>&gt;forced to have the same complexity as 5/3 (or 3/2 same as 4/3). (I<br/>think<br/>&gt;Dan Wolf asked you the same question).</p><p>Use just the denominator. That&apos;s based on the derivation I described<br/>above. One assumption in that derivation is that the pitch of the upper<br/>note is fixed when comparing different intervals. If instead you fix the<br/>center of the interval, I think my derivation can be modified to show<br/>that the certainty is inversely proportional to the sum of the numerator<br/>and the denominator. According to you, that sum models critical-band<br/>roughness very well up to ratios of 17, so it seems _both_ components of<br/>dissonance can be modeled by the sum of the numerator and the<br/>denominator!</p><p>&gt;&gt;showing that a strict<br/>&gt;&gt;lattice approach like all those we&apos;ve been discussing is much more<br/>&gt;&gt;likely to be meaningful if we restrict ourselves to, say, the<br/>&gt;&gt;11-odd-limit, than if we restrict ourselves to the 11-prime-limit, the<br/>&gt;&gt;7-prime-limit, the 5-prime-limit, or even the 3-prime-limit?</p><p>&gt;I didn&apos;t think we needed to limit the complexity measure in this way<br/>&gt;because the tolerance function should take care of it in a less<br/>arbitrary<br/>&gt;way. Isn&apos;t what-you-are-talking-about a more user-friendly but less<br/>&gt;accurate way of modelling tolerance?</p><p>Yes. I have a more explicit provision for tolerance in this context<br/>which I&apos;ll dig up soon.</p><p>&gt;Certainly a useful point in that<br/>&gt;tradeoff, and I agree odd-limit is much better than prime-limit for<br/>this<br/>&gt;purpose of &quot;limiting the limit&quot;.</p><p>Yup.</p><p>&gt;&gt;And again, a triangular lattice would be more appropriate.</p><p>&gt;So what does the corresponding equation (or algorithm) look like, to<br/>get<br/>&gt;complexity from n/d, without ignoring factors of 2.</p><p>If you mean in a lattice context, I haven&apos;t thought about<br/>octave-specific lattices. As for the usual octave-invariant lattice, it<br/>appears Paul Hahn found an error in his original algorithm, but I&apos;m sure<br/>if anyone can correct it, he can.</p><p>&gt;I&apos;m convinced that n+d isn&apos;t a good enough complexity measure *in<br/>chords*,<br/>&gt;but remind me what is wrong with it for intervals?</p><p>Nothing!</p></div><h3>Dave Keenan &#x3C;d.keenan@xx.xxx.xxx&#x3E;</h3><span>3/11/1999 10:12:15 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>[Paul Erlich:]<br/>&gt;Re-reading Dave&apos;s e-mail to me, I see that the sum [of numerator and<br/>&gt;denominator] only models<br/>&gt;critical-band roughness well if the _sum_ is 17 or less. So intervals<br/>&gt;like 11/8 and 13/8 are already too complex for this type of formula, as<br/>&gt;they are too close to other ratios of similar complexity to be clearly<br/>&gt;distinguished from them.</p><p>Agreed.</p><p>&gt;Note that my harmonic entropy model, to the<br/>&gt;accuracy that I&apos;ve evaluated it so far, did not show local minima at<br/>&gt;11/8 or 13/8, but there was a very tiny one at 11/6.</p><p>Neato!</p><p>I&apos;d be interested to see your harmonic entropy applied to, not a Farey<br/>series but, all ratios n/d, where n&gt;d, n+d &lt;= N, where N is large, say 40<br/>or 80. Will this still predict low dissonance for n/1 where n&gt;=16?</p><p>Regards,<br/>-- Dave Keenan<br/><a href="http://uq.net.au/~zzdkeena">http://uq.net.au/~zzdkeena</a></p></div><h3>Paul Hahn &#x3C;Paul-Hahn@xxxxxxx.xxxxx.xxxx&#x3E;</h3><span>3/12/1999 8:30:47 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Thu, 11 Mar 1999, Paul H. Erlich wrote:<br/>&gt;                           As for the usual octave-invariant lattice, it<br/>&gt; appears Paul Hahn found an error in his original algorithm, but I&apos;m sure<br/>&gt; if anyone can correct it, he can.</p><p>I&apos;m touched by your faith in me.  In any case, I have in fact found a<br/>simpler algorithm, though I think Manuel has beaten me to it already,<br/>since it sounds like he has already coded it up.  Nevertheless, here it<br/>is, for the curious:</p><p>Given a Fokker-style interval vector (I1, I2, . . . In):</p><p>1.  Go to the rightmost nonzero exponent; add the product of its<br/>absolute value with the log of its base to the total.</p><p>2.  Use that exponent to cancel out as many exponents of the opposite<br/>sign as possible, starting to its immediate left and working right;<br/>discard anything remaining of that exponent.</p><p>&#x9;Example: starting with, say, (4 2 -3), we would add 3 lg(7) to<br/>&#x9;our total, then cancel the -3 against the 2, then the remaining<br/>        -1 against the 4, leaving (3 0 0).  OTOH, starting with<br/>&#x9;(-2 3 5), we would add 5 lg(7) to our total, then cancel 2 of<br/>&#x9;the 5 against the -2 and discard the remainder, leaving (0 3 0).</p><p>3.  If any nonzero exponents remain, go back to step one, otherwise<br/>stop.</p><p>To illustrate on the list of intervals Manuel culled from an earlier<br/>post of mine:</p><p>225:224&#x9;(2 2 -1):<br/>&#x9;1st iteration&#x9;lg(7);&#x9;&#x9;&#x9;(2 1 0)<br/>&#x9;2nd iteration&#x9;lg(7) + lg(5);&#x9;&#x9;(2 0 0)<br/>&#x9;3rd iteration&#x9;lg(7) + lg(5) + 2 lg(3);(0 0 0) Done.</p><p>126:125&#x9;(2 -3 1):<br/>&#x9;1st iteration&#x9;lg(7);&#x9;&#x9;&#x9;(2 -2 0)<br/>&#x9;2nd iteration&#x9;lg(7) + 2 lg(5);&#x9;(0  0 0) Done.</p><p>128:125 (0 -3):<br/>&#x9;1st iteration&#x9;3 lg(5);&#x9;&#x9;(0 0) Done.</p><p>81:80&#x9;(4 -1):<br/>&#x9;1st iteration&#x9;lg(5);&#x9;&#x9;&#x9;(3 0)<br/>&#x9;2nd iteration&#x9;lg(5) + 3 lg(3);&#x9;(0 0) Done.</p><p>64:63&#x9;(-2 0 -1):<br/>&#x9;1st iteration&#x9;lg(7);&#x9;&#x9;&#x9;(-2 0 0)<br/>&#x9;2nd iteration&#x9;lg(7) + 2 lg(3);&#x9;( 0 0 0) Done.</p><p>50:49&#x9;(0 2 -2):<br/>&#x9;1st iteration&#x9;2 lg(7);&#x9;&#x9;(0 0 0) Done.</p><p>--pH &lt;<a href="mailto:manynote@lib-rary.wustl.edu">manynote@lib-rary.wustl.edu</a>&gt; <a href="http://library.wustl.edu/~manynote">http://library.wustl.edu/~manynote</a><br/>    O<br/>   /\        &quot;How about that?  The guy can&apos;t run six balls,<br/>  -\-\-- o    and they make him president.&quot;</p><p>             NOTE: dehyphenate node to remove spamblock.          &lt;*&gt;</p></div><h3>Dave Keenan &#x3C;d.keenan@xx.xxx.xxx&#x3E;</h3><span>3/12/1999 8:40:45 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>Dear tuning folk,</p><p>As promised, I&apos;ve put up a spreadsheet with a chart comparing 10 different<br/>complexity measures for all ratios n/d up to n+d=21.</p><p><a href="http://uq.net.au/~zzdkeena/Music/HarmonicComplexity.xls">http://uq.net.au/~zzdkeena/Music/HarmonicComplexity.xls</a> 174kB</p><p>Unfortunately the complexity comparison is not complete. I need a third<br/>opinion on how Euler&apos;s totient function and gradus suavatis apply to<br/>ratios. Also, for whole numbers, is the totient function equal to the prime<br/>(or one less) in the case of primes?</p><p>I&apos;ll also take requests to include other dyadic complexity measures.</p><p>Remember, complexity alone is not dissonance. You need tolerance when<br/>complexity is high. :-)</p><p>I&apos;ve also put up my spreadsheet implementation of Sethares&apos; dissonance<br/>curve (from timbre) algorithm. Thanks Bill. Unfortunately the number of<br/>points it can plot is limited to 28 so you have to zoom in on areas of<br/>interest by changing the start point and reducing the step size.</p><p><a href="http://uq.net.au/~zzdkeena/Music/SetharesDissonance.xls">http://uq.net.au/~zzdkeena/Music/SetharesDissonance.xls</a> 1.5MB</p><p>Regards,<br/>-- Dave Keenan<br/><a href="http://uq.net.au/~zzdkeena">http://uq.net.au/~zzdkeena</a></p></div><h3>Paul H. Erlich &#x3C;PErlich@xxxxxxxxxxxxx.xxxx&#x3E;</h3><span>3/12/1999 3:23:56 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I&apos;ve thought about it and I don&apos;t agree with the algorithm Manuel and<br/>Paul Hahn have come up with for computing the city-block metric on the<br/>triangular lattice. The problem comes down, as usual, to an<br/>over-reliance on primes. For instance, the algorithm makes 11:10 a<br/>single step of length lg(11), while 9:5 is a step of length lg(3) plus a<br/>step of lg(5), which is longer. But if you have an 11-axis in the<br/>lattice, then that means you&apos;re considering 11-limit intervals<br/>consonant, so you should also consider 9-limit intervals consonant, and<br/>you should have a 9-axis too. That would make 9:5 a single step of<br/>length lg(9).</p><p>Clearly in this lattice 9:5 occurs in two different places. But, Paul<br/>Hahn himself wrote,</p><p>&gt;If you&apos;re an odd-limit proponent such as myself, things get a little<br/>&gt;complicated at the 9-limit and above, since 9 is composite, and<br/>&gt;odd-factorization does not necessarily yield unique results.  However,<br/>&gt;the minimum complexity is achieved by assigning the 9 exponent as large<br/>&gt;as possible, and the 3 exponent 0 or 1 as appropriate.</p><p>So it appears he agreed with me to begin with, then renegged (or<br/>forgot). Another way to think about it is that the lattice with prime<br/>axes is the basic construct, so every pitch appears only once, but then<br/>you have &quot;wormholes&quot; with shorter than the apparent length for intervals<br/>like 9:5.</p><p>The correct algorithm is of course much simpler than the last one Paul<br/>H. described. It is: remove all factors of two, then take the log of the<br/>denominator or the numerator, whichever is larger.</p></div><h3>Paul Hahn &#x3C;Paul-Hahn@xxxxxxx.xxxxx.xxxx&#x3E;</h3><span>3/13/1999 8:19:13 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Fri, 12 Mar 1999, Paul H. Erlich wrote:<br/>&gt; I&apos;ve thought about it and I don&apos;t agree with the algorithm Manuel and<br/>&gt; Paul Hahn have come up with for computing the city-block metric on the<br/>&gt; triangular lattice. [snip]<br/>&gt; So it appears he agreed with me to begin with, then renegged (or<br/>&gt; forgot).</p><p>I do wish you wouldn&apos;t use such emotionally charged words, especially in<br/>view of the fact that I have neither &quot;reneged&quot; nor forgotten.<br/>Everything I&apos;ve written on this subject so far works using vectors<br/>including separate exponents for the odd composites, provided you<br/>reduce them as I described in an earlier article.</p><p>&gt; The correct algorithm is of course much simpler than the last one Paul<br/>&gt; H. described. It is: remove all factors of two, then take the log of the<br/>&gt; denominator or the numerator, whichever is larger.</p><p>No, it isn&apos;t.  Consider 225:224, for example: your way (and with my<br/>uncorrected algorithm for the weighted version) the complexity is<br/>2 lg(3) + 2 lg(5), when in fact the correct version, if you work it out<br/>on a lattice or use my corrected algorithm, is 2 lg(3) + lg(5) + lg(7).</p><p>--pH &lt;<a href="mailto:manynote@lib-rary.wustl.edu">manynote@lib-rary.wustl.edu</a>&gt; <a href="http://library.wustl.edu/~manynote">http://library.wustl.edu/~manynote</a><br/>    O<br/>   /\        &quot;How about that?  The guy can&apos;t run six balls,<br/>  -\-\-- o    and they make him president.&quot;</p><p>             NOTE: dehyphenate node to remove spamblock.          &lt;*&gt;</p></div><h3>Paul H. Erlich &#x3C;PErlich@xxxxxxxxxxxxx.xxxx&#x3E;</h3><span>3/16/1999 12:21:19 PM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>I wrote,</p><p>&gt;&gt; I&apos;ve thought about it and I don&apos;t agree with the algorithm Manuel and<br/>&gt;&gt; Paul Hahn have come up with for computing the city-block metric on<br/>the<br/>&gt;&gt; triangular lattice. [snip]<br/>&gt;&gt; So it appears he agreed with me to begin with, then renegged (or<br/>&gt;&gt; forgot).</p><p>&gt;I do wish you wouldn&apos;t use such emotionally charged words, especially<br/>in<br/>&gt;view of the fact that I have neither &quot;reneged&quot; nor forgotten.<br/>&gt;Everything I&apos;ve written on this subject so far works using vectors<br/>&gt;including separate exponents for the odd composites, provided you<br/>&gt;reduce them as I described in an earlier article.</p><p>I didn&apos;t think there was any emotion there. But there does appear to be<br/>a conflict between your/Manuel&apos;s algorithm and using all odd composites,<br/>as my 9:5 vs. 11:5 example pointed out.</p><p>&gt;&gt; The correct algorithm is of course much simpler than the last one<br/>Paul<br/>&gt;&gt; H. described. It is: remove all factors of two, then take the log of<br/>the<br/>&gt;&gt; denominator or the numerator, whichever is larger.</p><p>&gt;No, it isn&apos;t.  Consider 225:224, for example: your way (and with my<br/>&gt;uncorrected algorithm for the weighted version) the complexity is<br/>&gt;2 lg(3) + 2 lg(5), when in fact the correct version, if you work it out<br/>&gt;on a lattice or use my corrected algorithm, is 2 lg(3) + lg(5) + lg(7).</p><p>I stand by my way and would rather you addressed my examples of 9:5 and<br/>11:5 instead of 225:224 (for which the lattice evaluation is unlikely to<br/>be very meaningful anyway). If it must be discussed, my value for<br/>225:224 assumes there is a 225-axis or 225-wormholes in the lattice. It<br/>might make more sense to pick an odd limit for the lattice like 7, 9, or<br/>11, but then you couldn&apos;t evaluate anything with 13 in it. Perhaps a<br/>separate complexity measure for several choices of odd limit would<br/>satisfy us both (but complicate the matter greatly).</p></div><h3>Paul Hahn &#x3C;Paul-Hahn@xxxxxxx.xxxxx.xxxx&#x3E;</h3><span>3/17/1999 8:34:11 AM</span><div style='margin: 0px 20px 20px; padding: 20px; background-color: #eee'><p>On Tue, 16 Mar 1999, Paul H. Erlich wrote:<br/>&gt;&gt; I do wish you wouldn&apos;t use such emotionally charged words, especially in<br/>&gt;&gt; view of the fact that I have neither &quot;reneged&quot; nor forgotten.<br/>&gt;&gt; Everything I&apos;ve written on this subject so far works using vectors<br/>&gt;&gt; including separate exponents for the odd composites, provided you<br/>&gt;&gt; reduce them as I described in an earlier article.<br/>&gt;<br/>&gt; I didn&apos;t think there was any emotion there.</p><p>You don&apos;t think that someone might show a bit of heat at being (wrongly)<br/>accused of having reneged on something?  Aside from that, I find it a<br/>bit presumptuous that sufficient commitment was assumed for the word<br/>&quot;renege&quot; to be used in the first place.</p><p>&gt;                                             But there does appear to be<br/>&gt; a conflict between your/Manuel&apos;s algorithm and using all odd composites,<br/>&gt; as my 9:5 vs. 11:5 example pointed out.</p><p>No, there isn&apos;t.  (Sheesh!  I don&apos;t remember the last time I&apos;ve had to<br/>argue so hard to convince someone I agreed with him.  Though I have a<br/>feeling that it was Paul E. that time, too. 8-)&gt; )</p><p>Look.  Let&apos;s look at what I said about using the algorithms in an<br/>odd-limit vs. prime-limit environment, _which you (Paul E.) yourself<br/>quoted_:</p><p>| If you&apos;re an odd-limit proponent such as myself, things get a little<br/>| complicated at the 9-limit and above, since 9 is composite, and<br/>| odd-factorization does not necessarily yield unique results.  However,<br/>                                                                ^^^^^^^^<br/>| the minimum complexity is achieved by assigning the 9 exponent as large<br/>  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br/>| as possible, and the 3 exponent 0 or 1 as appropriate.<br/>  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p><p>Okay.  The 11-limit prime vectors for 9:5 and 11:5 are (2 -1 0 0) and<br/>(0 -1 0 1).  Converting these to the optimal 11-limit odd-factor vectors<br/>as described above, we get (0 -1 0 1 0) for 9:5 and (0 -1 0 0 1) for<br/>11:5.</p><p>Now apply the algorithms:</p><p>Simple version:</p><p>&#x9;In each interval, the absolute values of the sums of the<br/>&#x9;positive and negative exponents are 1.  Hence, each of these are<br/>&#x9;primary/consonant intervals within the 11-limit.</p><p>Weighted version:</p><p>&#x9;(0 -1 0 1 0):<br/>&#x9;1st iteration&#x9;lg(9)&#x9;&#x9;(0 0 0 0 0) done</p><p>&#x9;(0 -1 0 0 1):<br/>&#x9;1st iteration&#x9;lg(11)&#x9;&#x9;(0 0 0 0 0) done</p><p>See?  It works.</p><p>Incidentally, since you don&apos;t like the 225:224 example, let&apos;s just<br/>consider the 9:5 within the traditional 5-limit (vector [2 -1]).  With<br/>your largest-odd-factor method, we get lg(9).  However, the shortest<br/>path through the 5-limit lattice for 9:5 is a 3:2 and a 6:5, or<br/>lg(3) + lg(5).  Eh?</p><p>&gt;&gt;&gt; The correct algorithm is of course much simpler than the last one Paul<br/>&gt;&gt;&gt; H. described. It is: remove all factors of two, then take the log of the<br/>&gt;&gt;&gt; denominator or the numerator, whichever is larger.<br/>&gt;<br/>&gt;&gt; No, it isn&apos;t.  Consider 225:224, for example: [snip]<br/>&gt;<br/>&gt; I stand by my way and would rather you addressed my examples of 9:5 and<br/>&gt; 11:5</p><p>See above.</p><p>&gt;      instead of 225:224 (for which the lattice evaluation is unlikely to<br/>&gt; be very meaningful anyway).</p><p>Meaningfulness is a separate question.  As we have already seen, we all<br/>use these various metric functions for slightly different purposes and<br/>in slightly different ways.  But is the function not defined on a large<br/>interval like 225:224?  Is not the shortest path through the 7-limit<br/>lattice one septimal interval, one 5-limit interval, and two 3-limit<br/>intervals?</p><p>&gt;                             If it must be discussed, my value for<br/>&gt; 225:224 assumes there is a 225-axis or 225-wormholes in the lattice.</p><p>As Carl Lumma has already pointed out, this seems quite strange--it<br/>would seem to imply that you can only apply this metric to intervals<br/>which one considers primary/within a given odd limit/consonant.<br/>IOW, it _is_ (log of) odd-limit.  Which would negate the whole point of<br/>using a city-block function in the first place.</p><p>--pH &lt;<a href="mailto:manynote@lib-rary.wustl.edu">manynote@lib-rary.wustl.edu</a>&gt; <a href="http://library.wustl.edu/~manynote">http://library.wustl.edu/~manynote</a><br/>    O<br/>   /\        &quot;How about that?  The guy can&apos;t run six balls,<br/>  -\-\-- o    and they make him president.&quot;</p><p>             NOTE: dehyphenate node to remove spamblock.          &lt;*&gt;</p></div>